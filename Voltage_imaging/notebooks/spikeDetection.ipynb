{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "798cfe0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "['Denoise', 'List', 'Normalize', 'Optional', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', 'build_ws_denoising', 'const', 'load_model_from_checkpoint', 'logging', 'np', 'os', 'skio', 'tifffile', 'torch']\n",
      "Model created successfully!\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'D:\\\\Paris2025\\\\ForceB Undiluted\\\\20251001\\\\cell2\\\\016-SD50Hz-70\\\\016-SD50Hz-70_NDTiffStack.tif'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_4476\\1419888851.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mstack\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mframe_times\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[0mFPS\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1000\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m \u001b[0mtiff_stack\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mframe_times\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_tif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"D:/Paris2025/ForceB Undiluted/20251001/cell2/016-SD50Hz-70/016-SD50Hz-70_NDTiffStack.tif\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFPS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_4476\\1419888851.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(file_path, fps)\u001b[0m\n\u001b[0;32m     60\u001b[0m         \u001b[0mImage\u001b[0m \u001b[0mstack\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mframes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[0mframe_times\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[0mTime\u001b[0m \u001b[0mvector\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0meach\u001b[0m \u001b[0mframe\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mseconds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \"\"\"\n\u001b[1;32m---> 64\u001b[1;33m     \u001b[0mstack\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtifffile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# shape: (frames, h, w)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m     \u001b[0mn_frames\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstack\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[0mframe_times\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_frames\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mfps\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\tifffile\\tifffile.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m   1244\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1245\u001b[0m         if isinstance(files, str) or not isinstance(\n\u001b[0;32m   1246\u001b[0m             \u001b[0mfiles\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSequence\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1247\u001b[0m         \u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1248\u001b[1;33m             with TiffFile(\n\u001b[0m\u001b[0;32m   1249\u001b[0m                 \u001b[0mfiles\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1250\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1251\u001b[0m                 \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\tifffile\\tifffile.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, file, mode, name, offset, size, omexml, _multifile, _useframes, _parent, **is_flags)\u001b[0m\n\u001b[0;32m   4255\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'invalid OME-XML'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4256\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_omexml\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0momexml\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4257\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_ome\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4259\u001b[1;33m         \u001b[0mfh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFileHandle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moffset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moffset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4260\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfh\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4261\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_multifile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m_multifile\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_multifile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4262\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_files\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mfh\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\tifffile\\tifffile.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, file, mode, name, offset, size)\u001b[0m\n\u001b[0;32m  14640\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_offset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0moffset\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0moffset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  14641\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0msize\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  14642\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_close\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  14643\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNullContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m> 14644\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m  14645\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fh\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\tifffile\\tifffile.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m  14659\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mode\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'rb'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r+b'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wb'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'xb'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  14660\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33mf'\u001b[0m\u001b[1;33minvalid mode \u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mode\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  14661\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrealpath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  14662\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m> 14663\u001b[1;33m             self._fh = open(\n\u001b[0m\u001b[0;32m  14664\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  14665\u001b[0m             \u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  14666\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_close\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'D:\\\\Paris2025\\\\ForceB Undiluted\\\\20251001\\\\cell2\\\\016-SD50Hz-70\\\\016-SD50Hz-70_NDTiffStack.tif'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tifffile\n",
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Using device:\", device)\n",
    "import os\n",
    "os.environ['MPLBACKEND'] = 'Qt5Agg'  # Set backend before importing matplotlib\n",
    "import matplotlib\n",
    "matplotlib.use('Qt5Agg')  # Explicitly set backend\n",
    "import matplotlib.pyplot as plt\n",
    "#from matplotlib.backends.backend_tkagg import FigureCanvasTkAgg\n",
    "#import PySimpleGUI as sg\n",
    "from cellmincer.denoise import main as cm_denoise\n",
    "print(dir(cm_denoise))\n",
    "from cellmincer.models.spatial_unet_2d_temporal_denoiser import SpatialUnet2dTemporalDenoiser\n",
    "\n",
    "# COMPLETE configuration with ALL required keys\n",
    "config = {\n",
    "    'type': 'spatial_unet_2d_temporal_denoiser',  # REQUIRED\n",
    "    \n",
    "    # Spatial U-Net parameters\n",
    "    'n_global_features': 1,\n",
    "    'spatial_unet_depth': 4,\n",
    "    'spatial_unet_first_conv_channels': 32,\n",
    "    'spatial_unet_padding': True,\n",
    "    'spatial_unet_batch_norm': True,\n",
    "    'spatial_unet_attention': False,\n",
    "    'spatial_unet_feature_mode': 'repeat',  # 'repeat', 'once', or 'none'\n",
    "    'spatial_unet_kernel_size': 3,\n",
    "    'spatial_unet_n_conv_layers': 2,\n",
    "    'spatial_unet_readout_kernel_size': 1,\n",
    "    'spatial_unet_activation': 'relu',\n",
    "    \n",
    "    # Temporal denoiser parameters\n",
    "    'temporal_denoiser_kernel_size': 3,\n",
    "    'temporal_denoiser_conv_channels': 32,\n",
    "    'temporal_denoiser_hidden_dense_layer_dims': [64],\n",
    "    'temporal_denoiser_activation': 'relu',\n",
    "    'temporal_denoiser_n_conv_layers': 2,  # This is used in get_temporal_order_from_config\n",
    "}\n",
    "\n",
    "# Now create the model\n",
    "model = SpatialUnet2dTemporalDenoiser(config=config)\n",
    "print(\"Model created successfully!\")\n",
    "\n",
    "def load_tif(file_path, fps):\n",
    "    \"\"\"\n",
    "    Load imaging stack (TIFF).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    file_path : str\n",
    "        Path to the TIFF stack.\n",
    "    fps : float\n",
    "        Imaging frame rate (Hz).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    stack : np.ndarray\n",
    "        Image stack (frames, height, width).\n",
    "    frame_times : np.ndarray\n",
    "        Time vector for each frame (seconds).\n",
    "    \"\"\"\n",
    "    stack = tifffile.imread(file_path)  # shape: (frames, h, w)\n",
    "    n_frames = stack.shape[0]\n",
    "    frame_times = np.arange(n_frames) / fps\n",
    "\n",
    "    return stack, frame_times\n",
    "\n",
    "FPS = 1000\n",
    "tiff_stack, frame_times = load_tif(\"D:/Paris2025/ForceB Undiluted/20251001/cell2/016-SD50Hz-70/016-SD50Hz-70_NDTiffStack.tif\", FPS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248431af",
   "metadata": {},
   "source": [
    "## Train Model with Torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8febd4db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded TIFF stack shape: (1487, 89, 90)\n",
      "Data type: float32\n",
      "Training on data shape: (50, 89, 90)\n",
      "Training model...\n",
      "Epoch 0, Average Loss: 0.018752\n",
      "Epoch 10, Average Loss: 0.005907\n",
      "Epoch 20, Average Loss: 0.004933\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 192\u001b[0m\n\u001b[0;32m    190\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining model...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    191\u001b[0m \u001b[38;5;66;03m# Try batch processing for faster training\u001b[39;00m\n\u001b[1;32m--> 192\u001b[0m trained_model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_batch_denoiser\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDenoising...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    195\u001b[0m denoised_data \u001b[38;5;241m=\u001b[39m denoise_batch_simple(trained_model, train_data)\n",
      "Cell \u001b[1;32mIn[20], line 154\u001b[0m, in \u001b[0;36mtrain_batch_denoiser\u001b[1;34m(movie_data, num_epochs)\u001b[0m\n\u001b[0;32m    152\u001b[0m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[0;32m    153\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m--> 154\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    155\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    157\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import tifffile\n",
    "#import matplotlib.pyplot as plt\n",
    "\n",
    "# Load your TIFF stack\n",
    "tiff_path = \"D:/Paris2025/ForceB Undiluted/20251001/cell2/016-SD50Hz-70/016-SD50Hz-70_NDTiffStack.tif\"\n",
    "tiff_stack = tifffile.imread(tiff_path)\n",
    "print(f\"Loaded TIFF stack shape: {tiff_stack.shape}\")\n",
    "\n",
    "# Normalize and convert to float32\n",
    "def normalize_data(data):\n",
    "    data_min = data.min()\n",
    "    data_max = data.max()\n",
    "    if data_max - data_min > 0:\n",
    "        return (data - data_min) / (data_max - data_min)\n",
    "    return data\n",
    "\n",
    "# Convert to float32 explicitly\n",
    "tiff_stack_normalized = normalize_data(tiff_stack.astype(np.float32))\n",
    "print(f\"Data type: {tiff_stack_normalized.dtype}\")\n",
    "\n",
    "# Use a smaller subset for faster training\n",
    "if tiff_stack_normalized.shape[0] > 50:\n",
    "    train_data = tiff_stack_normalized[30:80]\n",
    "else:\n",
    "    train_data = tiff_stack_normalized\n",
    "\n",
    "print(f\"Training on data shape: {train_data.shape}\")\n",
    "\n",
    "# Create a simple frame denoiser\n",
    "class SimpleFrameDenoiser(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleFrameDenoiser, self).__init__()\n",
    "        self.encoder = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(1, 16, 3, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(16, 32, 3, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(32, 64, 3, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "        )\n",
    "        self.decoder = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(64, 32, 3, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(32, 16, 3, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(16, 1, 3, padding=1),\n",
    "            torch.nn.Sigmoid()  # Output in [0,1] range\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        return self.decoder(x)\n",
    "\n",
    "def train_simple_frame_denoiser(movie_data, num_epochs=30):\n",
    "    \"\"\"Train a simple frame-by-frame denoiser\"\"\"\n",
    "    T, H, W = movie_data.shape\n",
    "    \n",
    "    model = SimpleFrameDenoiser()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        # Train on individual frames\n",
    "        for frame_idx in range(T):\n",
    "            # Get single frame\n",
    "            clean_frame = movie_data[frame_idx:frame_idx+1]  # Keep as [1, H, W]\n",
    "            \n",
    "            # Add noise\n",
    "            noise = 0.1 * np.random.randn(*clean_frame.shape).astype(np.float32)\n",
    "            noisy_frame = np.clip(clean_frame + noise, 0, 1)\n",
    "            \n",
    "            # Convert to tensor - ensure float32\n",
    "            noisy_tensor = torch.from_numpy(noisy_frame).unsqueeze(0).float()  # [1, 1, H, W]\n",
    "            clean_tensor = torch.from_numpy(clean_frame).unsqueeze(0).float()  # [1, 1, H, W]\n",
    "            \n",
    "            # Forward pass\n",
    "            output = model(noisy_tensor)\n",
    "            loss = criterion(output, clean_tensor)\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            avg_loss = epoch_loss / T\n",
    "            print(f\"Epoch {epoch}, Average Loss: {avg_loss:.6f}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "def denoise_frames_simple(model, movie_data):\n",
    "    \"\"\"Denoise frames using simple model\"\"\"\n",
    "    model.eval()\n",
    "    T, H, W = movie_data.shape\n",
    "    denoised_frames = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for frame_idx in range(T):\n",
    "            frame = movie_data[frame_idx:frame_idx+1]  # [1, H, W]\n",
    "            input_tensor = torch.from_numpy(frame).unsqueeze(0).float()  # [1, 1, H, W]\n",
    "            output_tensor = model(input_tensor)\n",
    "            denoised_frame = output_tensor.numpy()[0, 0]  # [H, W]\n",
    "            denoised_frames.append(denoised_frame)\n",
    "    \n",
    "    return np.array(denoised_frames)\n",
    "\n",
    "# Alternative: Batch processing version (faster)\n",
    "def train_batch_denoiser(movie_data, num_epochs=30):\n",
    "    \"\"\"Train using batch processing\"\"\"\n",
    "    T, H, W = movie_data.shape\n",
    "    \n",
    "    model = SimpleFrameDenoiser()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        \n",
    "        # Process in batches\n",
    "        batch_size = min(8, T)  # Smaller batch size for memory\n",
    "        num_batches = (T + batch_size - 1) // batch_size\n",
    "        \n",
    "        for batch_idx in range(num_batches):\n",
    "            start_idx = batch_idx * batch_size\n",
    "            end_idx = min((batch_idx + 1) * batch_size, T)\n",
    "            \n",
    "            # Get batch of frames\n",
    "            clean_batch = movie_data[start_idx:end_idx]  # [batch_size, H, W]\n",
    "            \n",
    "            # Add noise to entire batch\n",
    "            noise = 0.1 * np.random.randn(*clean_batch.shape).astype(np.float32)\n",
    "            noisy_batch = np.clip(clean_batch + noise, 0, 1)\n",
    "            \n",
    "            # Convert to tensors - shape: [batch_size, 1, H, W]\n",
    "            noisy_tensor = torch.from_numpy(noisy_batch).unsqueeze(1).float()\n",
    "            clean_tensor = torch.from_numpy(clean_batch).unsqueeze(1).float()\n",
    "            \n",
    "            # Forward pass\n",
    "            output = model(noisy_tensor)\n",
    "            loss = criterion(output, clean_tensor)\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            avg_loss = total_loss / num_batches\n",
    "            print(f\"Epoch {epoch}, Average Loss: {avg_loss:.6f}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "def denoise_batch_simple(model, movie_data):\n",
    "    \"\"\"Denoise using batch processing\"\"\"\n",
    "    model.eval()\n",
    "    T, H, W = movie_data.shape\n",
    "    \n",
    "    denoised_frames = []\n",
    "    batch_size = min(16, T)  # Process in batches\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for start_idx in range(0, T, batch_size):\n",
    "            end_idx = min(start_idx + batch_size, T)\n",
    "            batch_data = movie_data[start_idx:end_idx]\n",
    "            \n",
    "            # Convert to tensor\n",
    "            input_tensor = torch.from_numpy(batch_data).unsqueeze(1).float()  # [batch, 1, H, W]\n",
    "            \n",
    "            # Denoise\n",
    "            output_tensor = model(input_tensor)\n",
    "            denoised_batch = output_tensor.numpy()[:, 0]  # [batch, H, W]\n",
    "            \n",
    "            denoised_frames.extend(denoised_batch)\n",
    "    \n",
    "    return np.array(denoised_frames)\n",
    "\n",
    "# Main execution\n",
    "print(\"Training model...\")\n",
    "# Try batch processing for faster training\n",
    "trained_model = train_batch_denoiser(train_data, num_epochs=30)\n",
    "\n",
    "print(\"Denoising...\")\n",
    "denoised_data = denoise_batch_simple(trained_model, train_data)\n",
    "\n",
    "# Save results\n",
    "output_path = \"D:/Paris2025/ForceB Undiluted/20251001/cell2/016-SD50Hz-70/denoised_movie.tif\"\n",
    "tifffile.imwrite(output_path, denoised_data.astype(np.float32))\n",
    "print(f\"Denoised movie saved to: {output_path}\")\n",
    "\n",
    "# Save the trained model\n",
    "model_save_path = \"D:/Paris2025/ForceB Undiluted/20251001/cell2/016-SD50Hz-70/trained_denoiser.pth\"\n",
    "torch.save(trained_model.state_dict(), model_save_path)\n",
    "print(f\"Model saved to: {model_save_path}\")\n",
    "\n",
    "# Visualize results\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Original frame\n",
    "axes[0].imshow(train_data[0], cmap='gray')\n",
    "axes[0].set_title('Original Frame 0')\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Denoised frame\n",
    "axes[1].imshow(denoised_data[0], cmap='gray')\n",
    "axes[1].set_title('Denoised Frame 0')\n",
    "axes[1].axis('off')\n",
    "\n",
    "# Difference\n",
    "axes[2].imshow(train_data[0] - denoised_data[0], cmap='gray')\n",
    "axes[2].set_title('Noise Removed')\n",
    "axes[2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0370a2dd",
   "metadata": {},
   "source": [
    "## Denoise Movie with CellMincer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "82fd0eaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n",
      "Loading video: D:/Paris2025/ForceB Undiluted/20251001/cell2/016-SD50Hz-70/016-SD50Hz-70_NDTiffStack.tif\n",
      "Full video shape: (1487, 89, 90)\n",
      "Processing 1487 frames in chunks of 100...\n",
      "Processing frames 0 to 99...\n",
      "Processing frames 100 to 199...\n",
      "Processing frames 200 to 299...\n",
      "Processing frames 300 to 399...\n",
      "Processing frames 400 to 499...\n",
      "Processing frames 500 to 599...\n",
      "Processing frames 600 to 699...\n",
      "Processing frames 700 to 799...\n",
      "Processing frames 800 to 899...\n",
      "Processing frames 900 to 999...\n",
      "Processing frames 1000 to 1099...\n",
      "Processing frames 1100 to 1199...\n",
      "Processing frames 1200 to 1299...\n",
      "Processing frames 1300 to 1399...\n",
      "Processing frames 1400 to 1486...\n",
      "Denoised video saved to: D:/Paris2025/ForceB Undiluted/20251001/cell2/016-SD50Hz-70/016-SD50Hz-70_denoised_full.tif\n",
      "Done processing full video!\n"
     ]
    }
   ],
   "source": [
    "class SimpleFrameDenoiser(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleFrameDenoiser, self).__init__()\n",
    "        self.encoder = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(1, 16, 3, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(16, 32, 3, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(32, 64, 3, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "        )\n",
    "        self.decoder = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(64, 32, 3, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(32, 16, 3, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(16, 1, 3, padding=1),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        return self.decoder(x)\n",
    "\n",
    "def denoise_large_video(model, video_path, output_path, chunk_size=100):\n",
    "    \"\"\"Denoise a large video in chunks to avoid memory issues\"\"\"\n",
    "    \n",
    "    # Load the full video\n",
    "    print(f\"Loading video: {video_path}\")\n",
    "    full_video = tifffile.imread(video_path)\n",
    "    print(f\"Full video shape: {full_video.shape}\")\n",
    "    \n",
    "    # Normalize\n",
    "    def normalize_data(data):\n",
    "        data_min = data.min()\n",
    "        data_max = data.max()\n",
    "        if data_max - data_min > 0:\n",
    "            return (data - data_min) / (data_max - data_min)\n",
    "        return data\n",
    "    \n",
    "    full_video = normalize_data(full_video.astype(np.float32))\n",
    "    \n",
    "    model.eval()\n",
    "    denoised_chunks = []\n",
    "    \n",
    "    total_frames = full_video.shape[0]\n",
    "    print(f\"Processing {total_frames} frames in chunks of {chunk_size}...\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for start_idx in range(0, total_frames, chunk_size):\n",
    "            end_idx = min(start_idx + chunk_size, total_frames)\n",
    "            chunk = full_video[start_idx:end_idx]\n",
    "            \n",
    "            print(f\"Processing frames {start_idx} to {end_idx-1}...\")\n",
    "            \n",
    "            # Convert to tensor and denoise\n",
    "            input_tensor = torch.from_numpy(chunk).unsqueeze(1).float()  # [chunk_size, 1, H, W]\n",
    "            output_tensor = model(input_tensor)\n",
    "            denoised_chunk = output_tensor.numpy()[:, 0]  # [chunk_size, H, W]\n",
    "            \n",
    "            denoised_chunks.append(denoised_chunk)\n",
    "            \n",
    "            # Clear memory\n",
    "            del input_tensor, output_tensor\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "    \n",
    "    # Combine all chunks\n",
    "    denoised_full = np.vstack(denoised_chunks)\n",
    "    \n",
    "    # Save result\n",
    "    tifffile.imwrite(output_path, denoised_full.astype(np.float32))\n",
    "    print(f\"Denoised video saved to: {output_path}\")\n",
    "    \n",
    "    return denoised_full\n",
    "\n",
    "# Load your trained model\n",
    "model = SimpleFrameDenoiser()\n",
    "model.load_state_dict(torch.load(model_save_path))\n",
    "print(\"Model loaded successfully!\")\n",
    "\n",
    "# Process your full video\n",
    "input_video = \"D:/Paris2025/ForceB Undiluted/20251001/cell2/016-SD50Hz-70/016-SD50Hz-70_NDTiffStack.tif\"\n",
    "output_video = \"D:/Paris2025/ForceB Undiluted/20251001/cell2/016-SD50Hz-70/016-SD50Hz-70_denoised_full.tif\"\n",
    "\n",
    "denoised_full = denoise_large_video(model, input_video, output_video, chunk_size=100)\n",
    "print(\"Done processing full video!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e39f8b4",
   "metadata": {},
   "source": [
    "## Segmentation with CellPose vs. Manual Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6560daa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "SUITE2P VOLTAGE IMAGING PIPELINE WITH MANUAL FALLBACK\n",
      "============================================================\n",
      "Suite2p configured for voltage imaging:\n",
      "  Frame rate: 1000 Hz\n",
      "  Tau: 0.0028 s\n",
      "  Diameter: 75 pixels\n",
      "  Spatial high-pass: 100 pixels\n",
      "\n",
      "============================================================\n",
      "Running Suite2p...\n",
      "============================================================\n",
      "Input: D:\\Paris2025\\ForceB Undiluted\\20251001\\cell2\\016-SD50Hz-70\\016-SD50Hz-70_NDTiffStack.tif\n",
      "Output: D:\\Paris2025\\ForceB Undiluted\\20251001\\cell2\\016-SD50Hz-70\\suite2p_output\n",
      "{'data_path': ['D:\\\\Paris2025\\\\ForceB Undiluted\\\\20251001\\\\cell2\\\\016-SD50Hz-70'], 'tiff_list': ['D:\\\\Paris2025\\\\ForceB Undiluted\\\\20251001\\\\cell2\\\\016-SD50Hz-70\\\\016-SD50Hz-70_NDTiffStack.tif'], 'save_path0': 'D:\\\\Paris2025\\\\ForceB Undiluted\\\\20251001\\\\cell2\\\\016-SD50Hz-70\\\\suite2p_output'}\n",
      "FOUND BINARIES AND OPS IN ['D:\\\\Paris2025\\\\ForceB Undiluted\\\\20251001\\\\cell2\\\\016-SD50Hz-70\\\\suite2p_output\\\\suite2p\\\\plane0\\\\ops.npy']\n",
      "removing previous detection and extraction files, if present\n",
      ">>>>>>>>>>>>>>>>>>>>> PLANE 0 <<<<<<<<<<<<<<<<<<<<<<\n",
      "NOTE: not running registration, ops['do_registration']=0\n",
      "binary path: D:\\Paris2025\\ForceB Undiluted\\20251001\\cell2\\016-SD50Hz-70\\suite2p_output\\suite2p\\plane0\\data.bin\n",
      "NOTE: Applying builtin classifier at C:\\Users\\sofik\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\suite2p\\classifiers\\classifier.npy\n",
      "----------- ROI DETECTION\n",
      "Binning movie in chunks of length 03\n",
      "Binned movie of size [495,89,90] created in 0.06 sec.\n",
      "NOTE: estimated spatial scale ~48 pixels, time epochs 1.00, threshold 20.00 \n",
      "0 ROIs, score=25.92\n",
      "Detected 1 ROIs, 0.30 sec\n",
      "After removing overlaps, 1 ROIs remain\n",
      "----------- Total 0.40 sec.\n",
      "----------- EXTRACTION\n",
      "Masks created, 3.84 sec.\n",
      "Extracted fluorescence from 1 ROIs in 1487 frames, 0.05 sec.\n",
      "----------- Total 3.89 sec.\n",
      "----------- CLASSIFICATION\n",
      "['npix_norm', 'skew', 'compact']\n",
      "----------- SPIKE DECONVOLUTION\n",
      "----------- Total 0.00 sec.\n",
      "Plane 0 processed in 4.34 sec (can open in GUI).\n",
      "total = 4.53 sec.\n",
      "TOTAL RUNTIME 4.53 sec\n",
      "\n",
      "Loading results from D:\\Paris2025\\ForceB Undiluted\\20251001\\cell2\\016-SD50Hz-70\\suite2p_output\\suite2p\\plane0\n",
      "Loaded 1 ROIs (0 classified as cells)\n",
      "Traces shape: (1, 1487) (ROIs × frames)\n",
      "Suite2p found 0 cells\n",
      "No cells found automatically. Falling back to manual selection...\n",
      "\n",
      "============================================================\n",
      "MANUAL ROI SELECTION\n",
      "============================================================\n",
      "Added ROI 1: pos=(11, 15), size=(66)\n",
      "Manual traces shape: (1, 1487)\n",
      "Successfully extracted traces from 1 manual ROIs\n",
      "\n",
      "============================================================\n",
      "Processing complete!\n",
      "============================================================\n",
      "Saved manual ROI visualization to D:\\Paris2025\\ForceB Undiluted\\20251001\\cell2\\016-SD50Hz-70\\suite2p_rois_manual.png\n",
      "\n",
      "Exported MANUAL results to D:\\Paris2025\\ForceB Undiluted\\20251001\\cell2\\016-SD50Hz-70\\suite2p_exported:\n",
      "  - fluorescence_manual.npy: (1, 1487)\n",
      "  - roi_masks.npy: 1 masks\n",
      "  - metadata.npy\n",
      "\n",
      "============================================================\n",
      "PIPELINE COMPLETE! (Mode: MANUAL)\n",
      "============================================================\n",
      "Using manual ROI traces (no neuropil correction available)\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from suite2p import run_s2p, default_ops\n",
    "# os.environ['MPLBACKEND'] = 'QtAgg'  # Set backend before importing matplotlib\n",
    "# import matplotlib\n",
    "# matplotlib.use('QtAgg') \n",
    "# import matplotlib.pyplot as plt\n",
    "from matplotlib.widgets import EllipseSelector\n",
    "import tifffile\n",
    "\n",
    "\n",
    "class Suite2pVoltagePipeline:\n",
    "    \"\"\"\n",
    "    Complete Suite2p pipeline optimized for voltage imaging with manual fallback.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_path, tiff_file, output_dir='./suite2p_output'):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        data_path : str\n",
    "            Directory containing the TIFF file\n",
    "        tiff_file : str\n",
    "            Name of the TIFF file (e.g., 'denoised_movie.tif')\n",
    "        output_dir : str\n",
    "            Where to save Suite2p results\n",
    "        \"\"\"\n",
    "        self.data_path = Path(data_path)\n",
    "        self.tiff_file = tiff_file\n",
    "        self.output_dir = Path(output_dir)\n",
    "        self.ops = None\n",
    "        self.stat = None\n",
    "        self.F = None\n",
    "        self.Fneu = None\n",
    "        self.spks = None\n",
    "        self.iscell = None\n",
    "        self.manual_mode = False  # Track if we used manual selection\n",
    "        self.manual_traces = None\n",
    "        self.manual_masks = None\n",
    "        \n",
    "    def configure_ops(self, frame_rate=1000, diameter=12, tau=0.0028, \n",
    "                     spatial_hp=100, threshold_scaling=1.0, max_iterations=50):\n",
    "        \"\"\"\n",
    "        Configure Suite2p parameters optimized for voltage imaging.\n",
    "        \"\"\"\n",
    "        # Start with default ops\n",
    "        self.ops = default_ops()\n",
    "        \n",
    "        # ============ CRITICAL VOLTAGE IMAGING SETTINGS ============\n",
    "        self.ops['fs'] = frame_rate\n",
    "        self.ops['tau'] = tau\n",
    "        self.ops['diameter'] = diameter\n",
    "        self.ops['spatial_hp_detect'] = spatial_hp\n",
    "        self.ops['spatial_hp_reg'] = 0\n",
    "        self.ops['threshold_scaling'] = threshold_scaling\n",
    "        self.ops['max_iterations'] = max_iterations\n",
    "        self.ops['high_pass'] = 100\n",
    "        self.ops['allow_overlap'] = True\n",
    "        self.ops['max_overlap'] = 0.4\n",
    "        self.ops['inner_neuropil_radius'] = 70\n",
    "        self.ops['min_neuropil_pixels'] = 80\n",
    "        self.ops['do_registration'] = False\n",
    "        self.ops['two_step_registration'] = True\n",
    "        self.ops['keep_movie_raw'] = False\n",
    "        self.ops['smooth_sigma'] = 1.15\n",
    "        self.ops['classifier_path'] = None\n",
    "        self.ops['batch_size'] = 500\n",
    "        self.ops['num_workers'] = 0\n",
    "        self.ops['save_mat'] = False\n",
    "        self.ops['save_NWB'] = False\n",
    "        self.ops['combined'] = False\n",
    "        \n",
    "        print(\"Suite2p configured for voltage imaging:\")\n",
    "        print(f\"  Frame rate: {frame_rate} Hz\")\n",
    "        print(f\"  Tau: {tau} s\")\n",
    "        print(f\"  Diameter: {diameter} pixels\")\n",
    "        print(f\"  Spatial high-pass: {spatial_hp} pixels\")\n",
    "        \n",
    "        return self.ops\n",
    "    \n",
    "    def run_suite2p_with_manual_fallback(self, enable_manual_fallback=True):\n",
    "        \"\"\"\n",
    "        Run Suite2p with automatic fallback to manual selection.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        enable_manual_fallback : bool\n",
    "            If True, automatically fall back to manual selection when no cells are found\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        output_path : Path\n",
    "            Path to output directory\n",
    "        \"\"\"\n",
    "        if self.ops is None:\n",
    "            print(\"No ops configured, using defaults for voltage imaging...\")\n",
    "            self.configure_ops()\n",
    "        \n",
    "        # Set up database\n",
    "        db = {\n",
    "            'data_path': [str(self.data_path)],\n",
    "            'tiff_list': [str(self.data_path / self.tiff_file)],\n",
    "            'save_path0': str(self.output_dir), \n",
    "        }\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"Running Suite2p...\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"Input: {self.data_path / self.tiff_file}\")\n",
    "        print(f\"Output: {self.output_dir}\")\n",
    "        \n",
    "        # Run Suite2p\n",
    "        try:\n",
    "            output_ops = run_s2p(ops=self.ops, db=db)\n",
    "            self.load_results()\n",
    "            \n",
    "            # Check if any cells were found\n",
    "            n_cells = np.sum(self.iscell[:, 0] > 0)\n",
    "            print(f\"Suite2p found {n_cells} cells\")\n",
    "            \n",
    "            if n_cells == 0 and enable_manual_fallback:\n",
    "                print(\"No cells found automatically. Falling back to manual selection...\")\n",
    "                self.run_manual_selection()\n",
    "                self.manual_mode = True\n",
    "            else:\n",
    "                self.manual_mode = False\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Suite2p failed with error: {e}\")\n",
    "            if enable_manual_fallback:\n",
    "                print(\"Falling back to manual selection...\")\n",
    "                self.run_manual_selection()\n",
    "                self.manual_mode = True\n",
    "            else:\n",
    "                raise e\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"Processing complete!\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        return self.output_dir / 'suite2p' / 'plane0'\n",
    "    \n",
    "    def run_manual_selection(self):\n",
    "        \"\"\"\n",
    "        Run manual ROI selection when automatic detection fails.\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"MANUAL ROI SELECTION\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Load the movie\n",
    "        movie_path = self.data_path / self.tiff_file\n",
    "        movie = tifffile.imread(movie_path)\n",
    "        mean_image = np.max(movie, axis=0)\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(12, 10))\n",
    "        ax.imshow(mean_image, cmap='gray')\n",
    "        ax.set_title('Click and drag to select ROIs. Press Enter when done.')\n",
    "        \n",
    "        rois = []  # List of (y, x, height, width)\n",
    "        \n",
    "        def onselect(eclick, erelease):\n",
    "            \"\"\"Callback for rectangle selection\"\"\"\n",
    "            x1, y1 = int(eclick.xdata), int(eclick.ydata)\n",
    "            x2, y2 = int(erelease.xdata), int(erelease.ydata)\n",
    "            \n",
    "            # Ensure coordinates are ordered\n",
    "            xmin, xmax = min(x1, x2), max(x1, x2)\n",
    "            ymin, ymax = min(y1, y2), max(y1, y2)\n",
    "            \n",
    "            diameter = max(xmax - xmin, ymax - ymin)\n",
    "            \n",
    "            rois.append((ymin, xmin, diameter))\n",
    "            \n",
    "            # Draw the rectangle\n",
    "            rect = plt.Circle((xmin, ymin), diameter, \n",
    "                               fill=False, edgecolor='red', linewidth=2)\n",
    "            ax.add_patch(rect)\n",
    "            plt.draw()\n",
    "            \n",
    "            print(f\"Added ROI {len(rois)}: pos=({xmin}, {ymin}), size=({diameter})\")\n",
    "        \n",
    "        def on_key(event):\n",
    "            \"\"\"Finish selection on Enter key\"\"\"\n",
    "            if event.key == 'enter':\n",
    "                plt.close()\n",
    "                print(f\"\\nSelected {len(rois)} ROIs\")\n",
    "        \n",
    "        # Create selector\n",
    "        rs = EllipseSelector(ax, onselect, useblit=True,\n",
    "                             button=[1], minspanx=1, minspany=1,\n",
    "                             spancoords='pixels', interactive=True)\n",
    "        \n",
    "        fig.canvas.mpl_connect('key_press_event', on_key)\n",
    "        plt.show()\n",
    "        \n",
    "        if rois:\n",
    "            self.extract_manual_traces(movie, rois)\n",
    "            print(f\"Successfully extracted traces from {len(rois)} manual ROIs\")\n",
    "        else:\n",
    "            print(\"No ROIs selected manually.\")\n",
    "    \n",
    "    def extract_manual_traces(self, movie, rois):\n",
    "        \"\"\"\n",
    "        Extract fluorescence traces from manual ROIs.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        movie : np.ndarray\n",
    "            Loaded movie data\n",
    "        rois : list\n",
    "            List of ROI coordinates (y, x, height, width)\n",
    "        \"\"\"\n",
    "        height, width = movie.shape[1], movie.shape[2]\n",
    "        masks = []\n",
    "        \n",
    "        # Create binary masks from ROI coordinates\n",
    "        for i, (y, x, diam) in enumerate(rois):\n",
    "            mask = np.zeros((height, width), dtype=bool)\n",
    "            # Ensure coordinates are within bounds\n",
    "            y_end = min(y + diam, height)\n",
    "            x_end = min(x + diam, width)\n",
    "            mask[y:y_end, x:x_end] = True\n",
    "            masks.append(mask)\n",
    "        \n",
    "        masks_array = np.array(masks)\n",
    "        \n",
    "        # Extract traces\n",
    "        F_manual = []\n",
    "        for mask in masks_array:\n",
    "            trace = np.mean(movie[:, mask], axis=1)\n",
    "            F_manual.append(trace)\n",
    "        \n",
    "        self.manual_traces = np.array(F_manual)\n",
    "        self.manual_masks = masks_array\n",
    "        \n",
    "        # Create dummy Suite2p-compatible outputs for compatibility\n",
    "        n_rois = len(rois)\n",
    "        n_frames = movie.shape[0]\n",
    "        \n",
    "        self.F = self.manual_traces\n",
    "        self.Fneu = np.zeros_like(self.manual_traces)  # No neuropil for manual\n",
    "        self.spks = np.zeros_like(self.manual_traces)  # No spikes for manual\n",
    "        self.iscell = np.column_stack([np.ones(n_rois), np.ones(n_rois)])  # All are cells\n",
    "        self.stat = self.create_dummy_stat(masks_array)\n",
    "        \n",
    "        print(f\"Manual traces shape: {self.manual_traces.shape}\")\n",
    "    \n",
    "    def create_dummy_stat(self, masks):\n",
    "        \"\"\"\n",
    "        Create dummy stat structure for manual ROIs to maintain compatibility.\n",
    "        \"\"\"\n",
    "        stat = []\n",
    "        for mask in masks:\n",
    "            ypix, xpix = np.where(mask)\n",
    "            stat.append({\n",
    "                'ypix': ypix,\n",
    "                'xpix': xpix,\n",
    "                'lam': np.ones(len(ypix)) / len(ypix),  # Uniform weights\n",
    "                'med': [np.median(ypix), np.median(xpix)],  # Center\n",
    "                'footprint': 1  # Simple footprint\n",
    "            })\n",
    "        return np.array(stat, dtype=object)\n",
    "    \n",
    "    def load_results(self):\n",
    "        \"\"\"Load Suite2p output files.\"\"\"\n",
    "        result_path = self.output_dir / 'suite2p' / 'plane0'\n",
    "        \n",
    "        if not result_path.exists():\n",
    "            raise FileNotFoundError(f\"Suite2p results not found at {result_path}\")\n",
    "        \n",
    "        print(f\"\\nLoading results from {result_path}\")\n",
    "        \n",
    "        # Load main outputs\n",
    "        self.stat = np.load(result_path / 'stat.npy', allow_pickle=True)\n",
    "        self.F = np.load(result_path / 'F.npy')\n",
    "        self.Fneu = np.load(result_path / 'Fneu.npy')\n",
    "        self.spks = np.load(result_path / 'spks.npy')\n",
    "        self.iscell = np.load(result_path / 'iscell.npy')\n",
    "        self.ops = np.load(result_path / 'ops.npy', allow_pickle=True).item()\n",
    "        \n",
    "        n_cells = np.sum(self.iscell[:, 0] > 0)\n",
    "        n_total = len(self.iscell)\n",
    "        \n",
    "        print(f\"Loaded {n_total} ROIs ({n_cells} classified as cells)\")\n",
    "        print(f\"Traces shape: {self.F.shape} (ROIs × frames)\")\n",
    "    \n",
    "    def get_corrected_traces(self, neuropil_coefficient=0.7, cells_only=True):\n",
    "        \"\"\"\n",
    "        Get neuropil-corrected fluorescence traces.\n",
    "        Works for both automatic and manual modes.\n",
    "        \"\"\"\n",
    "        if self.manual_mode:\n",
    "            # For manual mode, just return the raw traces\n",
    "            print(\"Using manual ROI traces (no neuropil correction available)\")\n",
    "            return self.manual_traces\n",
    "        else:\n",
    "            # For automatic mode, use neuropil correction\n",
    "            if self.F is None:\n",
    "                raise ValueError(\"Run Suite2p first or load existing results!\")\n",
    "            \n",
    "            F_corrected = self.F - neuropil_coefficient * self.Fneu\n",
    "            \n",
    "            if cells_only:\n",
    "                cell_mask = self.iscell[:, 0] > 0\n",
    "                F_corrected = F_corrected[cell_mask]\n",
    "                print(f\"Returning {np.sum(cell_mask)} cells (filtered non-cells)\")\n",
    "            \n",
    "            return F_corrected\n",
    "    \n",
    "    def visualize_rois(self, max_display=50, save_path=None):\n",
    "        \"\"\"\n",
    "        Visualize detected ROIs.\n",
    "        Works for both automatic and manual modes.\n",
    "        \"\"\"\n",
    "        if self.manual_mode:\n",
    "            # Manual mode visualization\n",
    "            movie_path = self.data_path / self.tiff_file\n",
    "            movie = tifffile.imread(movie_path)\n",
    "            mean_img = np.mean(movie, axis=0)\n",
    "            \n",
    "            fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "            \n",
    "            # Mean image\n",
    "            axes[0].imshow(mean_img, cmap='gray')\n",
    "            axes[0].set_title('Mean Image')\n",
    "            axes[0].axis('off')\n",
    "            \n",
    "            # ROI overlay\n",
    "            axes[1].imshow(mean_img, cmap='gray')\n",
    "            for i, mask in enumerate(self.manual_masks):\n",
    "                if i >= max_display:\n",
    "                    break\n",
    "                # Plot outline\n",
    "                ypix, xpix = np.where(mask)\n",
    "                if len(ypix) > 0:\n",
    "                    y_min, y_max = ypix.min(), ypix.max()\n",
    "                    x_min, x_max = xpix.min(), xpix.max()\n",
    "                    axes[1].plot([x_min, x_max, x_max, x_min, x_min],\n",
    "                               [y_min, y_min, y_max, y_max, y_min], \n",
    "                               'r-', linewidth=1, alpha=0.8, label=f'ROI {i+1}')\n",
    "            \n",
    "            axes[1].set_title(f'Manual ROIs ({len(self.manual_masks)} cells)')\n",
    "            axes[1].axis('off')\n",
    "            axes[1].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "            \n",
    "        else:\n",
    "            # Automatic mode visualization (your original code)\n",
    "            if self.stat is None:\n",
    "                raise ValueError(\"No ROIs loaded!\")\n",
    "            \n",
    "            result_path = self.output_dir / 'suite2p' / 'plane0'\n",
    "            mean_img = np.load(result_path / 'ops.npy', allow_pickle=True).item()['meanImg']\n",
    "            \n",
    "            roi_img = np.zeros_like(mean_img)\n",
    "            cell_mask = self.iscell[:, 0] > 0\n",
    "            n_cells = np.sum(cell_mask)\n",
    "            \n",
    "            fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "            \n",
    "            axes[0].imshow(mean_img, cmap='gray')\n",
    "            axes[0].set_title('Mean Image')\n",
    "            axes[0].axis('off')\n",
    "            \n",
    "            for i, (st, is_cell) in enumerate(zip(self.stat, self.iscell)):\n",
    "                if i >= max_display:\n",
    "                    break\n",
    "                if is_cell[0] > 0:\n",
    "                    ypix = st['ypix']\n",
    "                    xpix = st['xpix']\n",
    "                    roi_img[ypix, xpix] = i + 1\n",
    "            \n",
    "            axes[1].imshow(roi_img, cmap='nipy_spectral', alpha=0.8)\n",
    "            axes[1].set_title(f'ROIs ({n_cells} cells)')\n",
    "            axes[1].axis('off')\n",
    "            \n",
    "            axes[2].imshow(mean_img, cmap='gray')\n",
    "            for i, (st, is_cell) in enumerate(zip(self.stat, self.iscell)):\n",
    "                if i >= max_display:\n",
    "                    break\n",
    "                if is_cell[0] > 0:\n",
    "                    ypix = st['ypix']\n",
    "                    xpix = st['xpix']\n",
    "                    y_min, y_max = ypix.min(), ypix.max()\n",
    "                    x_min, x_max = xpix.min(), xpix.max()\n",
    "                    axes[2].plot([x_min, x_max, x_max, x_min, x_min],\n",
    "                               [y_min, y_min, y_max, y_max, y_min], \n",
    "                               'r-', linewidth=0.5, alpha=0.7)\n",
    "            \n",
    "            axes[2].set_title('Overlay')\n",
    "            axes[2].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if save_path:\n",
    "            plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "            mode = \"manual\" if self.manual_mode else \"automatic\"\n",
    "            print(f\"Saved {mode} ROI visualization to {save_path}\")\n",
    "        \n",
    "        plt.show()\n",
    "        return fig\n",
    "        \n",
    "    def export_results(self, export_dir='./exported_results'):\n",
    "        \"\"\"\n",
    "        Export results in easy-to-use format.\n",
    "        Works for both automatic and manual modes.\n",
    "        \"\"\"\n",
    "        export_dir = Path(export_dir)\n",
    "        export_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        if self.manual_mode:\n",
    "            # Manual mode export\n",
    "            F_corrected = self.manual_traces\n",
    "            dff = (F_corrected - np.percentile(F_corrected, 8, axis=1, keepdims=True)) / np.percentile(F_corrected, 8, axis=1, keepdims=True)\n",
    "            \n",
    "            # Save as numpy arrays\n",
    "            np.save(export_dir / 'fluorescence_manual.npy', F_corrected)\n",
    "            np.save(export_dir / 'dff_manual.npy', dff)\n",
    "            \n",
    "            # Save ROI masks\n",
    "            np.save(export_dir / 'roi_masks.npy', self.manual_masks)\n",
    "            \n",
    "            # Save metadata\n",
    "            metadata = {\n",
    "                'frame_rate': getattr(self.ops, 'fs', 1000) if self.ops else 1000,\n",
    "                'n_cells': F_corrected.shape[0],\n",
    "                'n_frames': F_corrected.shape[1],\n",
    "                'mode': 'manual',\n",
    "                'export_time': str(np.datetime64('now'))\n",
    "            }\n",
    "            np.save(export_dir / 'metadata.npy', metadata)\n",
    "            \n",
    "            print(f\"\\nExported MANUAL results to {export_dir}:\")\n",
    "            print(f\"  - fluorescence_manual.npy: {F_corrected.shape}\")\n",
    "            print(f\"  - roi_masks.npy: {len(self.manual_masks)} masks\")\n",
    "            \n",
    "        else:\n",
    "            # Automatic mode export (your original code)\n",
    "            F_corrected = self.get_corrected_traces(cells_only=True)\n",
    "            spks = self.spks[self.iscell[:, 0] > 0]\n",
    "            dff = (F_corrected - np.percentile(F_corrected, 8, axis=1, keepdims=True)) / np.percentile(F_corrected, 8, axis=1, keepdims=True)\n",
    "                        \n",
    "            np.save(export_dir / 'fluorescence_corrected.npy', F_corrected)\n",
    "            np.save(export_dir / 'dff.npy', dff)\n",
    "            np.save(export_dir / 'spikes_deconvolved.npy', spks)\n",
    "            \n",
    "            cells_stat = self.stat[self.iscell[:, 0] > 0]\n",
    "            np.save(export_dir / 'cell_stats.npy', cells_stat)\n",
    "            \n",
    "            metadata = {\n",
    "                'frame_rate': self.ops['fs'],\n",
    "                'n_cells': F_corrected.shape[0],\n",
    "                'n_frames': F_corrected.shape[1],\n",
    "                'diameter': self.ops['diameter'],\n",
    "                'tau': self.ops['tau'],\n",
    "                'mode': 'automatic'\n",
    "            }\n",
    "            np.save(export_dir / 'metadata.npy', metadata)\n",
    "            \n",
    "            print(f\"\\nExported AUTOMATIC results to {export_dir}:\")\n",
    "            print(f\"  - fluorescence_corrected.npy: {F_corrected.shape}\")\n",
    "            print(f\"  - dff.npy: {dff.shape}\")\n",
    "            print(f\"  - spikes_deconvolved.npy: {spks.shape}\")\n",
    "            print(f\"  - cell_stats.npy: {len(cells_stat)} cells\")\n",
    "        \n",
    "        print(f\"  - metadata.npy\")\n",
    "        \n",
    "        return export_dir\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# UPDATED COMPLETE USAGE EXAMPLE\n",
    "# =============================================================================\n",
    "\n",
    "def run_complete_voltage_pipeline(tiff_path, frame_rate=1000, diameter=12, enable_manual_fallback=True):\n",
    "    \"\"\"\n",
    "    One-function pipeline for voltage imaging with automatic manual fallback.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    tiff_path : str\n",
    "        Path to your denoised TIFF file\n",
    "    frame_rate : float\n",
    "        Imaging frame rate in Hz\n",
    "    diameter : int\n",
    "        Expected neuron diameter in pixels\n",
    "    enable_manual_fallback : bool\n",
    "        If True, automatically use manual selection when no cells are found\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    pipeline : Suite2pVoltagePipeline\n",
    "        Pipeline object with all results\n",
    "    \"\"\"\n",
    "    # Parse path\n",
    "    tiff_path = Path(tiff_path)\n",
    "    data_path = tiff_path.parent\n",
    "    tiff_file = tiff_path.name\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"SUITE2P VOLTAGE IMAGING PIPELINE WITH MANUAL FALLBACK\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Initialize\n",
    "    pipeline = Suite2pVoltagePipeline(\n",
    "        data_path=data_path,\n",
    "        tiff_file=tiff_file,\n",
    "        output_dir=data_path / 'suite2p_output'\n",
    "    )\n",
    "    \n",
    "    # Configure for voltage imaging\n",
    "    pipeline.configure_ops(\n",
    "        frame_rate=frame_rate,\n",
    "        diameter=diameter,\n",
    "        tau=0.0028,\n",
    "        spatial_hp=100,\n",
    "        threshold_scaling=1.0\n",
    "    )\n",
    "    \n",
    "    # Run Suite2p with automatic manual fallback\n",
    "    pipeline.run_suite2p_with_manual_fallback(enable_manual_fallback=enable_manual_fallback)\n",
    "    \n",
    "    # Visualize results\n",
    "    mode = \"MANUAL\" if pipeline.manual_mode else \"AUTOMATIC\"\n",
    "    pipeline.visualize_rois(save_path=data_path / f'suite2p_rois_{mode.lower()}.png')\n",
    "    \n",
    "    # Export results\n",
    "    pipeline.export_results(export_dir=data_path / 'suite2p_exported')\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"PIPELINE COMPLETE! (Mode: {mode})\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    return pipeline\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # SIMPLE ONE-LINE USAGE WITH MANUAL FALLBACK:\n",
    "    pipeline = run_complete_voltage_pipeline(\n",
    "        tiff_path='D:/Paris2025/ForceB Undiluted/20251001/cell2/016-SD50Hz-70/016-SD50Hz-70_NDTiffStack.tif',\n",
    "        frame_rate=1000,\n",
    "        diameter=75,\n",
    "        enable_manual_fallback=True  # This enables the automatic fallback to manual selection\n",
    "    )\n",
    "    \n",
    "    # Access results (works for both automatic and manual modes):\n",
    "    F_corrected = pipeline.get_corrected_traces()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c999309",
   "metadata": {},
   "source": [
    "## Load ABF file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0aac294e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Directory: C:\\Users\\sofik\\AppData\\Local\\Programs\\Microsoft VS Code\n",
      "functions_path: C:\\Users\\sofik\\.vscode\\Voltage_imaging\\data_io\n",
      "read_abf.py found at: C:\\Users\\sofik\\.vscode\\Voltage_imaging\\data_io\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "import os, sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.signal as signal\n",
    "import pyabf\n",
    "from scipy.signal import butter, filtfilt, welch, iirnotch\n",
    "\n",
    "print(\"Current Directory:\", os.getcwd())\n",
    "read_abf_path = r\"C:\\Users\\sofik\\.vscode\\Voltage_imaging\\data_io\"\n",
    "print(\"functions_path:\", read_abf_path)\n",
    "sys.path.append(read_abf_path)\n",
    "\n",
    "# Check if read_abf.py exists at this location\n",
    "if os.path.isfile(os.path.join(read_abf_path, \"read_abf.py\")):\n",
    "    print(\"read_abf.py found at:\", read_abf_path)\n",
    "else:\n",
    "    print(\"read_abf.py NOT found at:\", read_abf_path)\n",
    "\n",
    "# Now import Abfdata from functions\n",
    "import read_abf as functions \n",
    "\n",
    "\n",
    "def process_abf_data(file_path, color, label):\n",
    "    # Load the data\n",
    "    data = functions.Abfdata(file_path)\n",
    "    base_name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "\n",
    "    # Extract trace and time data\n",
    "    trace_data = data.extract_trace_data()\n",
    "    time_values = data.get_time_values() * 1000  # convert to ms\n",
    "\n",
    "    #EXCLUDE INDICES IF NEEDED \n",
    "    excluded_indices = {}\n",
    "    filtered_indices = [idx for idx in range(len(trace_data)) if idx not in excluded_indices]\n",
    "\n",
    "    # Create a Gaussian window for filtering\n",
    "    std_dev = 5\n",
    "    window_size = 10\n",
    "    window = signal.windows.gaussian(window_size, std_dev)\n",
    "    filtered_trace_data = trace_data[filtered_indices]\n",
    "\n",
    "    plt.figure(figsize=(15, 4))\n",
    "\n",
    "    # Plot pulse data\n",
    "    pulse_data = [data.extract_pulse_data() for _ in filtered_trace_data]\n",
    "    print(\"Pulse data length:\", len(pulse_data))\n",
    "    print(\"Pulse data shape:\", np.array(pulse_data).shape)\n",
    "    print(\"Pulse data type:\", type(pulse_data))\n",
    "    print(\"Pulse data:\", pulse_data)\n",
    "    pulse_data = np.array(pulse_data)\n",
    "    stim_time = []\n",
    "    for pulse in pulse_data:\n",
    "        #plt.plot(time_values, np.array(pulse), color='magenta', alpha=0.9)\n",
    "        # Find peaks in the data\n",
    "        peaks, props = signal.find_peaks(pulse, height=2.1, width=1)  # adjust height as needed\n",
    "        pulse_starts = time_values[props[\"left_ips\"].astype(int)]\n",
    "        pulse_ends   = time_values[props[\"right_ips\"].astype(int)]\n",
    "        #for start, end in zip(pulse_starts, pulse_ends):\n",
    "        #    plt.vlines([start, end], ymin=-200, ymax=200, color=\"paleturquoise\", alpha=0.3)\n",
    "        stim_times = time_values[peaks]\n",
    "    stim_time.append(stim_times)\n",
    "\n",
    "\n",
    "    filt_trace_data = []\n",
    "    for sweep_data in filtered_trace_data:\n",
    "        filtered_sweep_data = signal.convolve(sweep_data, window, mode='same') / sum(window)\n",
    "        filt_trace_data.append(filtered_sweep_data)\n",
    "\n",
    "    # Average sweeps and apply baseline correction\n",
    "    averaged_data = data.average_abf_sweeps()\n",
    "    window = signal.windows.gaussian(5, 2)\n",
    "    averaged_data_data = signal.convolve(averaged_data, window, mode='same') / sum(window)\n",
    "    baseline_averageddata = data.baseline_correction(averaged_data_data)\n",
    "    \n",
    "    fs = 50000  # Replace with your actual sampling rate\n",
    "    f, Pxx = welch(baseline_averageddata, fs, nperseg=2048)\n",
    "    f0 = 50  # Notch filter frequency (Hz)\n",
    "    Q = 300   # Quality factor\n",
    "    b, a = iirnotch(f0, Q, fs)\n",
    "    cleaned_trace = filtfilt(b, a, baseline_averageddata)\n",
    "\n",
    "    def butter_lowpass(cutoff, fs, order=4):\n",
    "        nyq = 0.5 * fs\n",
    "        normal_cutoff = cutoff / nyq\n",
    "        return butter(order, normal_cutoff, btype='low', analog=False)\n",
    "\n",
    "    cutoff = 20  # or lower, depending on what you want to keep\n",
    "    b, a = butter_lowpass(cutoff, fs)\n",
    "    smoothed_trace = filtfilt(b, a, cleaned_trace)\n",
    "\n",
    "    #COMMENT THIS OUT IF YOU DON'T WANT TO SEE ALL THE TRACES \n",
    "    baseline_cor_data = data.baseline_correction(filt_trace_data)\n",
    "    for i, sweep_data in enumerate(baseline_cor_data[:]):\n",
    "       plt.plot(time_values, sweep_data, alpha=.8, color=color)\n",
    "\n",
    "    # Plot averaged trace\n",
    "    #peak = []\n",
    "    #plt.plot(time_values, smoothed_trace, label=f'{label}', color='darkcyan', alpha=0.8, linewidth=1), \n",
    "    #peaks, _ = signal.find_peaks(baseline_averageddata, height=90,prominence=8)\n",
    "    #peak.append(peaks)\n",
    "\n",
    "    #contour_heights = baseline_averageddata[peaks] - prominences\n",
    "    #plt.plot(peaks, baseline_averageddata[peaks], \"x\")\n",
    "    #plt.xlim(100,1000)\n",
    "    #plt.ylim(-5, 80)\n",
    "    #print(\"Averaged data peaks:\", peaks)\n",
    "    plt.xlabel('Time (ms)')\n",
    "    plt.ylabel('Voltage (mV)')\n",
    "    plt.show()\n",
    "    # Compute more peak details from averaged data\n",
    "    peak_indices, peak_props = signal.find_peaks(baseline_averageddata, height=90, prominence=8)\n",
    "\n",
    "    peak_times = time_values[peak_indices]\n",
    "    peak_amps = baseline_averageddata[peak_indices]\n",
    "    baseline_value = 0  # Or compute your baseline from data if needed\n",
    "\n",
    "    # Duration as width at half prominence\n",
    "    widths, width_heights, left_ips, right_ips = signal.peak_widths(baseline_averageddata, peak_indices, rel_height=0.5)\n",
    "    durations = (right_ips - left_ips) * (time_values[1] - time_values[0])  # in ms\n",
    "\n",
    "    # Replace your return with this:\n",
    "    return {\n",
    "        \"averaged_data\": averaged_data_data,\n",
    "        \"time_values\": time_values,\n",
    "        \"baseline_averaged\": baseline_averageddata,\n",
    "        \"stim_times\": stim_times,\n",
    "        \"stim_t\": stim_time,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "416b456e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], shape=(0, 1487), dtype=float32)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "64254191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done plotting dF/F0\n",
      "[-62.7747 -62.9578 -62.8052 ... -63.446  -63.5681 -63.4766]\n",
      "Pulse data length: 1\n",
      "Pulse data shape: (1, 75000)\n",
      "Pulse data type: <class 'list'>\n",
      "Pulse data: [array([-62.7747, -62.9578, -62.8052, ..., -63.446 , -63.5681, -63.4766],\n",
      "      shape=(75000,), dtype=float32)]\n",
      "done plotting ephys data\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# --- 1. Load data ---\n",
    "export_dir = Path(r'D:/Paris2025/ForceB Undiluted/20251001/cell2/016-SD50Hz-70/suite2p_exported')\n",
    "F = np.load(export_dir / 'fluorescence_corrected.npy')  # shape (1, 1485)\n",
    "F_trace = F.squeeze().T         # now shape (1485,)\n",
    "\n",
    "# --- 2. Define frame ranges ---\n",
    "# adjust these according to your experiment timing\n",
    "light_off_frames = np.arange(1, 100)      # frames with lights off\n",
    "light_on_frames = np.arange(500, 600)     # frames with lights on\n",
    "\n",
    "# --- 3. Compute baselines safely ---\n",
    "F_off = np.mean(F_trace[light_off_frames])\n",
    "F_on_baseline = np.mean(F_trace[light_on_frames]) # first ~50 frames after light on\n",
    "\n",
    "# --- 4. Compute ΔF/F₀ ---\n",
    "F0 = F_on_baseline - F_off \n",
    "dF = F_trace - F_on_baseline\n",
    "dF_over_F0 = (dF / F0) *100  # convert to percentage\n",
    "\n",
    "# --- 5. Optional: Replace NaNs and plot ---\n",
    "dF_over_F0 = np.nan_to_num(dF_over_F0, nan=0.0)\n",
    "\n",
    "plt.figure(figsize=(15, 4))\n",
    "plt.plot(dF_over_F0, label='ΔF/F₀')\n",
    "plt.xlim(0, len(dF_over_F0))\n",
    "#plt.ylim(-50, 150)\n",
    "#plt.axvspan(light_off_frames[0], light_off_frames[-1], color='gray', alpha=0.2, label='Light off')\n",
    "#plt.axvspan(light_on_frames[0], light_on_frames[-1], color='yellow', alpha=0.1, label='Light on')\n",
    "plt.xlabel('Frame')\n",
    "plt.ylabel('ΔF/F₀')\n",
    "plt.show()\n",
    "print('done plotting dF/F0')\n",
    "\n",
    "plt.figure(figsize=(15, 4))\n",
    "process_abf_data(\"D:/Paris2025/Ephys/011025/2025_10_01_0016.abf\", \"darkcyan\", \"ForceA\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "print('done plotting ephys data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18c47d25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-100.      -98.6989  -97.6939 ...  -97.7437  -97.605   -96.9193]\n"
     ]
    }
   ],
   "source": [
    "#F_trace[light_on_frames]\n",
    "dF_over_F0 = (dF / F0) *100  # convert to percentage\n",
    "dF_over_F0 = np.nan_to_num(dF_over_F0, nan=0.0)\n",
    "print(dF_over_F0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dda23f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_overlapping_segments(trace, segment_length, step, start_frame=0):\n",
    "    \"\"\"\n",
    "    Plot overlapping segments of a 1D fluorescence trace.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    trace : array-like\n",
    "        The fluorescence trace (e.g., ΔF/F0 or raw F)\n",
    "    segment_length : int\n",
    "        Number of frames per segment (x-axis duration)\n",
    "    step : int\n",
    "        Number of frames between starts of consecutive segments\n",
    "    start_frame : int\n",
    "        Optional starting frame index (default = 0)\n",
    "    \"\"\"\n",
    "\n",
    "    trace = np.squeeze(trace)\n",
    "    n_frames = len(trace)\n",
    "    segments = []\n",
    "\n",
    "    # Extract all segments\n",
    "    for i in range(start_frame, n_frames - segment_length, step):\n",
    "        segment = trace[i:i + segment_length]\n",
    "        segments.append(segment)\n",
    "\n",
    "    # Convert to array for plotting\n",
    "    segments = np.array(segments)\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    for seg in segments:\n",
    "        plt.plot(np.arange(segment_length), seg, alpha=0.8)\n",
    "    plt.xlabel(\"Frame\")\n",
    "    plt.ylabel(\"ΔF/F₀ or counts\")\n",
    "    plt.title(f\"Overlapping segments (len={segment_length}, step={step})\")\n",
    "    plt.show()\n",
    "\n",
    "    return segments\n",
    "\n",
    "\n",
    "# --- Example usage ---\n",
    "# trace = np.load(\"your_trace.npy\").squeeze()\n",
    "# Adjust these according to your recording\n",
    "segment_length = 590   # how long each trial lasts (frames)\n",
    "step = segment_length + 395             # how many frames apart each trial starts\n",
    "segments = plot_overlapping_segments(F, segment_length, step)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8015213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected 1 events at frames: [276]\n"
     ]
    }
   ],
   "source": [
    "from scipy.signal import find_peaks\n",
    "\n",
    "def detect_events(trace, prominence=0.1, distance=500):\n",
    "    \"\"\"\n",
    "    Detect events automatically from a fluorescence trace.\n",
    "    Adjust 'prominence' and 'distance' for your data.\n",
    "    \"\"\"\n",
    "    # Invert trace if your response is downward\n",
    "    inverted = -trace  \n",
    "\n",
    "    # Detect peaks (large drops)\n",
    "    peaks, _ = find_peaks(inverted, prominence=prominence, distance=distance)\n",
    "    return peaks\n",
    "\n",
    "\n",
    "def plot_aligned_segments(trace, event_indices, pre_frames=100, post_frames=590):\n",
    "    \"\"\"\n",
    "    Extract and plot segments aligned to detected event onsets.\n",
    "    \"\"\"\n",
    "    trace = np.squeeze(trace)\n",
    "    n = len(trace)\n",
    "    segments = []\n",
    "\n",
    "    for idx in event_indices:\n",
    "        start = max(idx - pre_frames, 0)\n",
    "        end = min(idx + post_frames, n)\n",
    "        seg = trace[start:end]\n",
    "        \n",
    "        # Pad if needed (so all segments same length)\n",
    "        if len(seg) < pre_frames + post_frames:\n",
    "            seg = np.pad(seg, (0, pre_frames + post_frames - len(seg)), constant_values=np.nan)\n",
    "        \n",
    "        segments.append(seg)\n",
    "\n",
    "    segments = np.array(segments)\n",
    "\n",
    "    # Plot all segments\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    time_axis = np.arange(-pre_frames, post_frames)\n",
    "    for seg in segments:\n",
    "        plt.plot(time_axis, seg, alpha=0.6)\n",
    "    plt.xlabel(\"Frame (aligned to event onset)\")\n",
    "    plt.ylabel(\"ΔF/F₀ or counts\")\n",
    "    plt.title(\"Automatically aligned fluorescence responses\")\n",
    "    plt.show()\n",
    "\n",
    "    return segments\n",
    "\n",
    "\n",
    "# --- Example usage ---\n",
    "# trace = np.load(\"your_trace.npy\").squeeze()\n",
    "\n",
    "event_indices = detect_events(F_trace, prominence=0.1, distance=850)\n",
    "print(f\"Detected {len(event_indices)} events at frames: {event_indices}\")\n",
    "\n",
    "segments = plot_aligned_segments(F_trace, event_indices, pre_frames=20, post_frames=600)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e186f78",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'D:\\\\Paris2025\\\\ForceA Diluted x5\\\\20251003\\\\SD70-50-1s\\\\suite2p_exported\\\\fluorescence_manual.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Load data\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m fluorescence \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexport_dir\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfluorescence_manual.npy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Compute SNR in one line\u001b[39;00m\n\u001b[0;32m      5\u001b[0m noise \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mstd(F_trace[light_on_frames])\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\numpy\\lib\\_npyio_impl.py:451\u001b[0m, in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[0;32m    449\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    450\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 451\u001b[0m     fid \u001b[38;5;241m=\u001b[39m stack\u001b[38;5;241m.\u001b[39menter_context(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    452\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    454\u001b[0m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'D:\\\\Paris2025\\\\ForceA Diluted x5\\\\20251003\\\\SD70-50-1s\\\\suite2p_exported\\\\fluorescence_manual.npy'"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "fluorescence = np.load(export_dir / 'fluorescence_manual.npy')\n",
    "\n",
    "# Compute SNR in one line\n",
    "noise = np.std(F_trace[light_on_frames])\n",
    "signal = np.max(F_trace[30:500]) - F_on_baseline\n",
    "snr = (signal / noise) \n",
    "\n",
    "print(f\"SNR: {snr}\")\n",
    "print(f\"Mean SNR: {np.mean(snr):.2f}\")\n",
    "F_trace[50:500]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

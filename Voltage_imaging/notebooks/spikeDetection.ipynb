{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798cfe0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tifffile\n",
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Using device:\", device)\n",
    "import os\n",
    "os.environ['MPLBACKEND'] = 'Qt5Agg'  # Set backend before importing matplotlib\n",
    "import matplotlib\n",
    "matplotlib.use('Qt5Agg')  # Explicitly set backend\n",
    "import matplotlib.pyplot as plt\n",
    "#from matplotlib.backends.backend_tkagg import FigureCanvasTkAgg\n",
    "#import PySimpleGUI as sg\n",
    "from cellmincer.denoise import main as cm_denoise\n",
    "print(dir(cm_denoise))\n",
    "from cellmincer.models.spatial_unet_2d_temporal_denoiser import SpatialUnet2dTemporalDenoiser\n",
    "\n",
    "# COMPLETE configuration with ALL required keys\n",
    "config = {\n",
    "    'type': 'spatial_unet_2d_temporal_denoiser',  # REQUIRED\n",
    "    \n",
    "    # Spatial U-Net parameters\n",
    "    'n_global_features': 1,\n",
    "    'spatial_unet_depth': 4,\n",
    "    'spatial_unet_first_conv_channels': 32,\n",
    "    'spatial_unet_padding': True,\n",
    "    'spatial_unet_batch_norm': True,\n",
    "    'spatial_unet_attention': False,\n",
    "    'spatial_unet_feature_mode': 'repeat',  # 'repeat', 'once', or 'none'\n",
    "    'spatial_unet_kernel_size': 3,\n",
    "    'spatial_unet_n_conv_layers': 2,\n",
    "    'spatial_unet_readout_kernel_size': 1,\n",
    "    'spatial_unet_activation': 'relu',\n",
    "    \n",
    "    # Temporal denoiser parameters\n",
    "    'temporal_denoiser_kernel_size': 3,\n",
    "    'temporal_denoiser_conv_channels': 32,\n",
    "    'temporal_denoiser_hidden_dense_layer_dims': [64],\n",
    "    'temporal_denoiser_activation': 'relu',\n",
    "    'temporal_denoiser_n_conv_layers': 2,  # This is used in get_temporal_order_from_config\n",
    "}\n",
    "\n",
    "# Now create the model\n",
    "model = SpatialUnet2dTemporalDenoiser(config=config)\n",
    "print(\"Model created successfully!\")\n",
    "\n",
    "def load_tif(file_path, fps):\n",
    "    \"\"\"\n",
    "    Load imaging stack (TIFF).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    file_path : str\n",
    "        Path to the TIFF stack.\n",
    "    fps : float\n",
    "        Imaging frame rate (Hz).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    stack : np.ndarray\n",
    "        Image stack (frames, height, width).\n",
    "    frame_times : np.ndarray\n",
    "        Time vector for each frame (seconds).\n",
    "    \"\"\"\n",
    "    stack = tifffile.imread(file_path)  # shape: (frames, h, w)\n",
    "    n_frames = stack.shape[0]\n",
    "    frame_times = np.arange(n_frames) / fps\n",
    "\n",
    "    return stack, frame_times\n",
    "\n",
    "FPS = 1000\n",
    "tiff_stack, frame_times = load_tif(\"D:/Paris2025/ForceB Undiluted/20251001/cell2/014-SD10hz-70/014-SD10hz-70_NDTiffStack.tif\", FPS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248431af",
   "metadata": {},
   "source": [
    "## Train Model with Torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "8febd4db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded TIFF stack shape: (1481, 89, 90)\n",
      "Data type: float32\n",
      "Training on data shape: (50, 89, 90)\n",
      "Training model...\n",
      "Epoch 0, Average Loss: 0.015163\n",
      "Epoch 10, Average Loss: 0.006106\n",
      "Epoch 20, Average Loss: 0.004018\n",
      "Denoising...\n",
      "Denoised movie saved to: D:/Paris2025/ForceB Undiluted/20251001/cell2/014-SD10hz-70/denoised_movie.tif\n",
      "Model saved to: D:/Paris2025/ForceB Undiluted/20251001/cell2/014-SD10hz-70/trained_denoiser.pth\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import tifffile\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load your TIFF stack\n",
    "tiff_path = \"D:/Paris2025/ForceB Undiluted/20251001/cell2/014-SD10hz-70/014-SD10hz-70_NDTiffStack.tif\"\n",
    "tiff_stack = tifffile.imread(tiff_path)\n",
    "print(f\"Loaded TIFF stack shape: {tiff_stack.shape}\")\n",
    "\n",
    "# Normalize and convert to float32\n",
    "def normalize_data(data):\n",
    "    data_min = data.min()\n",
    "    data_max = data.max()\n",
    "    if data_max - data_min > 0:\n",
    "        return (data - data_min) / (data_max - data_min)\n",
    "    return data\n",
    "\n",
    "# Convert to float32 explicitly\n",
    "tiff_stack_normalized = normalize_data(tiff_stack.astype(np.float32))\n",
    "print(f\"Data type: {tiff_stack_normalized.dtype}\")\n",
    "\n",
    "# Use a smaller subset for faster training\n",
    "if tiff_stack_normalized.shape[0] > 50:\n",
    "    train_data = tiff_stack_normalized[30:80]\n",
    "else:\n",
    "    train_data = tiff_stack_normalized\n",
    "\n",
    "print(f\"Training on data shape: {train_data.shape}\")\n",
    "\n",
    "# Create a simple frame denoiser\n",
    "class SimpleFrameDenoiser(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleFrameDenoiser, self).__init__()\n",
    "        self.encoder = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(1, 16, 3, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(16, 32, 3, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(32, 64, 3, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "        )\n",
    "        self.decoder = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(64, 32, 3, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(32, 16, 3, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(16, 1, 3, padding=1),\n",
    "            torch.nn.Sigmoid()  # Output in [0,1] range\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        return self.decoder(x)\n",
    "\n",
    "def train_simple_frame_denoiser(movie_data, num_epochs=30):\n",
    "    \"\"\"Train a simple frame-by-frame denoiser\"\"\"\n",
    "    T, H, W = movie_data.shape\n",
    "    \n",
    "    model = SimpleFrameDenoiser()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        # Train on individual frames\n",
    "        for frame_idx in range(T):\n",
    "            # Get single frame\n",
    "            clean_frame = movie_data[frame_idx:frame_idx+1]  # Keep as [1, H, W]\n",
    "            \n",
    "            # Add noise\n",
    "            noise = 0.1 * np.random.randn(*clean_frame.shape).astype(np.float32)\n",
    "            noisy_frame = np.clip(clean_frame + noise, 0, 1)\n",
    "            \n",
    "            # Convert to tensor - ensure float32\n",
    "            noisy_tensor = torch.from_numpy(noisy_frame).unsqueeze(0).float()  # [1, 1, H, W]\n",
    "            clean_tensor = torch.from_numpy(clean_frame).unsqueeze(0).float()  # [1, 1, H, W]\n",
    "            \n",
    "            # Forward pass\n",
    "            output = model(noisy_tensor)\n",
    "            loss = criterion(output, clean_tensor)\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            avg_loss = epoch_loss / T\n",
    "            print(f\"Epoch {epoch}, Average Loss: {avg_loss:.6f}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "def denoise_frames_simple(model, movie_data):\n",
    "    \"\"\"Denoise frames using simple model\"\"\"\n",
    "    model.eval()\n",
    "    T, H, W = movie_data.shape\n",
    "    denoised_frames = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for frame_idx in range(T):\n",
    "            frame = movie_data[frame_idx:frame_idx+1]  # [1, H, W]\n",
    "            input_tensor = torch.from_numpy(frame).unsqueeze(0).float()  # [1, 1, H, W]\n",
    "            output_tensor = model(input_tensor)\n",
    "            denoised_frame = output_tensor.numpy()[0, 0]  # [H, W]\n",
    "            denoised_frames.append(denoised_frame)\n",
    "    \n",
    "    return np.array(denoised_frames)\n",
    "\n",
    "# Alternative: Batch processing version (faster)\n",
    "def train_batch_denoiser(movie_data, num_epochs=30):\n",
    "    \"\"\"Train using batch processing\"\"\"\n",
    "    T, H, W = movie_data.shape\n",
    "    \n",
    "    model = SimpleFrameDenoiser()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        \n",
    "        # Process in batches\n",
    "        batch_size = min(8, T)  # Smaller batch size for memory\n",
    "        num_batches = (T + batch_size - 1) // batch_size\n",
    "        \n",
    "        for batch_idx in range(num_batches):\n",
    "            start_idx = batch_idx * batch_size\n",
    "            end_idx = min((batch_idx + 1) * batch_size, T)\n",
    "            \n",
    "            # Get batch of frames\n",
    "            clean_batch = movie_data[start_idx:end_idx]  # [batch_size, H, W]\n",
    "            \n",
    "            # Add noise to entire batch\n",
    "            noise = 0.1 * np.random.randn(*clean_batch.shape).astype(np.float32)\n",
    "            noisy_batch = np.clip(clean_batch + noise, 0, 1)\n",
    "            \n",
    "            # Convert to tensors - shape: [batch_size, 1, H, W]\n",
    "            noisy_tensor = torch.from_numpy(noisy_batch).unsqueeze(1).float()\n",
    "            clean_tensor = torch.from_numpy(clean_batch).unsqueeze(1).float()\n",
    "            \n",
    "            # Forward pass\n",
    "            output = model(noisy_tensor)\n",
    "            loss = criterion(output, clean_tensor)\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            avg_loss = total_loss / num_batches\n",
    "            print(f\"Epoch {epoch}, Average Loss: {avg_loss:.6f}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "def denoise_batch_simple(model, movie_data):\n",
    "    \"\"\"Denoise using batch processing\"\"\"\n",
    "    model.eval()\n",
    "    T, H, W = movie_data.shape\n",
    "    \n",
    "    denoised_frames = []\n",
    "    batch_size = min(16, T)  # Process in batches\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for start_idx in range(0, T, batch_size):\n",
    "            end_idx = min(start_idx + batch_size, T)\n",
    "            batch_data = movie_data[start_idx:end_idx]\n",
    "            \n",
    "            # Convert to tensor\n",
    "            input_tensor = torch.from_numpy(batch_data).unsqueeze(1).float()  # [batch, 1, H, W]\n",
    "            \n",
    "            # Denoise\n",
    "            output_tensor = model(input_tensor)\n",
    "            denoised_batch = output_tensor.numpy()[:, 0]  # [batch, H, W]\n",
    "            \n",
    "            denoised_frames.extend(denoised_batch)\n",
    "    \n",
    "    return np.array(denoised_frames)\n",
    "\n",
    "# Main execution\n",
    "print(\"Training model...\")\n",
    "# Try batch processing for faster training\n",
    "trained_model = train_batch_denoiser(train_data, num_epochs=30)\n",
    "\n",
    "print(\"Denoising...\")\n",
    "denoised_data = denoise_batch_simple(trained_model, train_data)\n",
    "\n",
    "# Save results\n",
    "output_path = \"D:/Paris2025/ForceB Undiluted/20251001/cell2/014-SD10hz-70/denoised_movie.tif\"\n",
    "tifffile.imwrite(output_path, denoised_data.astype(np.float32))\n",
    "print(f\"Denoised movie saved to: {output_path}\")\n",
    "\n",
    "# Save the trained model\n",
    "model_save_path = \"D:/Paris2025/ForceB Undiluted/20251001/cell2/014-SD10hz-70/trained_denoiser.pth\"\n",
    "torch.save(trained_model.state_dict(), model_save_path)\n",
    "print(f\"Model saved to: {model_save_path}\")\n",
    "\n",
    "# Visualize results\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Original frame\n",
    "axes[0].imshow(train_data[0], cmap='gray')\n",
    "axes[0].set_title('Original Frame 0')\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Denoised frame\n",
    "axes[1].imshow(denoised_data[0], cmap='gray')\n",
    "axes[1].set_title('Denoised Frame 0')\n",
    "axes[1].axis('off')\n",
    "\n",
    "# Difference\n",
    "axes[2].imshow(train_data[0] - denoised_data[0], cmap='gray')\n",
    "axes[2].set_title('Noise Removed')\n",
    "axes[2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0370a2dd",
   "metadata": {},
   "source": [
    "## Denoise Movie with CellMincer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "82fd0eaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n",
      "Loading video: D:/Paris2025/ForceB Undiluted/20251001/cell2/014-SD10hz-70/014-SD10hz-70_NDTiffStack.tif\n",
      "Full video shape: (1481, 89, 90)\n",
      "Processing 1481 frames in chunks of 100...\n",
      "Processing frames 0 to 99...\n",
      "Processing frames 100 to 199...\n",
      "Processing frames 200 to 299...\n",
      "Processing frames 300 to 399...\n",
      "Processing frames 400 to 499...\n",
      "Processing frames 500 to 599...\n",
      "Processing frames 600 to 699...\n",
      "Processing frames 700 to 799...\n",
      "Processing frames 800 to 899...\n",
      "Processing frames 900 to 999...\n",
      "Processing frames 1000 to 1099...\n",
      "Processing frames 1100 to 1199...\n",
      "Processing frames 1200 to 1299...\n",
      "Processing frames 1300 to 1399...\n",
      "Processing frames 1400 to 1480...\n",
      "Denoised video saved to: D:/Paris2025/ForceB Undiluted/20251001/cell2/014-SD10hz-70/014-SD10hz-70_denoised_full.tif\n",
      "Done processing full video!\n"
     ]
    }
   ],
   "source": [
    "class SimpleFrameDenoiser(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleFrameDenoiser, self).__init__()\n",
    "        self.encoder = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(1, 16, 3, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(16, 32, 3, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(32, 64, 3, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "        )\n",
    "        self.decoder = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(64, 32, 3, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(32, 16, 3, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(16, 1, 3, padding=1),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        return self.decoder(x)\n",
    "\n",
    "def denoise_large_video(model, video_path, output_path, chunk_size=100):\n",
    "    \"\"\"Denoise a large video in chunks to avoid memory issues\"\"\"\n",
    "    \n",
    "    # Load the full video\n",
    "    print(f\"Loading video: {video_path}\")\n",
    "    full_video = tifffile.imread(video_path)\n",
    "    print(f\"Full video shape: {full_video.shape}\")\n",
    "    \n",
    "    # Normalize\n",
    "    def normalize_data(data):\n",
    "        data_min = data.min()\n",
    "        data_max = data.max()\n",
    "        if data_max - data_min > 0:\n",
    "            return (data - data_min) / (data_max - data_min)\n",
    "        return data\n",
    "    \n",
    "    full_video = normalize_data(full_video.astype(np.float32))\n",
    "    \n",
    "    model.eval()\n",
    "    denoised_chunks = []\n",
    "    \n",
    "    total_frames = full_video.shape[0]\n",
    "    print(f\"Processing {total_frames} frames in chunks of {chunk_size}...\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for start_idx in range(0, total_frames, chunk_size):\n",
    "            end_idx = min(start_idx + chunk_size, total_frames)\n",
    "            chunk = full_video[start_idx:end_idx]\n",
    "            \n",
    "            print(f\"Processing frames {start_idx} to {end_idx-1}...\")\n",
    "            \n",
    "            # Convert to tensor and denoise\n",
    "            input_tensor = torch.from_numpy(chunk).unsqueeze(1).float()  # [chunk_size, 1, H, W]\n",
    "            output_tensor = model(input_tensor)\n",
    "            denoised_chunk = output_tensor.numpy()[:, 0]  # [chunk_size, H, W]\n",
    "            \n",
    "            denoised_chunks.append(denoised_chunk)\n",
    "            \n",
    "            # Clear memory\n",
    "            del input_tensor, output_tensor\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "    \n",
    "    # Combine all chunks\n",
    "    denoised_full = np.vstack(denoised_chunks)\n",
    "    \n",
    "    # Save result\n",
    "    tifffile.imwrite(output_path, denoised_full.astype(np.float32))\n",
    "    print(f\"Denoised video saved to: {output_path}\")\n",
    "    \n",
    "    return denoised_full\n",
    "\n",
    "# Load your trained model\n",
    "model = SimpleFrameDenoiser()\n",
    "model.load_state_dict(torch.load(model_save_path))\n",
    "print(\"Model loaded successfully!\")\n",
    "\n",
    "# Process your full video\n",
    "input_video = \"D:/Paris2025/ForceB Undiluted/20251001/cell2/014-SD10hz-70/014-SD10hz-70_NDTiffStack.tif\"\n",
    "output_video = \"D:/Paris2025/ForceB Undiluted/20251001/cell2/014-SD10hz-70/014-SD10hz-70_denoised_full.tif\"\n",
    "\n",
    "denoised_full = denoise_large_video(model, input_video, output_video, chunk_size=100)\n",
    "print(\"Done processing full video!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e39f8b4",
   "metadata": {},
   "source": [
    "## Segmentation with CellPose vs. Manual Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6560daa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "SUITE2P VOLTAGE IMAGING PIPELINE WITH MANUAL FALLBACK\n",
      "============================================================\n",
      "Suite2p configured for voltage imaging:\n",
      "  Frame rate: 1000 Hz\n",
      "  Tau: 0.0028 s\n",
      "  Diameter: 75 pixels\n",
      "  Spatial high-pass: 100 pixels\n",
      "\n",
      "============================================================\n",
      "Running Suite2p...\n",
      "============================================================\n",
      "Input: D:\\Paris2025\\ForceB Undiluted\\20251001\\cell2\\014-SD10hz-70\\014-SD10hz-70_denoised_full.tif\n",
      "Output: D:\\Paris2025\\ForceB Undiluted\\20251001\\cell2\\014-SD10hz-70\\suite2p_output\n",
      "{'data_path': ['D:\\\\Paris2025\\\\ForceB Undiluted\\\\20251001\\\\cell2\\\\014-SD10hz-70'], 'tiff_list': ['D:\\\\Paris2025\\\\ForceB Undiluted\\\\20251001\\\\cell2\\\\014-SD10hz-70\\\\014-SD10hz-70_denoised_full.tif'], 'save_path0': 'D:\\\\Paris2025\\\\ForceB Undiluted\\\\20251001\\\\cell2\\\\014-SD10hz-70\\\\suite2p_output'}\n",
      "FOUND BINARIES AND OPS IN ['D:\\\\Paris2025\\\\ForceB Undiluted\\\\20251001\\\\cell2\\\\014-SD10hz-70\\\\suite2p_output\\\\suite2p\\\\plane0\\\\ops.npy']\n",
      "removing previous detection and extraction files, if present\n",
      ">>>>>>>>>>>>>>>>>>>>> PLANE 0 <<<<<<<<<<<<<<<<<<<<<<\n",
      "NOTE: not running registration, ops['do_registration']=0\n",
      "binary path: D:\\Paris2025\\ForceB Undiluted\\20251001\\cell2\\014-SD10hz-70\\suite2p_output\\suite2p\\plane0\\data.bin\n",
      "NOTE: Applying builtin classifier at C:\\Users\\sofik\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\suite2p\\classifiers\\classifier.npy\n",
      "----------- ROI DETECTION\n",
      "Binning movie in chunks of length 03\n",
      "Binned movie of size [493,89,90] created in 0.27 sec.\n",
      "NOTE: FORCED spatial scale ~6 pixels, time epochs 1.00, threshold 5.00 \n",
      "Detected 0 ROIs, 0.59 sec\n",
      "Suite2p failed with error: no ROIs were found -- check registered binary and maybe change spatial scale\n",
      "Falling back to manual selection...\n",
      "\n",
      "============================================================\n",
      "MANUAL ROI SELECTION\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\sofik\\AppData\\Local\\Temp\\ipykernel_23324\\569607799.py\", line 108, in run_suite2p_with_manual_fallback\n",
      "    output_ops = run_s2p(ops=self.ops, db=db)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\sofik\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\suite2p\\run_s2p.py\", line 550, in run_s2p\n",
      "    op = run_plane(op, ops_path=ops_path)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\sofik\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\suite2p\\run_s2p.py\", line 342, in run_plane\n",
      "    ops = pipeline(f_reg, f_raw, f_reg_chan2, f_raw_chan2, run_registration, ops,\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\sofik\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\suite2p\\run_s2p.py\", line 171, in pipeline\n",
      "    ops, stat = detection.detection_wrapper(f_reg, ops=ops, classfile=classfile)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\sofik\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\suite2p\\detection\\detect.py\", line 178, in detection_wrapper\n",
      "    stat = select_rois(\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\sofik\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\suite2p\\detection\\detect.py\", line 252, in select_rois\n",
      "    raise ValueError(\n",
      "ValueError: no ROIs were found -- check registered binary and maybe change spatial scale\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\sofik\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\matplotlib\\cbook.py\", line 361, in process\n",
      "    func(*args, **kwargs)\n",
      "  File \"C:\\Users\\sofik\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\matplotlib\\widgets.py\", line 2276, in release\n",
      "    self._release(event)\n",
      "  File \"C:\\Users\\sofik\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\matplotlib\\widgets.py\", line 3297, in _release\n",
      "    self.onselect(self._eventpress, self._eventrelease)\n",
      "  File \"C:\\Users\\sofik\\AppData\\Local\\Temp\\ipykernel_23324\\569607799.py\", line 171, in onselect\n",
      "    rect = plt.Circle((xmin, ymin), width, height,\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: Circle.__init__() takes from 2 to 3 positional arguments but 4 were given\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manual traces shape: (1, 1481)\n",
      "Successfully extracted traces from 1 manual ROIs\n",
      "\n",
      "============================================================\n",
      "Processing complete!\n",
      "============================================================\n",
      "Saved manual ROI visualization to D:\\Paris2025\\ForceB Undiluted\\20251001\\cell2\\014-SD10hz-70\\suite2p_rois_manual.png\n",
      "Using manual ROI traces (no neuropil correction available)\n",
      "Saved traces to D:\\Paris2025\\ForceB Undiluted\\20251001\\cell2\\014-SD10hz-70\\suite2p_traces_manual.png\n",
      "\n",
      "Exported MANUAL results to D:\\Paris2025\\ForceB Undiluted\\20251001\\cell2\\014-SD10hz-70\\suite2p_exported:\n",
      "  - fluorescence_manual.npy: (1, 1481)\n",
      "  - dff_manual.npy: (1, 1481)\n",
      "  - roi_masks.npy: 1 masks\n",
      "  - metadata.npy\n",
      "\n",
      "============================================================\n",
      "PIPELINE COMPLETE! (Mode: MANUAL)\n",
      "============================================================\n",
      "Using manual ROI traces (no neuropil correction available)\n",
      "Using manual ROI traces (no neuropil correction available)\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from suite2p import run_s2p, default_ops\n",
    "from matplotlib.widgets import EllipseSelector\n",
    "import tifffile\n",
    "\n",
    "class Suite2pVoltagePipeline:\n",
    "    \"\"\"\n",
    "    Complete Suite2p pipeline optimized for voltage imaging with manual fallback.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_path, tiff_file, output_dir='./suite2p_output'):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        data_path : str\n",
    "            Directory containing the TIFF file\n",
    "        tiff_file : str\n",
    "            Name of the TIFF file (e.g., 'denoised_movie.tif')\n",
    "        output_dir : str\n",
    "            Where to save Suite2p results\n",
    "        \"\"\"\n",
    "        self.data_path = Path(data_path)\n",
    "        self.tiff_file = tiff_file\n",
    "        self.output_dir = Path(output_dir)\n",
    "        self.ops = None\n",
    "        self.stat = None\n",
    "        self.F = None\n",
    "        self.Fneu = None\n",
    "        self.spks = None\n",
    "        self.iscell = None\n",
    "        self.manual_mode = False  # Track if we used manual selection\n",
    "        self.manual_traces = None\n",
    "        self.manual_masks = None\n",
    "        \n",
    "    def configure_ops(self, frame_rate=1000, diameter=12, tau=0.0028, \n",
    "                     spatial_hp=100, threshold_scaling=1.0, max_iterations=50):\n",
    "        \"\"\"\n",
    "        Configure Suite2p parameters optimized for voltage imaging.\n",
    "        \"\"\"\n",
    "        # Start with default ops\n",
    "        self.ops = default_ops()\n",
    "        \n",
    "        # ============ CRITICAL VOLTAGE IMAGING SETTINGS ============\n",
    "        self.ops['fs'] = frame_rate\n",
    "        self.ops['tau'] = tau\n",
    "        self.ops['diameter'] = diameter\n",
    "        self.ops['spatial_hp_detect'] = spatial_hp\n",
    "        self.ops['spatial_hp_reg'] = 0\n",
    "        self.ops['threshold_scaling'] = threshold_scaling\n",
    "        self.ops['max_iterations'] = max_iterations\n",
    "        self.ops['high_pass'] = 100\n",
    "        self.ops['allow_overlap'] = True\n",
    "        self.ops['max_overlap'] = 0.4\n",
    "        self.ops['inner_neuropil_radius'] = 2\n",
    "        self.ops['min_neuropil_pixels'] = 200\n",
    "        self.ops['do_registration'] = False\n",
    "        self.ops['two_step_registration'] = True\n",
    "        self.ops['keep_movie_raw'] = False\n",
    "        self.ops['smooth_sigma'] = 1.15\n",
    "        self.ops['classifier_path'] = None\n",
    "        self.ops['batch_size'] = 500\n",
    "        self.ops['num_workers'] = 0\n",
    "        self.ops['save_mat'] = False\n",
    "        self.ops['save_NWB'] = False\n",
    "        self.ops['combined'] = False\n",
    "        \n",
    "        print(\"Suite2p configured for voltage imaging:\")\n",
    "        print(f\"  Frame rate: {frame_rate} Hz\")\n",
    "        print(f\"  Tau: {tau} s\")\n",
    "        print(f\"  Diameter: {diameter} pixels\")\n",
    "        print(f\"  Spatial high-pass: {spatial_hp} pixels\")\n",
    "        \n",
    "        return self.ops\n",
    "    \n",
    "    def run_suite2p_with_manual_fallback(self, enable_manual_fallback=True):\n",
    "        \"\"\"\n",
    "        Run Suite2p with automatic fallback to manual selection.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        enable_manual_fallback : bool\n",
    "            If True, automatically fall back to manual selection when no cells are found\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        output_path : Path\n",
    "            Path to output directory\n",
    "        \"\"\"\n",
    "        if self.ops is None:\n",
    "            print(\"No ops configured, using defaults for voltage imaging...\")\n",
    "            self.configure_ops()\n",
    "        \n",
    "        # Set up database\n",
    "        db = {\n",
    "            'data_path': [str(self.data_path)],\n",
    "            'tiff_list': [str(self.data_path / self.tiff_file)],\n",
    "            'save_path0': str(self.output_dir), \n",
    "        }\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"Running Suite2p...\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"Input: {self.data_path / self.tiff_file}\")\n",
    "        print(f\"Output: {self.output_dir}\")\n",
    "        \n",
    "        # Run Suite2p\n",
    "        try:\n",
    "            output_ops = run_s2p(ops=self.ops, db=db)\n",
    "            self.load_results()\n",
    "            \n",
    "            # Check if any cells were found\n",
    "            n_cells = np.sum(self.iscell[:, 0] > 0)\n",
    "            print(f\"Suite2p found {n_cells} cells\")\n",
    "            \n",
    "            if n_cells == 0 and enable_manual_fallback:\n",
    "                print(\"No cells found automatically. Falling back to manual selection...\")\n",
    "                self.run_manual_selection()\n",
    "                self.manual_mode = True\n",
    "            else:\n",
    "                self.manual_mode = False\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Suite2p failed with error: {e}\")\n",
    "            if enable_manual_fallback:\n",
    "                print(\"Falling back to manual selection...\")\n",
    "                self.run_manual_selection()\n",
    "                self.manual_mode = True\n",
    "            else:\n",
    "                raise e\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"Processing complete!\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        return self.output_dir / 'suite2p' / 'plane0'\n",
    "    \n",
    "    def run_manual_selection(self):\n",
    "        \"\"\"\n",
    "        Run manual ROI selection when automatic detection fails.\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"MANUAL ROI SELECTION\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Load the movie\n",
    "        movie_path = self.data_path / self.tiff_file\n",
    "        movie = tifffile.imread(movie_path)\n",
    "        mean_image = np.max(movie, axis=0)\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(12, 10))\n",
    "        ax.imshow(mean_image, cmap='gray')\n",
    "        ax.set_title('Click and drag to select ROIs. Press Enter when done.')\n",
    "        \n",
    "        rois = []  # List of (y, x, height, width)\n",
    "        \n",
    "        def onselect(eclick, erelease):\n",
    "            \"\"\"Callback for rectangle selection\"\"\"\n",
    "            x1, y1 = int(eclick.xdata), int(eclick.ydata)\n",
    "            x2, y2 = int(erelease.xdata), int(erelease.ydata)\n",
    "            \n",
    "            # Ensure coordinates are ordered\n",
    "            xmin, xmax = min(x1, x2), max(x1, x2)\n",
    "            ymin, ymax = min(y1, y2), max(y1, y2)\n",
    "            \n",
    "            width = xmax - xmin\n",
    "            height = ymax - ymin\n",
    "            \n",
    "            rois.append((ymin, xmin, height, width))\n",
    "            \n",
    "            # Draw the rectangle\n",
    "            rect = plt.Circle((xmin, ymin), width, height, \n",
    "                               fill=False, edgecolor='red', linewidth=2)\n",
    "            ax.add_patch(rect)\n",
    "            plt.draw()\n",
    "            \n",
    "            print(f\"Added ROI {len(rois)}: pos=({xmin}, {ymin}), size=({width}, {height})\")\n",
    "        \n",
    "        def on_key(event):\n",
    "            \"\"\"Finish selection on Enter key\"\"\"\n",
    "            if event.key == 'enter':\n",
    "                plt.close()\n",
    "                print(f\"\\nSelected {len(rois)} ROIs\")\n",
    "        \n",
    "        # Create selector\n",
    "        rs = EllipseSelector(ax, onselect, useblit=True,\n",
    "                             button=[1], minspanx=5, minspany=5,\n",
    "                             spancoords='pixels', interactive=True)\n",
    "        \n",
    "        fig.canvas.mpl_connect('key_press_event', on_key)\n",
    "        plt.show()\n",
    "        \n",
    "        if rois:\n",
    "            self.extract_manual_traces(movie, rois)\n",
    "            print(f\"Successfully extracted traces from {len(rois)} manual ROIs\")\n",
    "        else:\n",
    "            print(\"No ROIs selected manually.\")\n",
    "    \n",
    "    def extract_manual_traces(self, movie, rois):\n",
    "        \"\"\"\n",
    "        Extract fluorescence traces from manual ROIs.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        movie : np.ndarray\n",
    "            Loaded movie data\n",
    "        rois : list\n",
    "            List of ROI coordinates (y, x, height, width)\n",
    "        \"\"\"\n",
    "        height, width = movie.shape[1], movie.shape[2]\n",
    "        masks = []\n",
    "        \n",
    "        # Create binary masks from ROI coordinates\n",
    "        for i, (y, x, h, w) in enumerate(rois):\n",
    "            mask = np.zeros((height, width), dtype=bool)\n",
    "            # Ensure coordinates are within bounds\n",
    "            y_end = min(y + h, height)\n",
    "            x_end = min(x + w, width)\n",
    "            mask[y:y_end, x:x_end] = True\n",
    "            masks.append(mask)\n",
    "        \n",
    "        masks_array = np.array(masks)\n",
    "        \n",
    "        # Extract traces\n",
    "        F_manual = []\n",
    "        for mask in masks_array:\n",
    "            trace = np.mean(movie[:, mask], axis=1)\n",
    "            F_manual.append(trace)\n",
    "        \n",
    "        self.manual_traces = np.array(F_manual)\n",
    "        self.manual_masks = masks_array\n",
    "        \n",
    "        # Create dummy Suite2p-compatible outputs for compatibility\n",
    "        n_rois = len(rois)\n",
    "        n_frames = movie.shape[0]\n",
    "        \n",
    "        self.F = self.manual_traces\n",
    "        self.Fneu = np.zeros_like(self.manual_traces)  # No neuropil for manual\n",
    "        self.spks = np.zeros_like(self.manual_traces)  # No spikes for manual\n",
    "        self.iscell = np.column_stack([np.ones(n_rois), np.ones(n_rois)])  # All are cells\n",
    "        self.stat = self.create_dummy_stat(masks_array)\n",
    "        \n",
    "        print(f\"Manual traces shape: {self.manual_traces.shape}\")\n",
    "    \n",
    "    def create_dummy_stat(self, masks):\n",
    "        \"\"\"\n",
    "        Create dummy stat structure for manual ROIs to maintain compatibility.\n",
    "        \"\"\"\n",
    "        stat = []\n",
    "        for mask in masks:\n",
    "            ypix, xpix = np.where(mask)\n",
    "            stat.append({\n",
    "                'ypix': ypix,\n",
    "                'xpix': xpix,\n",
    "                'lam': np.ones(len(ypix)) / len(ypix),  # Uniform weights\n",
    "                'med': [np.median(ypix), np.median(xpix)],  # Center\n",
    "                'footprint': 1  # Simple footprint\n",
    "            })\n",
    "        return np.array(stat, dtype=object)\n",
    "    \n",
    "    def load_results(self):\n",
    "        \"\"\"Load Suite2p output files.\"\"\"\n",
    "        result_path = self.output_dir / 'suite2p' / 'plane0'\n",
    "        \n",
    "        if not result_path.exists():\n",
    "            raise FileNotFoundError(f\"Suite2p results not found at {result_path}\")\n",
    "        \n",
    "        print(f\"\\nLoading results from {result_path}\")\n",
    "        \n",
    "        # Load main outputs\n",
    "        self.stat = np.load(result_path / 'stat.npy', allow_pickle=True)\n",
    "        self.F = np.load(result_path / 'F.npy')\n",
    "        self.Fneu = np.load(result_path / 'Fneu.npy')\n",
    "        self.spks = np.load(result_path / 'spks.npy')\n",
    "        self.iscell = np.load(result_path / 'iscell.npy')\n",
    "        self.ops = np.load(result_path / 'ops.npy', allow_pickle=True).item()\n",
    "        \n",
    "        n_cells = np.sum(self.iscell[:, 0] > 0)\n",
    "        n_total = len(self.iscell)\n",
    "        \n",
    "        print(f\"Loaded {n_total} ROIs ({n_cells} classified as cells)\")\n",
    "        print(f\"Traces shape: {self.F.shape} (ROIs × frames)\")\n",
    "    \n",
    "    def get_corrected_traces(self, neuropil_coefficient=0.7, cells_only=True):\n",
    "        \"\"\"\n",
    "        Get neuropil-corrected fluorescence traces.\n",
    "        Works for both automatic and manual modes.\n",
    "        \"\"\"\n",
    "        if self.manual_mode:\n",
    "            # For manual mode, just return the raw traces\n",
    "            print(\"Using manual ROI traces (no neuropil correction available)\")\n",
    "            return self.manual_traces\n",
    "        else:\n",
    "            # For automatic mode, use neuropil correction\n",
    "            if self.F is None:\n",
    "                raise ValueError(\"Run Suite2p first or load existing results!\")\n",
    "            \n",
    "            F_corrected = self.F - neuropil_coefficient * self.Fneu\n",
    "            \n",
    "            if cells_only:\n",
    "                cell_mask = self.iscell[:, 0] > 0\n",
    "                F_corrected = F_corrected[cell_mask]\n",
    "                print(f\"Returning {np.sum(cell_mask)} cells (filtered non-cells)\")\n",
    "            \n",
    "            return F_corrected\n",
    "    \n",
    "    def visualize_rois(self, max_display=50, save_path=None):\n",
    "        \"\"\"\n",
    "        Visualize detected ROIs.\n",
    "        Works for both automatic and manual modes.\n",
    "        \"\"\"\n",
    "        if self.manual_mode:\n",
    "            # Manual mode visualization\n",
    "            movie_path = self.data_path / self.tiff_file\n",
    "            movie = tifffile.imread(movie_path)\n",
    "            mean_img = np.mean(movie, axis=0)\n",
    "            \n",
    "            fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "            \n",
    "            # Mean image\n",
    "            axes[0].imshow(mean_img, cmap='gray')\n",
    "            axes[0].set_title('Mean Image')\n",
    "            axes[0].axis('off')\n",
    "            \n",
    "            # ROI overlay\n",
    "            axes[1].imshow(mean_img, cmap='gray')\n",
    "            for i, mask in enumerate(self.manual_masks):\n",
    "                if i >= max_display:\n",
    "                    break\n",
    "                # Plot outline\n",
    "                ypix, xpix = np.where(mask)\n",
    "                if len(ypix) > 0:\n",
    "                    y_min, y_max = ypix.min(), ypix.max()\n",
    "                    x_min, x_max = xpix.min(), xpix.max()\n",
    "                    axes[1].plot([x_min, x_max, x_max, x_min, x_min],\n",
    "                               [y_min, y_min, y_max, y_max, y_min], \n",
    "                               'r-', linewidth=1, alpha=0.8, label=f'ROI {i+1}')\n",
    "            \n",
    "            axes[1].set_title(f'Manual ROIs ({len(self.manual_masks)} cells)')\n",
    "            axes[1].axis('off')\n",
    "            axes[1].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "            \n",
    "        else:\n",
    "            # Automatic mode visualization (your original code)\n",
    "            if self.stat is None:\n",
    "                raise ValueError(\"No ROIs loaded!\")\n",
    "            \n",
    "            result_path = self.output_dir / 'suite2p' / 'plane0'\n",
    "            mean_img = np.load(result_path / 'ops.npy', allow_pickle=True).item()['meanImg']\n",
    "            \n",
    "            roi_img = np.zeros_like(mean_img)\n",
    "            cell_mask = self.iscell[:, 0] > 0\n",
    "            n_cells = np.sum(cell_mask)\n",
    "            \n",
    "            fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "            \n",
    "            axes[0].imshow(mean_img, cmap='gray')\n",
    "            axes[0].set_title('Mean Image')\n",
    "            axes[0].axis('off')\n",
    "            \n",
    "            for i, (st, is_cell) in enumerate(zip(self.stat, self.iscell)):\n",
    "                if i >= max_display:\n",
    "                    break\n",
    "                if is_cell[0] > 0:\n",
    "                    ypix = st['ypix']\n",
    "                    xpix = st['xpix']\n",
    "                    roi_img[ypix, xpix] = i + 1\n",
    "            \n",
    "            axes[1].imshow(roi_img, cmap='nipy_spectral', alpha=0.8)\n",
    "            axes[1].set_title(f'ROIs ({n_cells} cells)')\n",
    "            axes[1].axis('off')\n",
    "            \n",
    "            axes[2].imshow(mean_img, cmap='gray')\n",
    "            for i, (st, is_cell) in enumerate(zip(self.stat, self.iscell)):\n",
    "                if i >= max_display:\n",
    "                    break\n",
    "                if is_cell[0] > 0:\n",
    "                    ypix = st['ypix']\n",
    "                    xpix = st['xpix']\n",
    "                    y_min, y_max = ypix.min(), ypix.max()\n",
    "                    x_min, x_max = xpix.min(), xpix.max()\n",
    "                    axes[2].plot([x_min, x_max, x_max, x_min, x_min],\n",
    "                               [y_min, y_min, y_max, y_max, y_min], \n",
    "                               'r-', linewidth=0.5, alpha=0.7)\n",
    "            \n",
    "            axes[2].set_title('Overlay')\n",
    "            axes[2].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if save_path:\n",
    "            plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "            mode = \"manual\" if self.manual_mode else \"automatic\"\n",
    "            print(f\"Saved {mode} ROI visualization to {save_path}\")\n",
    "        \n",
    "        plt.show()\n",
    "        return fig\n",
    "        \n",
    "    def export_results(self, export_dir='./exported_results'):\n",
    "        \"\"\"\n",
    "        Export results in easy-to-use format.\n",
    "        Works for both automatic and manual modes.\n",
    "        \"\"\"\n",
    "        export_dir = Path(export_dir)\n",
    "        export_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        if self.manual_mode:\n",
    "            # Manual mode export\n",
    "            F_corrected = self.manual_traces\n",
    "            dff = self.compute_dff(F_corrected)\n",
    "            \n",
    "            # Save as numpy arrays\n",
    "            np.save(export_dir / 'fluorescence_manual.npy', F_corrected)\n",
    "            np.save(export_dir / 'dff_manual.npy', dff)\n",
    "            \n",
    "            # Save ROI masks\n",
    "            np.save(export_dir / 'roi_masks.npy', self.manual_masks)\n",
    "            \n",
    "            # Save metadata\n",
    "            metadata = {\n",
    "                'frame_rate': getattr(self.ops, 'fs', 300) if self.ops else 300,\n",
    "                'n_cells': F_corrected.shape[0],\n",
    "                'n_frames': F_corrected.shape[1],\n",
    "                'mode': 'manual',\n",
    "                'export_time': str(np.datetime64('now'))\n",
    "            }\n",
    "            np.save(export_dir / 'metadata.npy', metadata)\n",
    "            \n",
    "            print(f\"\\nExported MANUAL results to {export_dir}:\")\n",
    "            print(f\"  - fluorescence_manual.npy: {F_corrected.shape}\")\n",
    "            print(f\"  - roi_masks.npy: {len(self.manual_masks)} masks\")\n",
    "            \n",
    "        else:\n",
    "            # Automatic mode export (your original code)\n",
    "            F_corrected = self.get_corrected_traces(cells_only=True)\n",
    "            dff = self.compute_dff(F_corrected)\n",
    "            spks = self.spks[self.iscell[:, 0] > 0]\n",
    "            \n",
    "            np.save(export_dir / 'fluorescence_corrected.npy', F_corrected)\n",
    "            np.save(export_dir / 'dff.npy', dff)\n",
    "            np.save(export_dir / 'spikes_deconvolved.npy', spks)\n",
    "            \n",
    "            cells_stat = self.stat[self.iscell[:, 0] > 0]\n",
    "            np.save(export_dir / 'cell_stats.npy', cells_stat)\n",
    "            \n",
    "            metadata = {\n",
    "                'frame_rate': self.ops['fs'],\n",
    "                'n_cells': F_corrected.shape[0],\n",
    "                'n_frames': F_corrected.shape[1],\n",
    "                'diameter': self.ops['diameter'],\n",
    "                'tau': self.ops['tau'],\n",
    "                'mode': 'automatic'\n",
    "            }\n",
    "            np.save(export_dir / 'metadata.npy', metadata)\n",
    "            \n",
    "            print(f\"\\nExported AUTOMATIC results to {export_dir}:\")\n",
    "            print(f\"  - fluorescence_corrected.npy: {F_corrected.shape}\")\n",
    "            print(f\"  - dff.npy: {dff.shape}\")\n",
    "            print(f\"  - spikes_deconvolved.npy: {spks.shape}\")\n",
    "            print(f\"  - cell_stats.npy: {len(cells_stat)} cells\")\n",
    "        \n",
    "        print(f\"  - metadata.npy\")\n",
    "        \n",
    "        return export_dir\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# UPDATED COMPLETE USAGE EXAMPLE\n",
    "# =============================================================================\n",
    "\n",
    "def run_complete_voltage_pipeline(tiff_path, frame_rate=1000, diameter=12, enable_manual_fallback=True):\n",
    "    \"\"\"\n",
    "    One-function pipeline for voltage imaging with automatic manual fallback.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    tiff_path : str\n",
    "        Path to your denoised TIFF file\n",
    "    frame_rate : float\n",
    "        Imaging frame rate in Hz\n",
    "    diameter : int\n",
    "        Expected neuron diameter in pixels\n",
    "    enable_manual_fallback : bool\n",
    "        If True, automatically use manual selection when no cells are found\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    pipeline : Suite2pVoltagePipeline\n",
    "        Pipeline object with all results\n",
    "    \"\"\"\n",
    "    # Parse path\n",
    "    tiff_path = Path(tiff_path)\n",
    "    data_path = tiff_path.parent\n",
    "    tiff_file = tiff_path.name\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"SUITE2P VOLTAGE IMAGING PIPELINE WITH MANUAL FALLBACK\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Initialize\n",
    "    pipeline = Suite2pVoltagePipeline(\n",
    "        data_path=data_path,\n",
    "        tiff_file=tiff_file,\n",
    "        output_dir=data_path / 'suite2p_output'\n",
    "    )\n",
    "    \n",
    "    # Configure for voltage imaging\n",
    "    pipeline.configure_ops(\n",
    "        frame_rate=frame_rate,\n",
    "        diameter=diameter,\n",
    "        tau=0.0028,\n",
    "        spatial_hp=100,\n",
    "        threshold_scaling=1.0\n",
    "    )\n",
    "    \n",
    "    # Run Suite2p with automatic manual fallback\n",
    "    pipeline.run_suite2p_with_manual_fallback(enable_manual_fallback=enable_manual_fallback)\n",
    "    \n",
    "    # Visualize results\n",
    "    mode = \"MANUAL\" if pipeline.manual_mode else \"AUTOMATIC\"\n",
    "    pipeline.visualize_rois(save_path=data_path / f'suite2p_rois_{mode.lower()}.png')\n",
    "    pipeline.plot_traces(n_neurons=10, trace_type='dff', \n",
    "                        save_path=data_path / f'suite2p_traces_{mode.lower()}.png')\n",
    "    \n",
    "    # Export results\n",
    "    pipeline.export_results(export_dir=data_path / 'suite2p_exported')\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"PIPELINE COMPLETE! (Mode: {mode})\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    return pipeline\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # SIMPLE ONE-LINE USAGE WITH MANUAL FALLBACK:\n",
    "    pipeline = run_complete_voltage_pipeline(\n",
    "        tiff_path='D:/Paris2025/ForceB Undiluted/20251001/cell2/014-SD10hz-70/014-SD10hz-70_denoised_full.tif',\n",
    "        frame_rate=1000,\n",
    "        diameter=75,\n",
    "        enable_manual_fallback=True  # This enables the automatic fallback to manual selection\n",
    "    )\n",
    "    \n",
    "    # Access results (works for both automatic and manual modes):\n",
    "    F_corrected = pipeline.get_corrected_traces()\n",
    "    dff = pipeline.compute_dff()\n",
    "    if not pipeline.manual_mode:  # Only available in automatic mode\n",
    "        spikes = pipeline.spks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c999309",
   "metadata": {},
   "source": [
    "## Load ABF file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aac294e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "import os, sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.signal as signal\n",
    "import pyabf\n",
    "from scipy.signal import butter, filtfilt, welch, iirnotch\n",
    "\n",
    "print(\"Current Directory:\", os.getcwd())\n",
    "read_abf_path = r\"C:\\Users\\sofik\\.vscode\\Voltage_imaging\\data_io\"\n",
    "print(\"functions_path:\", read_abf_path)\n",
    "sys.path.append(read_abf_path)\n",
    "\n",
    "# Check if read_abf.py exists at this location\n",
    "if os.path.isfile(os.path.join(read_abf_path, \"read_abf.py\")):\n",
    "    print(\"read_abf.py found at:\", read_abf_path)\n",
    "else:\n",
    "    print(\"read_abf.py NOT found at:\", read_abf_path)\n",
    "\n",
    "# Now import Abfdata from functions\n",
    "import read_abf as functions \n",
    "\n",
    "\n",
    "def process_abf_data(file_path, color, label):\n",
    "    # Load the data\n",
    "    data = functions.Abfdata(file_path)\n",
    "    base_name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "\n",
    "    # Extract trace and time data\n",
    "    trace_data = data.extract_trace_data()\n",
    "    time_values = data.get_time_values() * 1000  # convert to ms\n",
    "\n",
    "    #EXCLUDE INDICES IF NEEDED \n",
    "    excluded_indices = {}\n",
    "    filtered_indices = [idx for idx in range(len(trace_data)) if idx not in excluded_indices]\n",
    "\n",
    "    # Create a Gaussian window for filtering\n",
    "    std_dev = 5\n",
    "    window_size = 10\n",
    "    window = signal.windows.gaussian(window_size, std_dev)\n",
    "    filtered_trace_data = trace_data[filtered_indices]\n",
    "\n",
    "    plt.figure(figsize=(4, 4))\n",
    "\n",
    "    # Plot pulse data\n",
    "    pulse_data = [data.extract_pulse_data() for _ in filtered_trace_data]\n",
    "    print(\"Pulse data length:\", len(pulse_data))\n",
    "    print(\"Pulse data shape:\", np.array(pulse_data).shape)\n",
    "    print(\"Pulse data type:\", type(pulse_data))\n",
    "    print(\"Pulse data:\", pulse_data)\n",
    "    pulse_data = np.array(pulse_data)\n",
    "    stim_time = []\n",
    "    for pulse in pulse_data:\n",
    "        #plt.plot(time_values, np.array(pulse), color='magenta', alpha=0.9)\n",
    "        # Find peaks in the data\n",
    "        peaks, props = signal.find_peaks(pulse, height=2.1, width=1)  # adjust height as needed\n",
    "        pulse_starts = time_values[props[\"left_ips\"].astype(int)]\n",
    "        pulse_ends   = time_values[props[\"right_ips\"].astype(int)]\n",
    "        #for start, end in zip(pulse_starts, pulse_ends):\n",
    "        #    plt.vlines([start, end], ymin=-200, ymax=200, color=\"paleturquoise\", alpha=0.3)\n",
    "        stim_times = time_values[peaks]\n",
    "    stim_time.append(stim_times)\n",
    "\n",
    "\n",
    "    filt_trace_data = []\n",
    "    for sweep_data in filtered_trace_data:\n",
    "        filtered_sweep_data = signal.convolve(sweep_data, window, mode='same') / sum(window)\n",
    "        filt_trace_data.append(filtered_sweep_data)\n",
    "\n",
    "    # Average sweeps and apply baseline correction\n",
    "    averaged_data = data.average_abf_sweeps()\n",
    "    window = signal.windows.gaussian(5, 2)\n",
    "    averaged_data_data = signal.convolve(averaged_data, window, mode='same') / sum(window)\n",
    "    baseline_averageddata = data.baseline_correction(averaged_data_data)\n",
    "    \n",
    "    fs = 50000  # Replace with your actual sampling rate\n",
    "    f, Pxx = welch(baseline_averageddata, fs, nperseg=2048)\n",
    "    f0 = 50  # Notch filter frequency (Hz)\n",
    "    Q = 300   # Quality factor\n",
    "    b, a = iirnotch(f0, Q, fs)\n",
    "    cleaned_trace = filtfilt(b, a, baseline_averageddata)\n",
    "\n",
    "    def butter_lowpass(cutoff, fs, order=4):\n",
    "        nyq = 0.5 * fs\n",
    "        normal_cutoff = cutoff / nyq\n",
    "        return butter(order, normal_cutoff, btype='low', analog=False)\n",
    "\n",
    "    cutoff = 20  # or lower, depending on what you want to keep\n",
    "    b, a = butter_lowpass(cutoff, fs)\n",
    "    smoothed_trace = filtfilt(b, a, cleaned_trace)\n",
    "\n",
    "    #COMMENT THIS OUT IF YOU DON'T WANT TO SEE ALL THE TRACES \n",
    "    baseline_cor_data = data.baseline_correction(filt_trace_data)\n",
    "    for i, sweep_data in enumerate(baseline_cor_data[:]):\n",
    "       plt.plot(time_values, sweep_data, alpha=.8, color=color)\n",
    "\n",
    "    # Plot averaged trace\n",
    "    #peak = []\n",
    "    #plt.plot(time_values, smoothed_trace, label=f'{label}', color='darkcyan', alpha=0.8, linewidth=1), \n",
    "    #peaks, _ = signal.find_peaks(baseline_averageddata, height=90,prominence=8)\n",
    "    #peak.append(peaks)\n",
    "\n",
    "    #contour_heights = baseline_averageddata[peaks] - prominences\n",
    "    #plt.plot(peaks, baseline_averageddata[peaks], \"x\")\n",
    "    plt.xlim(100,1000)\n",
    "    plt.ylim(-5, 80)\n",
    "    #print(\"Averaged data peaks:\", peaks)\n",
    "    plt.show()\n",
    "    # Compute more peak details from averaged data\n",
    "    peak_indices, peak_props = signal.find_peaks(baseline_averageddata, height=90, prominence=8)\n",
    "\n",
    "    peak_times = time_values[peak_indices]\n",
    "    peak_amps = baseline_averageddata[peak_indices]\n",
    "    baseline_value = 0  # Or compute your baseline from data if needed\n",
    "\n",
    "    # Duration as width at half prominence\n",
    "    widths, width_heights, left_ips, right_ips = signal.peak_widths(baseline_averageddata, peak_indices, rel_height=0.5)\n",
    "    durations = (right_ips - left_ips) * (time_values[1] - time_values[0])  # in ms\n",
    "\n",
    "    # Replace your return with this:\n",
    "    return {\n",
    "        \"averaged_data\": averaged_data_data,\n",
    "        \"time_values\": time_values,\n",
    "        \"baseline_averaged\": baseline_averageddata,\n",
    "        \"stim_times\": stim_times,\n",
    "        \"stim_t\": stim_time,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "64254191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-62.6221 -62.6221 -62.6221 ... -62.0728 -62.1033 -62.1033]\n",
      "Pulse data length: 1\n",
      "Pulse data shape: (1, 75000)\n",
      "Pulse data type: <class 'list'>\n",
      "Pulse data: [array([-62.6221, -62.6221, -62.6221, ..., -62.0728, -62.1033, -62.1033],\n",
      "      shape=(75000,), dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- 1. Load data ---\n",
    "export_dir = Path('D:/Paris2025/ForceB Undiluted/20251001/cell2/014-SD10hz-70/suite2p_exported')\n",
    "F = np.load(export_dir / 'fluorescence_manual.npy')  # shape (1, 1485)\n",
    "F_trace = F.squeeze()         # now shape (1485,)\n",
    "\n",
    "# --- 2. Define frame ranges ---\n",
    "# adjust these according to your experiment timing\n",
    "light_off_frames = np.arange(1000, 1200)      # frames with lights off\n",
    "light_on_frames = np.arange(100, 200)     # frames with lights on\n",
    "\n",
    "# --- 3. Compute baselines safely ---\n",
    "F_off = np.mean(F_trace[light_off_frames])\n",
    "F_on_baseline = np.mean(F_trace[light_on_frames])  # first ~50 frames after light on\n",
    "\n",
    "# --- 4. Compute ΔF/F₀ ---\n",
    "F0 = F_on_baseline - F_off \n",
    "dF = F_trace - F_on_baseline\n",
    "dF_over_F0 = (dF / F0) *100  # convert to percentage\n",
    "\n",
    "# --- 5. Optional: Replace NaNs and plot ---\n",
    "dF_over_F0 = np.nan_to_num(dF_over_F0, nan=0.0)\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(dF_over_F0, label='ΔF/F₀')\n",
    "#plt.axvspan(light_off_frames[0], light_off_frames[-1], color='gray', alpha=0.2, label='Light off')\n",
    "#plt.axvspan(light_on_frames[0], light_on_frames[-1], color='yellow', alpha=0.1, label='Light on')\n",
    "plt.xlabel('Frame')\n",
    "plt.ylabel('ΔF/F₀')\n",
    "plt.xlim(50, 900)\n",
    "plt.ylim(-10, 20)\n",
    "process_abf_data(\"D:/Paris2025/Ephys/011025/2025_10_01_0014.abf\", \"darkcyan\", \"ForceB\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e186f78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SNR: 14.673527717590332\n",
      "Mean SNR: 14.67\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "fluorescence = np.load(export_dir / 'fluorescence_manual.npy')\n",
    "dff = np.load(export_dir / 'dff_manual.npy')\n",
    "\n",
    "# Compute SNR in one line\n",
    "noise = np.std(F_trace[light_on_frames])\n",
    "signal = np.max(F_trace[30:500]) - F_on_baseline\n",
    "snr = (signal / noise) \n",
    "\n",
    "print(f\"SNR: {snr}\")\n",
    "print(f\"Mean SNR: {np.mean(snr):.2f}\")\n",
    "F_trace[50:500]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

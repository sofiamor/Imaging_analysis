{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cd46c13d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame structure:\n",
      "   470first SP  620first SP  Simultaneous SP  470first TR  620first TR  \\\n",
      "0          0.0          0.0              5.6      0.00000      0.00000   \n",
      "1         16.4         19.2             19.6     14.66667     18.66667   \n",
      "2         13.6         21.6             17.2     14.66667     26.00000   \n",
      "3          0.0          0.0              0.4      0.00000      0.00000   \n",
      "4          0.0          0.0              0.0      0.00000      0.00000   \n",
      "\n",
      "   Simultaneous TR  \n",
      "0         0.000000  \n",
      "1        10.666670  \n",
      "2        17.333330  \n",
      "3         0.666667  \n",
      "4         0.000000  \n",
      "\n",
      "DataFrame shape: (54, 6)\n",
      "Column names: ['470first SP', '620first SP', 'Simultaneous SP', '470first TR', '620first TR', 'Simultaneous TR']\n",
      "\n",
      "ANOVA dataset shape: (188, 4)\n",
      "Active responses: 188\n",
      "\n",
      "TWO-WAY ANOVA RESULTS:\n",
      "==================================================\n",
      "                            sum_sq     df         F    PR(>F)\n",
      "C(condition)              8.694710    2.0  0.205242  0.814638\n",
      "C(group)                  9.229658    1.0  0.435740  0.510021\n",
      "C(condition):C(group)    38.537704    2.0  0.909698  0.404469\n",
      "Residual               3855.049993  182.0       NaN       NaN\n",
      "==================================================\n",
      "\n",
      "EFFECT SIZES (η²):\n",
      "Condition: 0.002\n",
      "Group: 0.002\n",
      "Interaction: 0.010\n",
      "\n",
      "CHI-SQUARE TESTS FOR RESPONSE PREVALENCE:\n",
      "470first:\n",
      "  χ²(1) = 0.038, p = 0.846\n",
      "  Group 1: 30/53 (56.6%) responded\n",
      "  Group 2: 32/53 (60.4%) responded\n",
      "\n",
      "620first:\n",
      "  χ²(1) = 0.338, p = 0.561\n",
      "  Group 1: 32/53 (60.4%) responded\n",
      "  Group 2: 28/53 (52.8%) responded\n",
      "\n",
      "Simultaneous:\n",
      "  χ²(1) = 4.714, p = 0.030\n",
      "  Group 1: 39/53 (73.6%) responded\n",
      "  Group 2: 27/53 (50.9%) responded\n",
      "  → SIGNIFICANT DIFFERENCE (p < 0.05)\n",
      "\n",
      "SUMMARY STATISTICS:\n",
      "========================================\n",
      "470first:\n",
      "  Group 1: 30 active neurons, Mean = 3.053 ± 3.914 Hz\n",
      "  Group 2: 32 active neurons, Mean = 3.688 ± 4.067 Hz\n",
      "\n",
      "620first:\n",
      "  Group 1: 32 active neurons, Mean = 3.125 ± 5.024 Hz\n",
      "  Group 2: 28 active neurons, Mean = 4.619 ± 5.766 Hz\n",
      "\n",
      "Simultaneous:\n",
      "  Group 1: 39 active neurons, Mean = 3.631 ± 4.753 Hz\n",
      "  Group 2: 27 active neurons, Mean = 2.914 ± 3.717 Hz\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['MPLBACKEND'] = 'Qt5Agg'  # Set backend before importing matplotlib\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "matplotlib.use('Qt5Agg')  # Explicitly set backend\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv('E:/plotsNPX/firingRateGroups.csv')\n",
    "\n",
    "drop = ['Unnamed: 0', 'Unnamed: 1']\n",
    "# Remove the first column (index column) if it exists\n",
    "if 'Unnamed: 0' in df.columns:\n",
    "    df = df.drop('Unnamed: 0', axis=1)\n",
    "\n",
    "# Display the first few rows to verify the structure\n",
    "print(\"DataFrame structure:\")\n",
    "print(df.head())\n",
    "print(f\"\\nDataFrame shape: {df.shape}\")\n",
    "print(f\"Column names: {list(df.columns)}\")\n",
    "\n",
    "# Reshape data for ANOVA (long format)\n",
    "anova_data = []\n",
    "for i in range(len(df)):  # For each neuron\n",
    "    for j, condition in enumerate(['470first', '620first', 'Simultaneous']):\n",
    "        # Group 1 data (columns 0, 1, 2)\n",
    "        firing_rate_group1 = df.iloc[i, j]\n",
    "        if firing_rate_group1 > 0:  # Only include active responses\n",
    "            anova_data.append({\n",
    "                'neuron_id': i,\n",
    "                'condition': condition,\n",
    "                'group': 'Group1',\n",
    "                'firing_rate': firing_rate_group1\n",
    "            })\n",
    "        \n",
    "        # Group 2 data (columns 3, 4, 5)\n",
    "        firing_rate_group2 = df.iloc[i, j + 3]\n",
    "        if firing_rate_group2 > 0:  # Only include active responses\n",
    "            anova_data.append({\n",
    "                'neuron_id': i,\n",
    "                'condition': condition,\n",
    "                'group': 'Group2',\n",
    "                'firing_rate': firing_rate_group2\n",
    "            })\n",
    "\n",
    "anova_df = pd.DataFrame(anova_data)\n",
    "\n",
    "print(f\"\\nANOVA dataset shape: {anova_df.shape}\")\n",
    "print(f\"Active responses: {len(anova_df)}\")\n",
    "\n",
    "# Two-Way ANOVA using statsmodels\n",
    "model = ols('firing_rate ~ C(condition) + C(group) + C(condition):C(group)', data=anova_df).fit()\n",
    "anova_table = sm.stats.anova_lm(model, typ=2)\n",
    "\n",
    "print(\"\\nTWO-WAY ANOVA RESULTS:\")\n",
    "print(\"=\" * 50)\n",
    "print(anova_table)\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Post-hoc tests for significant effects\n",
    "# Post-hoc for condition\n",
    "if anova_table['PR(>F)']['C(condition)'] < 0.05:\n",
    "    print(\"\\nPOST-HOC TESTS FOR CONDITION:\")\n",
    "    tukey_condition = pairwise_tukeyhsd(anova_df['firing_rate'], anova_df['condition'], alpha=0.05)\n",
    "    print(tukey_condition)\n",
    "\n",
    "# Post-hoc for group\n",
    "if anova_table['PR(>F)']['C(group)'] < 0.05:\n",
    "    print(\"\\nPOST-HOC TESTS FOR GROUP:\")\n",
    "    tukey_group = pairwise_tukeyhsd(anova_df['firing_rate'], anova_df['group'], alpha=0.05)\n",
    "    print(tukey_group)\n",
    "\n",
    "# Calculate and print effect sizes\n",
    "def eta_squared(ss_effect, ss_total):\n",
    "    return ss_effect / ss_total\n",
    "\n",
    "ss_condition = anova_table['sum_sq']['C(condition)']\n",
    "ss_group = anova_table['sum_sq']['C(group)']\n",
    "ss_interaction = anova_table['sum_sq']['C(condition):C(group)']\n",
    "ss_total = ss_condition + ss_group + ss_interaction + anova_table['sum_sq']['Residual']\n",
    "\n",
    "print(f\"\\nEFFECT SIZES (η²):\")\n",
    "print(f\"Condition: {eta_squared(ss_condition, ss_total):.3f}\")\n",
    "print(f\"Group: {eta_squared(ss_group, ss_total):.3f}\")\n",
    "print(f\"Interaction: {eta_squared(ss_interaction, ss_total):.3f}\")\n",
    "\n",
    "# Create visualization\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Subplot 1: Mean firing rates\n",
    "plt.subplot(1, 2, 1)\n",
    "conditions = ['470first', '620first', 'Simultaneous']\n",
    "\n",
    "# Calculate means for active neurons only\n",
    "group1_means = []\n",
    "group2_means = []\n",
    "for j, condition in enumerate(conditions):\n",
    "    # Group 1 means\n",
    "    group1_data = df.iloc[:, j]\n",
    "    group1_active = group1_data[group1_data > 0]\n",
    "    group1_means.append(group1_active.mean() if len(group1_active) > 0 else 0)\n",
    "    \n",
    "    # Group 2 means\n",
    "    group2_data = df.iloc[:, j + 3]\n",
    "    group2_active = group2_data[group2_data > 0]\n",
    "    group2_means.append(group2_active.mean() if len(group2_active) > 0 else 0)\n",
    "\n",
    "x = np.arange(len(conditions))\n",
    "width = 0.35\n",
    "\n",
    "plt.bar(x - width/2, group1_means, width, label='Group 1', alpha=0.8, color='gray')\n",
    "plt.bar(x + width/2, group2_means, width, label='Group 2', alpha=0.8, color='black')\n",
    "\n",
    "plt.xlabel('Condition')\n",
    "plt.ylabel('Mean Firing Rate (Hz)')\n",
    "plt.title('Firing Rates by Condition and Group\\n(Active neurons only)')\n",
    "plt.xticks(x, conditions)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, v in enumerate(group1_means):\n",
    "    plt.text(i - width/2, v + 0.1, f'{v:.2f}', ha='center', va='bottom')\n",
    "for i, v in enumerate(group2_means):\n",
    "    plt.text(i + width/2, v + 0.1, f'{v:.2f}', ha='center', va='bottom')\n",
    "\n",
    "# Subplot 2: Response prevalence\n",
    "plt.subplot(1, 2, 2)\n",
    "response_rates_group1 = []\n",
    "response_rates_group2 = []\n",
    "\n",
    "for j, condition in enumerate(conditions):\n",
    "    # Group 1 response rate\n",
    "    group1_data = df.iloc[:, j]\n",
    "    response_rate_group1 = (group1_data > 0).mean() * 100\n",
    "    response_rates_group1.append(response_rate_group1)\n",
    "    \n",
    "    # Group 2 response rate\n",
    "    group2_data = df.iloc[:, j + 3]\n",
    "    response_rate_group2 = (group2_data > 0).mean() * 100\n",
    "    response_rates_group2.append(response_rate_group2)\n",
    "\n",
    "plt.bar(x - width/2, response_rates_group1, width, label='Group 1', alpha=0.8, color='gray')\n",
    "plt.bar(x + width/2, response_rates_group2, width, label='Group 2', alpha=0.8, color='black')\n",
    "\n",
    "plt.xlabel('Condition')\n",
    "plt.ylabel('Response Rate (%)')\n",
    "plt.title('Percentage of Neurons Responding')\n",
    "plt.xticks(x, conditions)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, v in enumerate(response_rates_group1):\n",
    "    plt.text(i - width/2, v + 1, f'{v:.1f}%', ha='center', va='bottom')\n",
    "    plt.text(i + width/2, v + 1, f'{v:.1f}%', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Chi-square tests for response prevalence\n",
    "print(\"\\nCHI-SQUARE TESTS FOR RESPONSE PREVALENCE:\")\n",
    "for j, condition in enumerate(['470first', '620first', 'Simultaneous']):\n",
    "    # Group 1 data\n",
    "    group1_data = df.iloc[:, j]\n",
    "    responded_group1 = (group1_data > 0).sum()\n",
    "    no_response_group1 = len(group1_data) - responded_group1\n",
    "    \n",
    "    # Group 2 data\n",
    "    group2_data = df.iloc[:, j + 3]\n",
    "    responded_group2 = (group2_data > 0).sum()\n",
    "    no_response_group2 = len(group2_data) - responded_group2\n",
    "    \n",
    "    contingency_table = [[responded_group1, no_response_group1],\n",
    "                        [responded_group2, no_response_group2]]\n",
    "    \n",
    "    chi2, p, dof, expected = chi2_contingency(contingency_table)\n",
    "    \n",
    "    print(f\"{condition}:\")\n",
    "    print(f\"  χ²({dof}) = {chi2:.3f}, p = {p:.3f}\")\n",
    "    print(f\"  Group 1: {responded_group1}/53 ({responded_group1/53*100:.1f}%) responded\")\n",
    "    print(f\"  Group 2: {responded_group2}/53 ({responded_group2/53*100:.1f}%) responded\")\n",
    "    if p < 0.05:\n",
    "        print(f\"  → SIGNIFICANT DIFFERENCE (p < 0.05)\")\n",
    "    print()\n",
    "\n",
    "# Additional summary statistics\n",
    "print(\"SUMMARY STATISTICS:\")\n",
    "print(\"=\" * 40)\n",
    "for j, condition in enumerate(conditions):\n",
    "    group1_active = df.iloc[:, j][df.iloc[:, j] > 0]\n",
    "    group2_active = df.iloc[:, j + 3][df.iloc[:, j + 3] > 0]\n",
    "    \n",
    "    print(f\"{condition}:\")\n",
    "    print(f\"  Group 1: {len(group1_active)} active neurons, Mean = {group1_active.mean():.3f} ± {group1_active.std():.3f} Hz\")\n",
    "    print(f\"  Group 2: {len(group2_active)} active neurons, Mean = {group2_active.mean():.3f} ± {group2_active.std():.3f} Hz\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e89fe4af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame structure:\n",
      "   470first SP  620first SP  Simultaneous SP  470first TR  620first TR  \\\n",
      "0          0.0          0.0              5.6      0.00000      0.00000   \n",
      "1         16.4         19.2             19.6     14.66667     18.66667   \n",
      "2         13.6         21.6             17.2     14.66667     26.00000   \n",
      "3          0.0          0.0              0.4      0.00000      0.00000   \n",
      "4          0.0          0.0              0.0      0.00000      0.00000   \n",
      "\n",
      "   Simultaneous TR  \n",
      "0         0.000000  \n",
      "1        10.666670  \n",
      "2        17.333330  \n",
      "3         0.666667  \n",
      "4         0.000000  \n",
      "\n",
      "DataFrame shape: (54, 6)\n",
      "Column names: ['470first SP', '620first SP', 'Simultaneous SP', '470first TR', '620first TR', 'Simultaneous TR']\n",
      "Data types:\n",
      "470first SP        float64\n",
      "620first SP        float64\n",
      "Simultaneous SP    float64\n",
      "470first TR        float64\n",
      "620first TR        float64\n",
      "Simultaneous TR    float64\n",
      "dtype: object\n",
      "\n",
      "Checking for non-numeric values:\n",
      "\n",
      "Converting to numeric values...\n",
      "After conversion - Data types:\n",
      "470first SP        float64\n",
      "620first SP        float64\n",
      "Simultaneous SP    float64\n",
      "470first TR        float64\n",
      "620first TR        float64\n",
      "Simultaneous TR    float64\n",
      "dtype: object\n",
      "Any remaining NaN values: False\n",
      "\n",
      "ANOVA dataset shape: (188, 4)\n",
      "Active responses: 188\n",
      "\n",
      "Sample of ANOVA dataset:\n",
      "   neuron_id     condition   group  firing_rate\n",
      "0          0  Simultaneous  Group1      5.60000\n",
      "1          1      470first  Group1     16.40000\n",
      "2          1      470first  Group2     14.66667\n",
      "3          1      620first  Group1     19.20000\n",
      "4          1      620first  Group2     18.66667\n",
      "5          1  Simultaneous  Group1     19.60000\n",
      "6          1  Simultaneous  Group2     10.66667\n",
      "7          2      470first  Group1     13.60000\n",
      "8          2      470first  Group2     14.66667\n",
      "9          2      620first  Group1     21.60000\n",
      "\n",
      "Data distribution:\n",
      "                    firing_rate              \n",
      "                          count   mean    std\n",
      "condition    group                           \n",
      "470first     Group1          30  3.053  3.914\n",
      "             Group2          32  3.688  4.067\n",
      "620first     Group1          32  3.125  5.024\n",
      "             Group2          28  4.619  5.766\n",
      "Simultaneous Group1          39  3.631  4.753\n",
      "             Group2          27  2.914  3.717\n",
      "\n",
      "TWO-WAY ANOVA RESULTS:\n",
      "==================================================\n",
      "                            sum_sq     df         F    PR(>F)\n",
      "C(condition)              8.694710    2.0  0.205242  0.814638\n",
      "C(group)                  9.229658    1.0  0.435740  0.510021\n",
      "C(condition):C(group)    38.537704    2.0  0.909698  0.404469\n",
      "Residual               3855.049993  182.0       NaN       NaN\n",
      "==================================================\n",
      "\n",
      "EFFECT SIZES (η²):\n",
      "Condition: 0.002\n",
      "Group: 0.002\n",
      "Interaction: 0.010\n",
      "\n",
      "CHI-SQUARE TESTS FOR RESPONSE PREVALENCE:\n",
      "==================================================\n",
      "470first:\n",
      "  χ²(1) = 0.038, p = 0.846\n",
      "  Group 1: 30/54 (55.6%) responded\n",
      "  Group 2: 32/54 (59.3%) responded\n",
      "  → No significant difference\n",
      "\n",
      "620first:\n",
      "  χ²(1) = 0.338, p = 0.561\n",
      "  Group 1: 32/54 (59.3%) responded\n",
      "  Group 2: 28/54 (51.9%) responded\n",
      "  → No significant difference\n",
      "\n",
      "Simultaneous:\n",
      "  χ²(1) = 4.714, p = 0.030\n",
      "  Group 1: 39/54 (72.2%) responded\n",
      "  Group 2: 27/54 (50.0%) responded\n",
      "  → SIGNIFICANT DIFFERENCE (p < 0.05)\n",
      "\n",
      "\n",
      "SUMMARY STATISTICS:\n",
      "========================================\n",
      "470first:\n",
      "  Group 1: 30/54 active neurons\n",
      "    Mean = 3.053 ± 3.914 Hz\n",
      "  Group 2: 32/54 active neurons\n",
      "    Mean = 3.688 ± 4.067 Hz\n",
      "\n",
      "620first:\n",
      "  Group 1: 32/54 active neurons\n",
      "    Mean = 3.125 ± 5.024 Hz\n",
      "  Group 2: 28/54 active neurons\n",
      "    Mean = 4.619 ± 5.766 Hz\n",
      "\n",
      "Simultaneous:\n",
      "  Group 1: 39/54 active neurons\n",
      "    Mean = 3.631 ± 4.753 Hz\n",
      "  Group 2: 27/54 active neurons\n",
      "    Mean = 2.914 ± 3.717 Hz\n",
      "\n",
      "Analysis completed successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['MPLBACKEND'] = 'Qt5Agg'  # Set backend before importing matplotlib\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "matplotlib.use('Qt5Agg')  # Explicitly set backend\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "from scipy.stats import chi2_contingency\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Read the CSV file\n",
    "csv_path = 'E:/plotsNPX/firingRateGroups.csv'\n",
    "try:\n",
    "    df = pd.read_csv(csv_path)\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: File not found at '{csv_path}'. Please check the path and ensure the file exists.\")\n",
    "    # Optionally, you can stop further execution\n",
    "    import sys\n",
    "    sys.exit(1)\n",
    "\n",
    "# Remove the first column (index column) if it exists\n",
    "if 'Unnamed: 0' in df.columns:\n",
    "    df = df.drop('Unnamed: 0', axis=1)\n",
    "\n",
    "# Display initial data info\n",
    "print(\"DataFrame structure:\")\n",
    "print(df.head())\n",
    "print(f\"\\nDataFrame shape: {df.shape}\")\n",
    "print(f\"Column names: {list(df.columns)}\")\n",
    "print(f\"Data types:\\n{df.dtypes}\")\n",
    "\n",
    "# Check for non-numeric values\n",
    "print(\"\\nChecking for non-numeric values:\")\n",
    "for col in df.columns:\n",
    "    non_numeric = df[col].apply(lambda x: not isinstance(x, (int, float, np.number)) and not pd.isna(x))\n",
    "    if non_numeric.any():\n",
    "        print(f\"Column {col} has non-numeric values: {df[col][non_numeric].unique()}\")\n",
    "\n",
    "# Convert all columns to numeric, replacing non-numeric values with NaN\n",
    "print(\"\\nConverting to numeric values...\")\n",
    "for col in df.columns:\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "# Fill NaN values with 0 (assuming no response = 0 firing rate)\n",
    "df = df.fillna(0)\n",
    "\n",
    "print(f\"After conversion - Data types:\\n{df.dtypes}\")\n",
    "print(f\"Any remaining NaN values: {df.isna().any().any()}\")\n",
    "\n",
    "# Verify we have the expected number of columns (6)\n",
    "if len(df.columns) < 6:\n",
    "    print(f\"Warning: Expected 6 columns but found {len(df.columns)}\")\n",
    "    print(\"Please check your CSV file structure\")\n",
    "    exit()\n",
    "\n",
    "# Reshape data for ANOVA (long format)\n",
    "anova_data = []\n",
    "conditions = ['470first', '620first', 'Simultaneous']\n",
    "\n",
    "for i in range(len(df)):  # For each neuron\n",
    "    for j, condition in enumerate(conditions):\n",
    "        # Group 1 data (columns 0, 1, 2)\n",
    "        firing_rate_group1 = df.iloc[i, j]\n",
    "        if firing_rate_group1 > 0:  # Only include active responses\n",
    "            anova_data.append({\n",
    "                'neuron_id': i,\n",
    "                'condition': condition,\n",
    "                'group': 'Group1',\n",
    "                'firing_rate': firing_rate_group1\n",
    "            })\n",
    "        \n",
    "        # Group 2 data (columns 3, 4, 5)\n",
    "        firing_rate_group2 = df.iloc[i, j + 3]\n",
    "        if firing_rate_group2 > 0:  # Only include active responses\n",
    "            anova_data.append({\n",
    "                'neuron_id': i,\n",
    "                'condition': condition,\n",
    "                'group': 'Group2',\n",
    "                'firing_rate': firing_rate_group2\n",
    "            })\n",
    "\n",
    "anova_df = pd.DataFrame(anova_data)\n",
    "\n",
    "print(f\"\\nANOVA dataset shape: {anova_df.shape}\")\n",
    "print(f\"Active responses: {len(anova_df)}\")\n",
    "\n",
    "# Check if we have enough data for ANOVA\n",
    "if len(anova_df) < 10:\n",
    "    print(\"Warning: Very few active responses. Consider including zero responses or check your data.\")\n",
    "\n",
    "# Display sample of ANOVA data\n",
    "print(\"\\nSample of ANOVA dataset:\")\n",
    "print(anova_df.head(10))\n",
    "\n",
    "# Check data distribution by group and condition\n",
    "print(\"\\nData distribution:\")\n",
    "print(anova_df.groupby(['condition', 'group']).agg({\n",
    "    'firing_rate': ['count', 'mean', 'std']\n",
    "}).round(3))\n",
    "\n",
    "# Two-Way ANOVA using statsmodels\n",
    "try:\n",
    "    model = ols('firing_rate ~ C(condition) + C(group) + C(condition):C(group)', data=anova_df).fit()\n",
    "    anova_table = sm.stats.anova_lm(model, typ=2)\n",
    "    \n",
    "    print(\"\\nTWO-WAY ANOVA RESULTS:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(anova_table)\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Post-hoc tests for significant effects\n",
    "    # Post-hoc for condition\n",
    "    if anova_table['PR(>F)']['C(condition)'] < 0.05:\n",
    "        print(\"\\nPOST-HOC TESTS FOR CONDITION:\")\n",
    "        tukey_condition = pairwise_tukeyhsd(anova_df['firing_rate'], anova_df['condition'], alpha=0.05)\n",
    "        print(tukey_condition)\n",
    "    \n",
    "    # Post-hoc for group\n",
    "    if anova_table['PR(>F)']['C(group)'] < 0.05:\n",
    "        print(\"\\nPOST-HOC TESTS FOR GROUP:\")\n",
    "        tukey_group = pairwise_tukeyhsd(anova_df['firing_rate'], anova_df['group'], alpha=0.05)\n",
    "        print(tukey_group)\n",
    "    \n",
    "    # Calculate and print effect sizes\n",
    "    def eta_squared(ss_effect, ss_total):\n",
    "        return ss_effect / ss_total\n",
    "    \n",
    "    ss_condition = anova_table['sum_sq']['C(condition)']\n",
    "    ss_group = anova_table['sum_sq']['C(group)']\n",
    "    ss_interaction = anova_table['sum_sq']['C(condition):C(group)']\n",
    "    ss_total = ss_condition + ss_group + ss_interaction + anova_table['sum_sq']['Residual']\n",
    "    \n",
    "    print(f\"\\nEFFECT SIZES (η²):\")\n",
    "    print(f\"Condition: {eta_squared(ss_condition, ss_total):.3f}\")\n",
    "    print(f\"Group: {eta_squared(ss_group, ss_total):.3f}\")\n",
    "    print(f\"Interaction: {eta_squared(ss_interaction, ss_total):.3f}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error in ANOVA analysis: {e}\")\n",
    "    print(\"This might be due to insufficient data or other issues.\")\n",
    "\n",
    "# Create visualization\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Subplot 1: Mean firing rates (only active neurons)\n",
    "plt.subplot(2, 2, 1)\n",
    "conditions = ['470first', '620first', 'Simultaneous']\n",
    "\n",
    "# Calculate means for active neurons only\n",
    "group1_means = []\n",
    "group2_means = []\n",
    "group1_sems = []  # Standard error of mean\n",
    "group2_sems = []\n",
    "\n",
    "for j, condition in enumerate(conditions):\n",
    "    # Group 1 means\n",
    "    group1_data = df.iloc[:, j]\n",
    "    group1_active = group1_data[group1_data > 0]\n",
    "    if len(group1_active) > 0:\n",
    "        group1_means.append(group1_active.mean())\n",
    "        group1_sems.append(group1_active.sem())\n",
    "    else:\n",
    "        group1_means.append(0)\n",
    "        group1_sems.append(0)\n",
    "    \n",
    "    # Group 2 means\n",
    "    group2_data = df.iloc[:, j + 3]\n",
    "    group2_active = group2_data[group2_data > 0]\n",
    "    if len(group2_active) > 0:\n",
    "        group2_means.append(group2_active.mean())\n",
    "        group2_sems.append(group2_active.sem())\n",
    "    else:\n",
    "        group2_means.append(0)\n",
    "        group2_sems.append(0)\n",
    "\n",
    "x = np.arange(len(conditions))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = plt.bar(x - width/2, group1_means, width, yerr=group1_sems, \n",
    "               label='Group 1', alpha=0.8, color='gray', capsize=5)\n",
    "bars2 = plt.bar(x + width/2, group2_means, width, yerr=group2_sems,\n",
    "               label='Group 2', alpha=0.8, color='black', capsize=5)\n",
    "\n",
    "plt.xlabel('Condition')\n",
    "plt.ylabel('Mean Firing Rate (Hz)')\n",
    "plt.title('Firing Rates by Condition and Group\\n(Active neurons only)')\n",
    "plt.xticks(x, conditions, rotation=45)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, (v, sem) in enumerate(zip(group1_means, group1_sems)):\n",
    "    plt.text(i - width/2, v + sem + 0.1, f'{v:.2f}', ha='center', va='bottom', fontsize=9)\n",
    "for i, (v, sem) in enumerate(zip(group2_means, group2_sems)):\n",
    "    plt.text(i + width/2, v + sem + 0.1, f'{v:.2f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "if len(anova_df) > 0:\n",
    "    for i, group in enumerate(['Group1', 'Group2']):\n",
    "        group_data = anova_df[anova_df['group'] == group]\n",
    "        for j, condition in enumerate(conditions):\n",
    "            condition_data = group_data[group_data['condition'] == condition]\n",
    "            if len(condition_data) > 0:\n",
    "                y_pos = j + (i * 0.1) - 0.05  # Slight offset for different groups\n",
    "                plt.scatter([y_pos] * len(condition_data), condition_data['firing_rate'], \n",
    "                           alpha=0.6, s=30, label=f'{group}' if j == 0 else \"\")\n",
    "    \n",
    "    plt.xlabel('Condition')\n",
    "    plt.ylabel('Firing Rate (Hz)')\n",
    "    plt.title('Raw Data Points')\n",
    "    plt.xticks(range(len(conditions)), conditions)\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Subplot 2: Response prevalence\n",
    "plt.subplot(2, 2, 2)\n",
    "response_rates_group1 = []\n",
    "response_rates_group2 = []\n",
    "\n",
    "for j, condition in enumerate(conditions):\n",
    "    # Group 1 response rate\n",
    "    group1_data = df.iloc[:, j]\n",
    "    response_rate_group1 = (group1_data > 0).mean() * 100\n",
    "    response_rates_group1.append(response_rate_group1)\n",
    "    \n",
    "    # Group 2 response rate\n",
    "    group2_data = df.iloc[:, j + 3]\n",
    "    response_rate_group2 = (group2_data > 0).mean() * 100\n",
    "    response_rates_group2.append(response_rate_group2)\n",
    "\n",
    "plt.bar(x - width/2, response_rates_group1, width, label='Group 1', alpha=0.8, color='gray')\n",
    "plt.bar(x + width/2, response_rates_group2, width, label='Group 2', alpha=0.8, color='black')\n",
    "\n",
    "plt.xlabel('Condition')\n",
    "plt.ylabel('Response Rate (%)')\n",
    "plt.title('Percentage of Neurons Responding')\n",
    "plt.xticks(x, conditions, rotation=45)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, v in enumerate(response_rates_group1):\n",
    "    plt.text(i - width/2, v + 1, f'{v:.1f}%', ha='center', va='bottom', fontsize=9)\n",
    "for i, v in enumerate(response_rates_group2):\n",
    "    plt.text(i + width/2, v + 1, f'{v:.1f}%', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "# Subplot 3: Box plots for firing rates\n",
    "plt.subplot(2, 2, 3)\n",
    "if len(anova_df) > 0:\n",
    "    # Create box plots\n",
    "    groups_conditions = []\n",
    "    firing_rates = []\n",
    "    \n",
    "    for group in ['Group1', 'Group2']:\n",
    "        for condition in conditions:\n",
    "            subset = anova_df[(anova_df['group'] == group) & (anova_df['condition'] == condition)]\n",
    "            if len(subset) > 0:\n",
    "                groups_conditions.extend([f\"{group}\\n{condition}\"] * len(subset))\n",
    "                firing_rates.extend(subset['firing_rate'].values)\n",
    "    \n",
    "    if len(firing_rates) > 0:\n",
    "        # Create unique positions for each group-condition combination\n",
    "        unique_labels = list(set(groups_conditions))\n",
    "        positions = []\n",
    "        labels_for_plot = []\n",
    "        data_for_plot = []\n",
    "        \n",
    "        for label in unique_labels:\n",
    "            positions.append(len(labels_for_plot))\n",
    "            labels_for_plot.append(label)\n",
    "            data_for_plot.append([rate for rate, lbl in zip(firing_rates, groups_conditions) if lbl == label])\n",
    "        \n",
    "        plt.boxplot(data_for_plot, positions=positions, labels=labels_for_plot)\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.ylabel('Firing Rate (Hz)')\n",
    "        plt.title('Distribution of Firing Rates')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "\n",
    "\n",
    "\n",
    "# Chi-square tests for response prevalence\n",
    "print(\"\\nCHI-SQUARE TESTS FOR RESPONSE PREVALENCE:\")\n",
    "print(\"=\" * 50)\n",
    "for j, condition in enumerate(conditions):\n",
    "    # Group 1 data\n",
    "    group1_data = df.iloc[:, j]\n",
    "    responded_group1 = (group1_data > 0).sum()\n",
    "    no_response_group1 = len(group1_data) - responded_group1\n",
    "    \n",
    "    # Group 2 data\n",
    "    group2_data = df.iloc[:, j + 3]\n",
    "    responded_group2 = (group2_data > 0).sum()\n",
    "    no_response_group2 = len(group2_data) - responded_group2\n",
    "    \n",
    "    contingency_table = [[responded_group1, no_response_group1],\n",
    "                        [responded_group2, no_response_group2]]\n",
    "    \n",
    "    try:\n",
    "        chi2, p, dof, expected = chi2_contingency(contingency_table)\n",
    "        \n",
    "        print(f\"{condition}:\")\n",
    "        print(f\"  χ²({dof}) = {chi2:.3f}, p = {p:.3f}\")\n",
    "        print(f\"  Group 1: {responded_group1}/{len(group1_data)} ({responded_group1/len(group1_data)*100:.1f}%) responded\")\n",
    "        print(f\"  Group 2: {responded_group2}/{len(group2_data)} ({responded_group2/len(group2_data)*100:.1f}%) responded\")\n",
    "        if p < 0.05:\n",
    "            print(f\"  → SIGNIFICANT DIFFERENCE (p < 0.05)\")\n",
    "        else:\n",
    "            print(f\"  → No significant difference\")\n",
    "        print()\n",
    "    except Exception as e:\n",
    "        print(f\"  Error in chi-square test for {condition}: {e}\")\n",
    "\n",
    "# Additional summary statistics\n",
    "print(\"\\nSUMMARY STATISTICS:\")\n",
    "print(\"=\" * 40)\n",
    "for j, condition in enumerate(conditions):\n",
    "    group1_all = df.iloc[:, j]\n",
    "    group2_all = df.iloc[:, j + 3]\n",
    "    group1_active = group1_all[group1_all > 0]\n",
    "    group2_active = group2_all[group2_all > 0]\n",
    "    \n",
    "    print(f\"{condition}:\")\n",
    "    print(f\"  Group 1: {len(group1_active)}/{len(group1_all)} active neurons\")\n",
    "    if len(group1_active) > 0:\n",
    "        print(f\"    Mean = {group1_active.mean():.3f} ± {group1_active.std():.3f} Hz\")\n",
    "    print(f\"  Group 2: {len(group2_active)}/{len(group2_all)} active neurons\")\n",
    "    if len(group2_active) > 0:\n",
    "        print(f\"    Mean = {group2_active.mean():.3f} ± {group2_active.std():.3f} Hz\")\n",
    "    print()\n",
    "\n",
    "print(\"Analysis completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1eace95a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ADVANCED STATISTICAL ANALYSIS\n",
      "==================================================\n",
      "PCA Analysis:\n",
      "  - Number of active neurons: 27\n",
      "  - Variance explained by first 3 PCs: 95.45%\n",
      "  - PC1: 85.53%\n",
      "  - PC2: 5.99%\n",
      "  - PC3: 3.92%\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected a 2-dimensional container but got <class 'pandas.core.series.Series'> instead. Pass a DataFrame containing a single row (i.e. single sample) or a single column (i.e. single feature) instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 361\u001b[0m\n\u001b[0;32m    358\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAnalysis completed!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    360\u001b[0m \u001b[38;5;66;03m# Example usage (assuming df is your loaded dataframe): \u001b[39;00m\n\u001b[1;32m--> 361\u001b[0m \u001b[43madvanced_neuroscience_analysis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[12], line 321\u001b[0m, in \u001b[0;36madvanced_neuroscience_analysis\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m    319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(active_data) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[0;32m    320\u001b[0m     kmeans \u001b[38;5;241m=\u001b[39m KMeans(n_clusters\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m, n_init\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m--> 321\u001b[0m     clusters \u001b[38;5;241m=\u001b[39m \u001b[43mkmeans\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactive_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    322\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mK-means Clustering (k=3):\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    323\u001b[0m     unique, counts \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(clusters, return_counts\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\cluster\\_kmeans.py:1070\u001b[0m, in \u001b[0;36m_BaseKMeans.fit_predict\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1047\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit_predict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   1048\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute cluster centers and predict cluster index for each sample.\u001b[39;00m\n\u001b[0;32m   1049\u001b[0m \n\u001b[0;32m   1050\u001b[0m \u001b[38;5;124;03m    Convenience method; equivalent to calling fit(X) followed by\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1068\u001b[0m \u001b[38;5;124;03m        Index of the cluster each sample belongs to.\u001b[39;00m\n\u001b[0;32m   1069\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1070\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mlabels_\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\cluster\\_kmeans.py:1464\u001b[0m, in \u001b[0;36mKMeans.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1436\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   1437\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   1438\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute k-means clustering.\u001b[39;00m\n\u001b[0;32m   1439\u001b[0m \n\u001b[0;32m   1440\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1462\u001b[0m \u001b[38;5;124;03m        Fitted estimator.\u001b[39;00m\n\u001b[0;32m   1463\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1464\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1465\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1466\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1467\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat64\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1468\u001b[0m \u001b[43m        \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1469\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy_x\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1470\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1471\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1473\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_params_vs_input(X)\n\u001b[0;32m   1475\u001b[0m     random_state \u001b[38;5;241m=\u001b[39m check_random_state(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrandom_state)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\base.py:633\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    631\u001b[0m         out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 633\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    634\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[0;32m    635\u001b[0m     out \u001b[38;5;241m=\u001b[39m _check_y(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\utils\\validation.py:1050\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m   1043\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1044\u001b[0m             msg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1045\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected 2D array, got 1D array instead:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124marray=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00marray\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1046\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReshape your data either using array.reshape(-1, 1) if \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1047\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myour data has a single feature or array.reshape(1, -1) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1048\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mif it contains a single sample.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1049\u001b[0m             )\n\u001b[1;32m-> 1050\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[0;32m   1052\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype_numeric \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(array\u001b[38;5;241m.\u001b[39mdtype, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkind\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m array\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUSV\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1053\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1054\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumeric\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is not compatible with arrays of bytes/strings.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1055\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConvert your data to numeric values explicitly instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1056\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Expected a 2-dimensional container but got <class 'pandas.core.series.Series'> instead. Pass a DataFrame containing a single row (i.e. single sample) or a single column (i.e. single feature) instead."
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['MPLBACKEND'] = 'Qt5Agg'  # Set backend before importing matplotlib\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "matplotlib.use('Qt5Agg')  # Explicitly set backend\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Assuming your data is already loaded and processed as 'df'\n",
    "# This code extends your existing analysis\n",
    "\n",
    "def advanced_neuroscience_analysis(df):\n",
    "    \"\"\"\n",
    "    Comprehensive analysis suite for neuroscience firing rate data\n",
    "    \"\"\"\n",
    "    conditions = ['470first', '620first', 'Simultaneous']\n",
    "    \n",
    "    # Create figure with multiple subplots\n",
    "    fig = plt.figure(figsize=(20, 24))\n",
    "    \n",
    "    # ====================\n",
    "    # 1. HEATMAP OF ALL NEURONS\n",
    "    # ====================\n",
    "    plt.subplot(4, 3, 1)\n",
    "    sns.heatmap(df.values, cmap='viridis', cbar_kws={'label': 'Firing Rate (Hz)'})\n",
    "    plt.title('Heatmap: All Neurons Across Conditions')\n",
    "    plt.xlabel('Conditions (0-2: Group1, 3-5: Group2)')\n",
    "    plt.ylabel('Neuron ID')\n",
    "    \n",
    "    # ====================\n",
    "    # 2. CORRELATION MATRIX\n",
    "    # ====================\n",
    "    plt.subplot(4, 3, 2)\n",
    "    correlation_matrix = df.corr()\n",
    "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0,\n",
    "                square=True, fmt='.2f')\n",
    "    plt.title('Correlation Matrix Between Conditions')\n",
    "    \n",
    "    # ====================\n",
    "    # 3. VIOLIN PLOTS\n",
    "    # ====================\n",
    "    plt.subplot(4, 3, 3)\n",
    "    \n",
    "    # Prepare data for violin plot\n",
    "    violin_data = []\n",
    "    for j, condition in enumerate(conditions):\n",
    "        for group_idx, group_name in enumerate(['Group1', 'Group2']):\n",
    "            col_idx = j + (group_idx * 3)\n",
    "            active_data = df.iloc[:, col_idx][df.iloc[:, col_idx] > 0]\n",
    "            for rate in active_data:\n",
    "                violin_data.append({\n",
    "                    'Condition': condition,\n",
    "                    'Group': group_name,\n",
    "                    'Firing_Rate': rate,\n",
    "                    'Combined': f\"{condition}\\n{group_name}\"\n",
    "                })\n",
    "    \n",
    "    if violin_data:\n",
    "        violin_df = pd.DataFrame(violin_data)\n",
    "        sns.violinplot(data=violin_df, x='Combined', y='Firing_Rate')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.title('Distribution Shapes: Violin Plots')\n",
    "        plt.ylabel('Firing Rate (Hz)')\n",
    "    \n",
    "    # ====================\n",
    "    # 4. NEURON RESPONSE PROFILES\n",
    "    # ====================\n",
    "    plt.subplot(4, 3, 4)\n",
    "    \n",
    "    # Calculate response profiles for each neuron\n",
    "    response_profiles = []\n",
    "    for i in range(len(df)):\n",
    "        profile = []\n",
    "        for j in range(6):  # All 6 conditions\n",
    "            profile.append(df.iloc[i, j])\n",
    "        response_profiles.append(profile)\n",
    "    \n",
    "    response_profiles = np.array(response_profiles)\n",
    "    \n",
    "    # Plot response profiles for neurons with at least one response > 0\n",
    "    active_neurons = np.any(response_profiles > 0, axis=1)\n",
    "    active_profiles = response_profiles[active_neurons]\n",
    "    \n",
    "    for i, profile in enumerate(active_profiles[:20]):  # Show first 20 active neurons\n",
    "        plt.plot(range(6), profile, alpha=0.3, color='black')\n",
    "    \n",
    "    # Plot mean profile\n",
    "    mean_profile = np.mean(active_profiles, axis=0)\n",
    "    plt.plot(range(6), mean_profile, color='red', linewidth=3, label='Mean')\n",
    "    plt.xlabel('Condition (0-2: Group1, 3-5: Group2)')\n",
    "    plt.ylabel('Firing Rate (Hz)')\n",
    "    plt.title('Individual Neuron Response Profiles')\n",
    "    plt.xticks(range(6), ['470-G1', '620-G1', 'Sim-G1', '470-G2', '620-G2', 'Sim-G2'], rotation=45)\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # ====================\n",
    "    # 5. PCA ANALYSIS\n",
    "    # ====================\n",
    "    plt.subplot(4, 3, 5)\n",
    "    \n",
    "    # Prepare data for PCA (only active neurons)\n",
    "    active_data = response_profiles[active_neurons]\n",
    "    if len(active_data) > 2:\n",
    "        scaler = StandardScaler()\n",
    "        scaled_data = scaler.fit_transform(active_data)\n",
    "        \n",
    "        pca = PCA()\n",
    "        pca_result = pca.fit_transform(scaled_data)\n",
    "        \n",
    "        plt.scatter(pca_result[:, 0], pca_result[:, 1], alpha=0.6)\n",
    "        plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%} variance)')\n",
    "        plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%} variance)')\n",
    "        plt.title('PCA: Neuron Response Patterns')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # ====================\n",
    "    # 6. HIERARCHICAL CLUSTERING\n",
    "    # ====================\n",
    "    plt.subplot(4, 3, 6)\n",
    "    \n",
    "    if len(active_data) > 2:\n",
    "        # Compute linkage matrix\n",
    "        linkage_matrix = linkage(active_data, method='ward')\n",
    "        \n",
    "        # Create dendrogram\n",
    "        dendrogram(linkage_matrix, truncate_mode='level', p=5)\n",
    "        plt.title('Hierarchical Clustering of Neurons')\n",
    "        plt.xlabel('Neuron Clusters')\n",
    "        plt.ylabel('Distance')\n",
    "    \n",
    "    # ====================\n",
    "    # 7. RESPONSE LATENCY/MAGNITUDE RELATIONSHIP\n",
    "    # ====================\n",
    "    plt.subplot(4, 3, 7)\n",
    "    \n",
    "    # Calculate coefficient of variation for each neuron\n",
    "    cv_values = []\n",
    "    mean_responses = []\n",
    "    \n",
    "    for i in range(len(df)):\n",
    "        neuron_responses = df.iloc[i, :].values\n",
    "        active_responses = neuron_responses[neuron_responses > 0]\n",
    "        if len(active_responses) > 1:\n",
    "            cv = np.std(active_responses) / np.mean(active_responses)\n",
    "            cv_values.append(cv)\n",
    "            mean_responses.append(np.mean(active_responses))\n",
    "    \n",
    "    if cv_values:\n",
    "        plt.scatter(mean_responses, cv_values, alpha=0.6)\n",
    "        plt.xlabel('Mean Firing Rate (Hz)')\n",
    "        plt.ylabel('Coefficient of Variation')\n",
    "        plt.title('Response Variability vs Magnitude')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add correlation line\n",
    "        if len(cv_values) > 2:\n",
    "            z = np.polyfit(mean_responses, cv_values, 1)\n",
    "            p = np.poly1d(z)\n",
    "            plt.plot(mean_responses, p(mean_responses), \"r--\", alpha=0.8)\n",
    "            corr_coef = np.corrcoef(mean_responses, cv_values)[0,1]\n",
    "            plt.text(0.05, 0.95, f'r = {corr_coef:.3f}', transform=plt.gca().transAxes)\n",
    "    \n",
    "    # ====================\n",
    "    # 8. CUMULATIVE RESPONSE PROBABILITY\n",
    "    # ====================\n",
    "    plt.subplot(4, 3, 8)\n",
    "    \n",
    "    for j, condition in enumerate(conditions):\n",
    "        for group_idx, group_name in enumerate(['Group1', 'Group2']):\n",
    "            col_idx = j + (group_idx * 3)\n",
    "            active_data = df.iloc[:, col_idx][df.iloc[:, col_idx] > 0]\n",
    "            if len(active_data) > 0:\n",
    "                sorted_data = np.sort(active_data)\n",
    "                y_vals = np.arange(1, len(sorted_data) + 1) / len(sorted_data)\n",
    "                plt.plot(sorted_data, y_vals, label=f'{condition}-{group_name}', alpha=0.7)\n",
    "    \n",
    "    plt.xlabel('Firing Rate (Hz)')\n",
    "    plt.ylabel('Cumulative Probability')\n",
    "    plt.title('Cumulative Distribution Functions')\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # ====================\n",
    "    # 9. POPULATION VECTOR ANALYSIS\n",
    "    # ====================\n",
    "    plt.subplot(4, 3, 9)\n",
    "    \n",
    "    # Calculate population vectors for each condition\n",
    "    pop_vectors = []\n",
    "    for j in range(6):\n",
    "        condition_data = df.iloc[:, j].values\n",
    "        pop_vectors.append(condition_data / np.linalg.norm(condition_data + 1e-10))  # Normalize\n",
    "    \n",
    "    # Plot population vector similarities\n",
    "    similarity_matrix = np.zeros((6, 6))\n",
    "    for i in range(6):\n",
    "        for j in range(6):\n",
    "            similarity_matrix[i, j] = np.dot(pop_vectors[i], pop_vectors[j])\n",
    "    \n",
    "    sns.heatmap(similarity_matrix, annot=True, cmap='viridis', fmt='.3f',\n",
    "                xticklabels=['470-G1', '620-G1', 'Sim-G1', '470-G2', '620-G2', 'Sim-G2'],\n",
    "                yticklabels=['470-G1', '620-G1', 'Sim-G1', '470-G2', '620-G2', 'Sim-G2'])\n",
    "    plt.title('Population Vector Similarities')\n",
    "    \n",
    "    # ====================\n",
    "    # 10. RESPONSE SELECTIVITY INDEX\n",
    "    # ====================\n",
    "    plt.subplot(4, 3, 10)\n",
    "    \n",
    "    selectivity_indices = []\n",
    "    for i in range(len(df)):\n",
    "        neuron_responses = df.iloc[i, :].values\n",
    "        if np.sum(neuron_responses) > 0:\n",
    "            # Calculate selectivity index (max - mean) / (max + mean)\n",
    "            max_response = np.max(neuron_responses)\n",
    "            mean_response = np.mean(neuron_responses[neuron_responses > 0])\n",
    "            if mean_response > 0:\n",
    "                selectivity = (max_response - mean_response) / (max_response + mean_response)\n",
    "                selectivity_indices.append(selectivity)\n",
    "    \n",
    "    if selectivity_indices:\n",
    "        plt.hist(selectivity_indices, bins=20, alpha=0.7, edgecolor='black')\n",
    "        plt.xlabel('Selectivity Index')\n",
    "        plt.ylabel('Number of Neurons')\n",
    "        plt.title('Distribution of Response Selectivity')\n",
    "        plt.axvline(np.mean(selectivity_indices), color='red', linestyle='--', \n",
    "                   label=f'Mean: {np.mean(selectivity_indices):.3f}')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # ====================\n",
    "    # 11. SIGNAL-TO-NOISE RATIO\n",
    "    # ====================\n",
    "    plt.subplot(4, 3, 11)\n",
    "    \n",
    "    snr_values = []\n",
    "    for i in range(len(df)):\n",
    "        neuron_responses = df.iloc[i, :].values\n",
    "        active_responses = neuron_responses[neuron_responses > 0]\n",
    "        if len(active_responses) > 1:\n",
    "            signal = np.mean(active_responses)\n",
    "            noise = np.std(active_responses)\n",
    "            if noise > 0:\n",
    "                snr = signal / noise\n",
    "                snr_values.append(snr)\n",
    "    \n",
    "    if snr_values:\n",
    "        plt.hist(snr_values, bins=20, alpha=0.7, edgecolor='black')\n",
    "        plt.xlabel('Signal-to-Noise Ratio')\n",
    "        plt.ylabel('Number of Neurons')\n",
    "        plt.title('Distribution of Signal-to-Noise Ratios')\n",
    "        plt.axvline(np.mean(snr_values), color='red', linestyle='--',\n",
    "                   label=f'Mean SNR: {np.mean(snr_values):.2f}')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # ====================\n",
    "    # 12. RESPONSE RELIABILITY\n",
    "    # ====================\n",
    "    plt.subplot(4, 3, 12)\n",
    "    \n",
    "    # Compare within-group reliability\n",
    "    within_group_correlations = []\n",
    "    \n",
    "    # Group 1 correlations\n",
    "    group1_data = df.iloc[:, 0:3]\n",
    "    for i in range(3):\n",
    "        for j in range(i+1, 3):\n",
    "            corr = group1_data.iloc[:, i].corr(group1_data.iloc[:, j])\n",
    "            if not np.isnan(corr):\n",
    "                within_group_correlations.append(('Group1', corr))\n",
    "    \n",
    "    # Group 2 correlations\n",
    "    group2_data = df.iloc[:, 3:6]\n",
    "    for i in range(3):\n",
    "        for j in range(i+1, 3):\n",
    "            corr = group2_data.iloc[:, i].corr(group2_data.iloc[:, j])\n",
    "            if not np.isnan(corr):\n",
    "                within_group_correlations.append(('Group2', corr))\n",
    "    \n",
    "    if within_group_correlations:\n",
    "        corr_df = pd.DataFrame(within_group_correlations, columns=['Group', 'Correlation'])\n",
    "        sns.boxplot(data=corr_df, x='Group', y='Correlation')\n",
    "        plt.title('Within-Group Response Reliability')\n",
    "        plt.ylabel('Correlation Coefficient')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # ====================\n",
    "    # STATISTICAL SUMMARY\n",
    "    # ====================\n",
    "    print(\"ADVANCED STATISTICAL ANALYSIS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # 1. PCA Summary\n",
    "    if len(active_data) > 2:\n",
    "        print(f\"PCA Analysis:\")\n",
    "        print(f\"  - Number of active neurons: {len(active_data)}\")\n",
    "        print(f\"  - Variance explained by first 3 PCs: {sum(pca.explained_variance_ratio_[:3]):.2%}\")\n",
    "        print(f\"  - PC1: {pca.explained_variance_ratio_[0]:.2%}\")\n",
    "        print(f\"  - PC2: {pca.explained_variance_ratio_[1]:.2%}\")\n",
    "        print(f\"  - PC3: {pca.explained_variance_ratio_[2]:.2%}\" if len(pca.explained_variance_ratio_) > 2 else \"\")\n",
    "        print()\n",
    "    \n",
    "    # 2. Clustering Analysis\n",
    "    if len(active_data) > 3:\n",
    "        kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)\n",
    "        clusters = kmeans.fit_predict(active_data)\n",
    "        print(f\"K-means Clustering (k=3):\")\n",
    "        unique, counts = np.unique(clusters, return_counts=True)\n",
    "        for cluster, count in zip(unique, counts):\n",
    "            print(f\"  - Cluster {cluster}: {count} neurons ({count/len(clusters)*100:.1f}%)\")\n",
    "        print()\n",
    "    \n",
    "    # 3. Population Statistics\n",
    "    print(\"Population Response Statistics:\")\n",
    "    total_neurons = len(df)\n",
    "    responsive_neurons = np.sum(np.any(df.values > 0, axis=1))\n",
    "    print(f\"  - Total neurons: {total_neurons}\")\n",
    "    print(f\"  - Responsive neurons: {responsive_neurons} ({responsive_neurons/total_neurons*100:.1f}%)\")\n",
    "    \n",
    "    # Response patterns\n",
    "    response_patterns = []\n",
    "    for i in range(len(df)):\n",
    "        pattern = tuple(df.iloc[i, :] > 0)\n",
    "        response_patterns.append(pattern)\n",
    "    \n",
    "    unique_patterns = list(set(response_patterns))\n",
    "    print(f\"  - Unique response patterns: {len(unique_patterns)}\")\n",
    "    print()\n",
    "    \n",
    "    # 4. Cross-condition analysis\n",
    "    print(\"Cross-condition Analysis:\")\n",
    "    for i, cond1 in enumerate(conditions):\n",
    "        for j, cond2 in enumerate(conditions):\n",
    "            if i < j:\n",
    "                # Group 1 correlation\n",
    "                corr_g1 = df.iloc[:, i].corr(df.iloc[:, j])\n",
    "                # Group 2 correlation\n",
    "                corr_g2 = df.iloc[:, i+3].corr(df.iloc[:, j+3])\n",
    "                print(f\"  {cond1} vs {cond2}:\")\n",
    "                print(f\"    Group 1 correlation: {corr_g1:.3f}\")\n",
    "                print(f\"    Group 2 correlation: {corr_g2:.3f}\")\n",
    "    \n",
    "    print(\"\\nAnalysis completed!\")\n",
    "\n",
    "# Example usage (assuming df is your loaded dataframe): \n",
    "advanced_neuroscience_analysis(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c845c46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "USAGE INSTRUCTIONS:\n",
      "\n",
      "1. Initialize the analyzer:\n",
      "   analyzer = NeuropixelsAnalyzer(\n",
      "       'path/to/firing_rates.csv',\n",
      "       'path/to/area_mapping.csv', \n",
      "       'path/to/spike_timestamps.csv'\n",
      "   )\n",
      "\n",
      "2. Run area comparison analysis:\n",
      "   area_df, burstiness, selectivity = analyzer.area_comparison_analysis()\n",
      "\n",
      "3. Run statistical analysis:\n",
      "   analyzer.integrated_statistical_analysis()\n",
      "\n",
      "4. Generate summary report:\n",
      "   analyzer.create_summary_report()\n",
      "\n",
      "EXAMPLE:\n",
      "analyzer = NeuropixelsAnalyzer(\n",
      "    'E:/plotsNPX/values4anova.csv',\n",
      "    'E:/plotsNPX/area_cluster_mapping.csv',\n",
      "    'E:/plotsNPX/spike_timestamps.csv'\n",
      ")\n",
      "\n",
      "analyzer.area_comparison_analysis()\n",
      "analyzer.integrated_statistical_analysis()\n",
      "analyzer.create_summary_report()\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['MPLBACKEND'] = 'Qt5Agg'  # Set backend before importing matplotlib\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "matplotlib.use('Qt5Agg')  # Explicitly set backend\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats, signal\n",
    "from scipy.stats import mannwhitneyu, kruskal\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class NeuropixelsAnalyzer:\n",
    "    def __init__(self, firing_rate_csv, area_mapping_csv, spike_timestamp_csv):\n",
    "        \"\"\"\n",
    "        Initialize the analyzer with your three CSV files\n",
    "        \n",
    "        Parameters:\n",
    "        - firing_rate_csv: path to firing rates (53 neurons x 6 conditions)\n",
    "        - area_mapping_csv: path to area mapping (area names + cluster IDs)\n",
    "        - spike_timestamp_csv: path to spike timestamps (cluster_id, timestamp)\n",
    "        \"\"\"\n",
    "        self.load_data(firing_rate_csv, area_mapping_csv, spike_timestamp_csv)\n",
    "        self.conditions = ['470first', '620first', 'Simultaneous']\n",
    "        \n",
    "    def load_data(self, firing_rate_csv, area_mapping_csv, spike_timestamp_csv):\n",
    "        \"\"\"Load and process all datasets\"\"\"\n",
    "        \n",
    "        print(\"Loading datasets...\")\n",
    "        \n",
    "        # Load firing rates\n",
    "        self.firing_rates = pd.read_csv(firing_rate_csv)\n",
    "        if 'Unnamed: 0' in self.firing_rates.columns:\n",
    "            self.firing_rates = self.firing_rates.drop('Unnamed: 0', axis=1)\n",
    "        \n",
    "        # Convert to numeric\n",
    "        for col in self.firing_rates.columns:\n",
    "            self.firing_rates[col] = pd.to_numeric(self.firing_rates[col], errors='coerce')\n",
    "        self.firing_rates = self.firing_rates.fillna(0)\n",
    "        \n",
    "        # Load area mapping\n",
    "        self.area_mapping = pd.read_csv(area_mapping_csv)\n",
    "        self.area_mapping.columns = ['brain_area', 'cluster_id']\n",
    "        \n",
    "        # Load spike timestamps\n",
    "        self.spike_data = pd.read_csv(spike_timestamp_csv)\n",
    "        self.spike_data.columns = ['cluster_id', 'timestamp']\n",
    "        \n",
    "        # Filter spike data for clusters of interest\n",
    "        self.clusters_of_interest = self.area_mapping['cluster_id'].values\n",
    "        self.aoi_spikes = self.spike_data[\n",
    "            self.spike_data['cluster_id'].isin(self.clusters_of_interest)\n",
    "        ].copy()\n",
    "        \n",
    "        print(f\"✓ Firing rates loaded: {self.firing_rates.shape}\")\n",
    "        print(f\"✓ Area mapping loaded: {len(self.area_mapping)} clusters\")\n",
    "        print(f\"✓ Spike timestamps loaded: {len(self.spike_data):,} total spikes\")\n",
    "        print(f\"✓ Area of interest spikes: {len(self.aoi_spikes):,} spikes\")\n",
    "        print(f\"✓ Brain areas: {self.area_mapping['brain_area'].unique()}\")\n",
    "        \n",
    "    def area_comparison_analysis(self):\n",
    "        \"\"\"Compare neural activity across different brain areas\"\"\"\n",
    "        \n",
    "        plt.figure(figsize=(20, 12))\n",
    "        \n",
    "        # Get unique brain areas\n",
    "        brain_areas = self.area_mapping['brain_area'].unique()\n",
    "        n_areas = len(brain_areas)\n",
    "        \n",
    "        # Color palette for areas\n",
    "        colors = plt.cm.Set3(np.linspace(0, 1, n_areas))\n",
    "        \n",
    "        # 1. FIRING RATES BY BRAIN AREA\n",
    "        plt.subplot(3, 4, 1)\n",
    "        area_firing_rates = []\n",
    "        area_labels = []\n",
    "        \n",
    "        print(\"Debug: Processing brain areas...\")\n",
    "        print(f\"Firing rates DataFrame shape: {self.firing_rates.shape}\")\n",
    "        print(f\"Firing rates DataFrame columns: {list(self.firing_rates.columns)}\")\n",
    "        \n",
    "        for area in brain_areas:\n",
    "            print(f\"\\nProcessing area: {area}\")\n",
    "            area_clusters = self.area_mapping[self.area_mapping['brain_area'] == area]['cluster_id'].values\n",
    "            print(f\"Clusters in {area}: {area_clusters}\")\n",
    "            \n",
    "            # FIXED: Instead of using enumerate on clusters_of_interest, \n",
    "            # directly find matching rows in firing_rates DataFrame\n",
    "            \n",
    "            # Assume the first column of firing_rates contains cluster IDs\n",
    "            firing_rates_cluster_col = self.firing_rates.columns[0] if len(self.firing_rates.columns) > 0 else None\n",
    "            \n",
    "            if firing_rates_cluster_col is not None:\n",
    "                # Find rows where cluster_id matches area_clusters\n",
    "                area_mask = self.firing_rates[firing_rates_cluster_col].isin(area_clusters)\n",
    "                area_indices = self.firing_rates.index[area_mask].tolist()\n",
    "                print(f\"Found indices for {area}: {area_indices}\")\n",
    "            else:\n",
    "                # If no cluster ID column, use positional matching (original approach but safer)\n",
    "                area_indices = []\n",
    "                for cluster in area_clusters:\n",
    "                    if cluster in self.clusters_of_interest:\n",
    "                        try:\n",
    "                            idx = list(self.clusters_of_interest).index(cluster)\n",
    "                            if idx < len(self.firing_rates):  # Check bounds\n",
    "                                area_indices.append(idx)\n",
    "                        except ValueError:\n",
    "                            print(f\"Cluster {cluster} not found in clusters_of_interest\")\n",
    "                print(f\"Positional indices for {area}: {area_indices}\")\n",
    "            \n",
    "            if area_indices:\n",
    "                # Verify indices are within bounds\n",
    "                max_idx = max(area_indices)\n",
    "                if max_idx >= len(self.firing_rates):\n",
    "                    print(f\"WARNING: Index {max_idx} out of bounds for DataFrame with {len(self.firing_rates)} rows\")\n",
    "                    area_indices = [idx for idx in area_indices if idx < len(self.firing_rates)]\n",
    "                    print(f\"Filtered indices: {area_indices}\")\n",
    "                \n",
    "                if area_indices:  # Check again after filtering\n",
    "                    try:\n",
    "                        # Get mean firing rate across all conditions for this area\n",
    "                        area_data = self.firing_rates.iloc[area_indices, :].values\n",
    "                        area_mean_rates = np.mean(area_data[area_data > 0]) if np.any(area_data > 0) else 0\n",
    "                        \n",
    "                        area_firing_rates.append(area_mean_rates)\n",
    "                        area_labels.append(area)\n",
    "                        print(f\"Successfully processed {area}: mean rate = {area_mean_rates:.3f}\")\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing area {area}: {e}\")\n",
    "                else:\n",
    "                    print(f\"No valid indices found for area {area}\")\n",
    "            else:\n",
    "                print(f\"No clusters found for area {area}\")\n",
    "        \n",
    "        # Continue with the rest of your plotting code...\n",
    "        if area_firing_rates:  # Only plot if we have data\n",
    "            bars = plt.bar(range(len(area_labels)), area_firing_rates, \n",
    "                        color=colors[:len(area_labels)], alpha=0.7)\n",
    "            plt.xlabel('Brain Area')\n",
    "            plt.ylabel('Mean Firing Rate (Hz)')\n",
    "            plt.title('Firing Rates by Brain Area')\n",
    "            plt.xticks(range(len(area_labels)), area_labels, rotation=45)\n",
    "            \n",
    "            # Add value labels\n",
    "            for i, v in enumerate(area_firing_rates):\n",
    "                plt.text(i, v + max(area_firing_rates)*0.01, f'{v:.1f}', \n",
    "                        ha='center', va='bottom')\n",
    "        else:\n",
    "            plt.text(0.5, 0.5, 'No data available', ha='center', va='center', transform=plt.gca().transAxes)\n",
    "            plt.title('Firing Rates by Brain Area - No Data')\n",
    "        \n",
    "        # 2. SPIKE COUNT DISTRIBUTION BY AREA\n",
    "        plt.subplot(3, 4, 2)\n",
    "        \n",
    "        for i, area in enumerate(brain_areas):\n",
    "            area_clusters = self.area_mapping[self.area_mapping['brain_area'] == area]['cluster_id'].values\n",
    "            area_spikes = self.aoi_spikes[self.aoi_spikes['cluster_id'].isin(area_clusters)]\n",
    "            \n",
    "            spike_counts_per_neuron = []\n",
    "            for cluster in area_clusters:\n",
    "                cluster_spike_count = len(area_spikes[area_spikes['cluster_id'] == cluster])\n",
    "                spike_counts_per_neuron.append(cluster_spike_count)\n",
    "            \n",
    "            if spike_counts_per_neuron:\n",
    "                plt.hist(spike_counts_per_neuron, bins=20, alpha=0.6, \n",
    "                        label=area, color=colors[i])\n",
    "        \n",
    "        plt.xlabel('Spike Count per Neuron')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.title('Spike Count Distribution by Area')\n",
    "        plt.legend()\n",
    "        if len(brain_areas) > 0:\n",
    "            plt.yscale('log')\n",
    "        \n",
    "        # 3. RESPONSE PATTERNS BY AREA AND CONDITION\n",
    "        plt.subplot(3, 4, 3)\n",
    "        \n",
    "        area_condition_means = []\n",
    "        processed_areas = []\n",
    "        \n",
    "        for area in brain_areas:\n",
    "            area_clusters = self.area_mapping[self.area_mapping['brain_area'] == area]['cluster_id'].values\n",
    "            \n",
    "            # Use the same fixed logic as above\n",
    "            firing_rates_cluster_col = self.firing_rates.columns[0] if len(self.firing_rates.columns) > 0 else None\n",
    "            \n",
    "            if firing_rates_cluster_col is not None:\n",
    "                area_mask = self.firing_rates[firing_rates_cluster_col].isin(area_clusters)\n",
    "                area_indices = self.firing_rates.index[area_mask].tolist()\n",
    "            else:\n",
    "                area_indices = []\n",
    "                for cluster in area_clusters:\n",
    "                    if cluster in self.clusters_of_interest:\n",
    "                        try:\n",
    "                            idx = list(self.clusters_of_interest).index(cluster)\n",
    "                            if idx < len(self.firing_rates):\n",
    "                                area_indices.append(idx)\n",
    "                        except ValueError:\n",
    "                            continue\n",
    "            \n",
    "            # Filter out-of-bounds indices\n",
    "            area_indices = [idx for idx in area_indices if idx < len(self.firing_rates)]\n",
    "            \n",
    "            if area_indices:\n",
    "                area_means = []\n",
    "                for j in range(min(6, self.firing_rates.shape[1])):  # Ensure we don't exceed column count\n",
    "                    condition_data = self.firing_rates.iloc[area_indices, j]\n",
    "                    active_responses = condition_data[condition_data > 0]\n",
    "                    mean_response = active_responses.mean() if len(active_responses) > 0 else 0\n",
    "                    area_means.append(mean_response)\n",
    "                \n",
    "                # Pad with zeros if we have fewer than 6 conditions\n",
    "                while len(area_means) < 6:\n",
    "                    area_means.append(0)\n",
    "                    \n",
    "                area_condition_means.append(area_means)\n",
    "                processed_areas.append(area)\n",
    "        \n",
    "        # Create heatmap only if we have data\n",
    "        if area_condition_means and processed_areas:\n",
    "            area_condition_df = pd.DataFrame(area_condition_means, \n",
    "                                        index=processed_areas,\n",
    "                                        columns=['470_G1', '620_G1', 'Sim_G1', \n",
    "                                                '470_G2', '620_G2', 'Sim_G2'])\n",
    "            \n",
    "            sns.heatmap(area_condition_df, annot=True, fmt='.1f', cmap='viridis',\n",
    "                    cbar_kws={'label': 'Mean Firing Rate (Hz)'})\n",
    "            plt.title('Response Patterns by Area & Condition')\n",
    "            plt.xlabel('Conditions')\n",
    "            plt.ylabel('Brain Areas')\n",
    "        else:\n",
    "            plt.text(0.5, 0.5, 'No data available', ha='center', va='center', transform=plt.gca().transAxes)\n",
    "            plt.title('Response Patterns - No Data')\n",
    "        \n",
    "        # Continue with the rest of your subplots...\n",
    "        # [Rest of the method remains the same]\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Return results\n",
    "        if 'area_condition_df' in locals():\n",
    "            return area_condition_df, [], []  # Return empty lists for other metrics if needed\n",
    "        else:\n",
    "            return pd.DataFrame(), [], []\n",
    "    \n",
    "    def integrated_statistical_analysis(self):\n",
    "        \"\"\"Statistical tests comparing areas and conditions\"\"\"\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"INTEGRATED STATISTICAL ANALYSIS\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        brain_areas = self.area_mapping['brain_area'].unique()\n",
    "        \n",
    "        # 1. AREA COMPARISONS\n",
    "        print(\"\\n1. BRAIN AREA COMPARISONS:\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Compare firing rates between areas\n",
    "        area_firing_data = []\n",
    "        area_labels = []\n",
    "        \n",
    "        for area in brain_areas:\n",
    "            area_clusters = self.area_mapping[self.area_mapping['brain_area'] == area]['cluster_id'].values\n",
    "            area_indices = [i for i, cluster in enumerate(self.clusters_of_interest) if cluster in area_clusters]\n",
    "            \n",
    "            if area_indices:\n",
    "                area_responses = self.firing_rates.iloc[area_indices, :].values.flatten()\n",
    "                active_responses = area_responses[area_responses > 0]\n",
    "                area_firing_data.extend(active_responses)\n",
    "                area_labels.extend([area] * len(active_responses))\n",
    "        \n",
    "        # Statistical test between areas\n",
    "        if len(brain_areas) > 1:\n",
    "            area_groups = []\n",
    "            for area in brain_areas:\n",
    "                area_responses = [rate for rate, label in zip(area_firing_data, area_labels) if label == area]\n",
    "                if area_responses:\n",
    "                    area_groups.append(area_responses)\n",
    "            \n",
    "            if len(area_groups) >= 2 and all(len(group) > 0 for group in area_groups):\n",
    "                try:\n",
    "                    h_stat, p_val = kruskal(*area_groups)\n",
    "                    print(f\"Kruskal-Wallis test between areas: H = {h_stat:.3f}, p = {p_val:.3f}\")\n",
    "                    \n",
    "                    if p_val < 0.05:\n",
    "                        print(\"→ SIGNIFICANT differences between brain areas\")\n",
    "                        \n",
    "                        # Pairwise comparisons\n",
    "                        print(\"\\nPairwise comparisons:\")\n",
    "                        for i in range(len(brain_areas)):\n",
    "                            for j in range(i+1, len(brain_areas)):\n",
    "                                area1, area2 = brain_areas[i], brain_areas[j]\n",
    "                                group1 = [rate for rate, label in zip(area_firing_data, area_labels) if label == area1]\n",
    "                                group2 = [rate for rate, label in zip(area_firing_data, area_labels) if label == area2]\n",
    "                                \n",
    "                                if len(group1) > 0 and len(group2) > 0:\n",
    "                                    u_stat, p_val = mannwhitneyu(group1, group2, alternative='two-sided')\n",
    "                                    print(f\"  {area1} vs {area2}: U = {u_stat:.1f}, p = {p_val:.3f}\")\n",
    "                    else:\n",
    "                        print(\"→ No significant differences between brain areas\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error in area comparison: {e}\")\n",
    "        \n",
    "        # 2. CONDITION-SPECIFIC AREA RESPONSES\n",
    "        print(\"\\n2. CONDITION-SPECIFIC RESPONSES BY AREA:\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        for condition_idx, condition in enumerate(['470_G1', '620_G1', 'Sim_G1', '470_G2', '620_G2', 'Sim_G2']):\n",
    "            print(f\"\\n{condition}:\")\n",
    "            \n",
    "            area_responses = []\n",
    "            area_names = []\n",
    "            \n",
    "            for area in brain_areas:\n",
    "                area_clusters = self.area_mapping[self.area_mapping['brain_area'] == area]['cluster_id'].values\n",
    "                area_indices = [i for i, cluster in enumerate(self.clusters_of_interest) if cluster in area_clusters]\n",
    "                \n",
    "                if area_indices:\n",
    "                    condition_responses = self.firing_rates.iloc[area_indices, condition_idx]\n",
    "                    active_responses = condition_responses[condition_responses > 0]\n",
    "                    \n",
    "                    if len(active_responses) > 0:\n",
    "                        mean_response = active_responses.mean()\n",
    "                        responsive_neurons = len(active_responses)\n",
    "                        total_neurons = len(area_indices)\n",
    "                        response_rate = responsive_neurons / total_neurons * 100\n",
    "                        \n",
    "                        print(f\"  {area}: {mean_response:.2f} ± {active_responses.std():.2f} Hz \"\n",
    "                              f\"({responsive_neurons}/{total_neurons} neurons, {response_rate:.1f}%)\")\n",
    "                        \n",
    "                        area_responses.extend(active_responses.values)\n",
    "                        area_names.extend([area] * len(active_responses))\n",
    "            \n",
    "            # Test for area differences within this condition\n",
    "            if len(brain_areas) > 1:\n",
    "                condition_area_groups = []\n",
    "                for area in brain_areas:\n",
    "                    area_resp = [resp for resp, name in zip(area_responses, area_names) if name == area]\n",
    "                    if len(area_resp) > 0:\n",
    "                        condition_area_groups.append(area_resp)\n",
    "                \n",
    "                if len(condition_area_groups) >= 2 and all(len(group) > 0 for group in condition_area_groups):\n",
    "                    try:\n",
    "                        h_stat, p_val = kruskal(*condition_area_groups)\n",
    "                        if p_val < 0.05:\n",
    "                            print(f\"  → Significant area differences (H = {h_stat:.2f}, p = {p_val:.3f})\")\n",
    "                        else:\n",
    "                            print(f\"  → No significant area differences (p = {p_val:.3f})\")\n",
    "                    except:\n",
    "                        print(\"  → Could not perform statistical test\")\n",
    "        \n",
    "        # 3. TEMPORAL PROPERTIES BY AREA\n",
    "        print(\"\\n3. TEMPORAL PROPERTIES BY AREA:\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        for area in brain_areas:\n",
    "            area_clusters = self.area_mapping[self.area_mapping['brain_area'] == area]['cluster_id'].values\n",
    "            area_spikes = self.aoi_spikes[self.aoi_spikes['cluster_id'].isin(area_clusters)]\n",
    "            \n",
    "            if len(area_spikes) > 100:\n",
    "                # Calculate temporal metrics\n",
    "                all_isis = []\n",
    "                cv_values = []\n",
    "                \n",
    "                for cluster in area_clusters:\n",
    "                    cluster_spikes = area_spikes[area_spikes['cluster_id'] == cluster]['timestamp'].values\n",
    "                    if len(cluster_spikes) > 2:\n",
    "                        isis = np.diff(sorted(cluster_spikes))\n",
    "                        isis = isis[isis > 0]\n",
    "                        \n",
    "                        if len(isis) > 0:\n",
    "                            all_isis.extend(isis * 1000)  # Convert to ms\n",
    "                            cv = np.std(isis) / np.mean(isis)\n",
    "                            cv_values.append(cv)\n",
    "                \n",
    "                if all_isis and cv_values:\n",
    "                    median_isi = np.median(all_isis)\n",
    "                    mean_cv = np.mean(cv_values)\n",
    "                    \n",
    "                    print(f\"{area}:\")\n",
    "                    print(f\"  Median ISI: {median_isi:.1f} ms\")\n",
    "                    print(f\"  Mean CV: {mean_cv:.2f} ({'regular' if mean_cv < 1 else 'irregular'})\")\n",
    "                    print(f\"  Neurons analyzed: {len([c for c in area_clusters if len(area_spikes[area_spikes['cluster_id']==c]) > 2])}\")\n",
    "    \n",
    "    def create_summary_report(self):\n",
    "        \"\"\"Generate a comprehensive summary report\"\"\"\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"NEUROPIXELS DATA SUMMARY REPORT\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Basic statistics\n",
    "        total_spikes = len(self.aoi_spikes)\n",
    "        n_clusters = len(self.clusters_of_interest)\n",
    "        brain_areas = self.area_mapping['brain_area'].unique()\n",
    "        recording_duration = (self.aoi_spikes['timestamp'].max() - self.aoi_spikes['timestamp'].min())\n",
    "        \n",
    "        print(f\"\\nRECORDING OVERVIEW:\")\n",
    "        print(f\"• Total neurons analyzed: {n_clusters}\")\n",
    "        print(f\"• Brain areas: {', '.join(brain_areas)} ({len(brain_areas)} areas)\")\n",
    "        print(f\"• Total spikes: {total_spikes:,}\")\n",
    "        print(f\"• Recording duration: {recording_duration:.1f} seconds ({recording_duration/60:.1f} minutes)\")\n",
    "        print(f\"• Average firing rate: {total_spikes/recording_duration/n_clusters:.2f} Hz per neuron\")\n",
    "        \n",
    "        # Area breakdown\n",
    "        print(f\"\\nAREA BREAKDOWN:\")\n",
    "        for area in brain_areas:\n",
    "            area_clusters = self.area_mapping[self.area_mapping['brain_area'] == area]['cluster_id'].values\n",
    "            n_area_clusters = len(area_clusters)\n",
    "            area_spikes = self.aoi_spikes[self.aoi_spikes['cluster_id'].isin(area_clusters)]\n",
    "            area_spike_count = len(area_spikes)\n",
    "            \n",
    "            print(f\"• {area}: {n_area_clusters} neurons, {area_spike_count:,} spikes \"\n",
    "                  f\"({area_spike_count/total_spikes*100:.1f}% of total)\")\n",
    "        \n",
    "        # Condition responses\n",
    "        print(f\"\\nCONDITION RESPONSES:\")\n",
    "        condition_names = ['470nm first (G1)', '620nm first (G1)', 'Simultaneous (G1)',\n",
    "                          '470nm first (G2)', '620nm first (G2)', 'Simultaneous (G2)']\n",
    "        \n",
    "        for i, condition in enumerate(condition_names):\n",
    "            responsive_neurons = (self.firing_rates.iloc[:, i] > 0).sum()\n",
    "            mean_response = self.firing_rates.iloc[:, i][self.firing_rates.iloc[:, i] > 0].mean()\n",
    "            response_rate = responsive_neurons / n_clusters * 100\n",
    "            \n",
    "            print(f\"• {condition}: {responsive_neurons}/{n_clusters} neurons respond \"\n",
    "                  f\"({response_rate:.1f}%), mean = {mean_response:.2f} Hz\")\n",
    "\n",
    "# Example usage instructions\n",
    "print(\"\"\"\n",
    "USAGE INSTRUCTIONS:\n",
    "\n",
    "1. Initialize the analyzer:\n",
    "   analyzer = NeuropixelsAnalyzer(\n",
    "       'path/to/firing_rates.csv',\n",
    "       'path/to/area_mapping.csv', \n",
    "       'path/to/spike_timestamps.csv'\n",
    "   )\n",
    "\n",
    "2. Run area comparison analysis:\n",
    "   area_df, burstiness, selectivity = analyzer.area_comparison_analysis()\n",
    "\n",
    "3. Run statistical analysis:\n",
    "   analyzer.integrated_statistical_analysis()\n",
    "\n",
    "4. Generate summary report:\n",
    "   analyzer.create_summary_report()\n",
    "\n",
    "EXAMPLE:\n",
    "analyzer = NeuropixelsAnalyzer(\n",
    "    'E:/plotsNPX/values4anova.csv',\n",
    "    'E:/plotsNPX/area_cluster_mapping.csv',\n",
    "    'E:/plotsNPX/spike_timestamps.csv'\n",
    ")\n",
    "\n",
    "analyzer.area_comparison_analysis()\n",
    "analyzer.integrated_statistical_analysis()\n",
    "analyzer.create_summary_report()\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae27e4eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n",
      "✓ Firing rates loaded: (53, 6)\n",
      "✓ Area mapping loaded: 89 clusters\n",
      "✓ Spike timestamps loaded: 437,483 total spikes\n",
      "✓ Area of interest spikes: 405,656 spikes\n",
      "✓ Brain areas: [\"'MRN'\" \"'MB'\" \"'SCiw'\" \"'SCig'\" \"'bsc'\" \"'root'\" \"'POST'\" \"'scwm'\"\n",
      " \"'VISp6b'\" \"'VISp6a'\" \"'VISp5'\" \"'VISp4'\" \"'VISp2_3'\"]\n",
      "Debug: Processing brain areas...\n",
      "Firing rates DataFrame shape: (53, 6)\n",
      "Firing rates DataFrame columns: ['470first SP', '620first SP', 'Simultaneous SP', '470first TR', '620first TR', 'Simultaneous TR']\n",
      "\n",
      "Processing area: 'MRN'\n",
      "Clusters in 'MRN': [  8  14  11  10  28  71  75  83  95  99  98 101 106 109]\n",
      "Found indices for 'MRN': []\n",
      "No clusters found for area 'MRN'\n",
      "\n",
      "Processing area: 'MB'\n",
      "Clusters in 'MB': [113 125 124 129]\n",
      "Found indices for 'MB': []\n",
      "No clusters found for area 'MB'\n",
      "\n",
      "Processing area: 'SCiw'\n",
      "Clusters in 'SCiw': [144]\n",
      "Found indices for 'SCiw': []\n",
      "No clusters found for area 'SCiw'\n",
      "\n",
      "Processing area: 'SCig'\n",
      "Clusters in 'SCig': [148 157 159 161 163 170 169 177 180 183 188 196 194 197 192 193 200 212\n",
      " 204 213 214 206 208 209 211 207 215 224 234 228 232 239 226 229 233 227\n",
      " 249 259 244 247 257 258 242 245 246 248 255 265 267 270]\n",
      "Found indices for 'SCig': []\n",
      "No clusters found for area 'SCig'\n",
      "\n",
      "Processing area: 'bsc'\n",
      "Clusters in 'bsc': [266 272]\n",
      "Found indices for 'bsc': []\n",
      "No clusters found for area 'bsc'\n",
      "\n",
      "Processing area: 'root'\n",
      "Clusters in 'root': [282]\n",
      "Found indices for 'root': []\n",
      "No clusters found for area 'root'\n",
      "\n",
      "Processing area: 'POST'\n",
      "Clusters in 'POST': [287 290 310 323]\n",
      "Found indices for 'POST': []\n",
      "No clusters found for area 'POST'\n",
      "\n",
      "Processing area: 'scwm'\n",
      "Clusters in 'scwm': [355]\n",
      "Found indices for 'scwm': []\n",
      "No clusters found for area 'scwm'\n",
      "\n",
      "Processing area: 'VISp6b'\n",
      "Clusters in 'VISp6b': [361 368]\n",
      "Found indices for 'VISp6b': []\n",
      "No clusters found for area 'VISp6b'\n",
      "\n",
      "Processing area: 'VISp6a'\n",
      "Clusters in 'VISp6a': [372 379 387]\n",
      "Found indices for 'VISp6a': []\n",
      "No clusters found for area 'VISp6a'\n",
      "\n",
      "Processing area: 'VISp5'\n",
      "Clusters in 'VISp5': [391 394 402 405 418]\n",
      "Found indices for 'VISp5': []\n",
      "No clusters found for area 'VISp5'\n",
      "\n",
      "Processing area: 'VISp4'\n",
      "Clusters in 'VISp4': [435]\n",
      "Found indices for 'VISp4': []\n",
      "No clusters found for area 'VISp4'\n",
      "\n",
      "Processing area: 'VISp2_3'\n",
      "Clusters in 'VISp2_3': [445]\n",
      "Found indices for 'VISp2_3': []\n",
      "No clusters found for area 'VISp2_3'\n",
      "\n",
      "======================================================================\n",
      "INTEGRATED STATISTICAL ANALYSIS\n",
      "======================================================================\n",
      "\n",
      "1. BRAIN AREA COMPARISONS:\n",
      "----------------------------------------\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "positional indexers are out-of-bounds",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 8\u001b[0m\n\u001b[0;32m      1\u001b[0m analyzer \u001b[38;5;241m=\u001b[39m NeuropixelsAnalyzer(\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mE:/plotsNPX/firingRateGroups.csv\u001b[39m\u001b[38;5;124m'\u001b[39m,           \u001b[38;5;66;03m# Your firing rate CSV\u001b[39;00m\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mE:/plotsNPX/allClusters.csv\u001b[39m\u001b[38;5;124m'\u001b[39m,  \u001b[38;5;66;03m# Your area mapping CSV  \u001b[39;00m\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mE:/plotsNPX/clustersFiringtimes.csv\u001b[39m\u001b[38;5;124m'\u001b[39m       \u001b[38;5;66;03m# Your spike timestamp CSV\u001b[39;00m\n\u001b[0;32m      5\u001b[0m )\n\u001b[0;32m      7\u001b[0m analyzer\u001b[38;5;241m.\u001b[39marea_comparison_analysis()\n\u001b[1;32m----> 8\u001b[0m \u001b[43manalyzer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintegrated_statistical_analysis\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m analyzer\u001b[38;5;241m.\u001b[39mcreate_summary_report()\n",
      "Cell \u001b[1;32mIn[21], line 272\u001b[0m, in \u001b[0;36mNeuropixelsAnalyzer.integrated_statistical_analysis\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    269\u001b[0m area_indices \u001b[38;5;241m=\u001b[39m [i \u001b[38;5;28;01mfor\u001b[39;00m i, cluster \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclusters_of_interest) \u001b[38;5;28;01mif\u001b[39;00m cluster \u001b[38;5;129;01min\u001b[39;00m area_clusters]\n\u001b[0;32m    271\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m area_indices:\n\u001b[1;32m--> 272\u001b[0m     area_responses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfiring_rates\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43marea_indices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[0;32m    273\u001b[0m     active_responses \u001b[38;5;241m=\u001b[39m area_responses[area_responses \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    274\u001b[0m     area_firing_data\u001b[38;5;241m.\u001b[39mextend(active_responses)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\core\\indexing.py:1184\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1182\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_scalar_access(key):\n\u001b[0;32m   1183\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_value(\u001b[38;5;241m*\u001b[39mkey, takeable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_takeable)\n\u001b[1;32m-> 1184\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_tuple\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1185\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1186\u001b[0m     \u001b[38;5;66;03m# we by definition only have the 0th axis\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m     axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxis \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\core\\indexing.py:1690\u001b[0m, in \u001b[0;36m_iLocIndexer._getitem_tuple\u001b[1;34m(self, tup)\u001b[0m\n\u001b[0;32m   1689\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_getitem_tuple\u001b[39m(\u001b[38;5;28mself\u001b[39m, tup: \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m-> 1690\u001b[0m     tup \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_tuple_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtup\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1691\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m suppress(IndexingError):\n\u001b[0;32m   1692\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_lowerdim(tup)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\core\\indexing.py:966\u001b[0m, in \u001b[0;36m_LocationIndexer._validate_tuple_indexer\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    964\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(key):\n\u001b[0;32m    965\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 966\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_key\u001b[49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    967\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    968\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    969\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLocation based indexing can only have \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    970\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_valid_types\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] types\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    971\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\core\\indexing.py:1612\u001b[0m, in \u001b[0;36m_iLocIndexer._validate_key\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1610\u001b[0m     \u001b[38;5;66;03m# check that the key does not exceed the maximum size of the index\u001b[39;00m\n\u001b[0;32m   1611\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(arr) \u001b[38;5;129;01mand\u001b[39;00m (arr\u001b[38;5;241m.\u001b[39mmax() \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m len_axis \u001b[38;5;129;01mor\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmin() \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m-\u001b[39mlen_axis):\n\u001b[1;32m-> 1612\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpositional indexers are out-of-bounds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1613\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1614\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan only index by location with a [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_valid_types\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mIndexError\u001b[0m: positional indexers are out-of-bounds"
     ]
    }
   ],
   "source": [
    "analyzer = NeuropixelsAnalyzer(\n",
    "    'E:/plotsNPX/firingRateGroups.csv',           # Your firing rate CSV\n",
    "    'E:/plotsNPX/allClusters.csv',  # Your area mapping CSV  \n",
    "    'E:/plotsNPX/clustersFiringtimes.csv'       # Your spike timestamp CSV\n",
    ")\n",
    "\n",
    "analyzer.area_comparison_analysis()\n",
    "analyzer.integrated_statistical_analysis()\n",
    "analyzer.create_summary_report()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14766592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data files...\n",
      "Firing rates shape: (53, 6)\n",
      "Area mapping shape: (89, 2)\n",
      "Spike times shape: (437483, 2)\n",
      "\n",
      "Firing rates columns: ['470first SP', '620first SP', 'Simultaneous SP', '470first TR', '620first TR', 'Simultaneous TR']\n",
      "Area mapping columns: [\"'MRN'\", '4']\n",
      "Spike times columns: ['75', '623']\n",
      "\n",
      "Total clusters in firing rate data: 53\n",
      "First 10 cluster IDs: [0.0, 14.66666667, 14.66666667, 0.0, 0.0, 0.0, 11.33333333, 0.0, 0.0, 0.666666667]\n",
      "\n",
      "Area mapping created for 13 clusters\n",
      "Clusters per area:\n",
      "  Unknown: 53\n",
      "\n",
      "============================================================\n",
      "AREA COMPARISON ANALYSIS\n",
      "============================================================\n",
      "\n",
      "Processing area: VISp6b\n",
      "Clusters in this area: 0\n",
      "  No clusters found for area VISp6b\n",
      "\n",
      "Processing area: VISp6a\n",
      "Clusters in this area: 0\n",
      "  No clusters found for area VISp6a\n",
      "\n",
      "Processing area: VISp5\n",
      "Clusters in this area: 0\n",
      "  No clusters found for area VISp5\n",
      "\n",
      "Processing area: VISp4\n",
      "Clusters in this area: 0\n",
      "  No clusters found for area VISp4\n",
      "\n",
      "Processing area: VISp2_3\n",
      "Clusters in this area: 0\n",
      "  No clusters found for area VISp2_3\n",
      "\n",
      "Insufficient data for area comparison (need at least 2 areas)\n",
      "No area results available. Run area_comparison_analysis() first.\n",
      "\n",
      "============================================================\n",
      "SUMMARY REPORT\n",
      "============================================================\n",
      "Dataset Overview:\n",
      "  - Total clusters: 53\n",
      "  - Areas mapped: 13\n",
      "  - Areas of interest analyzed: 5\n",
      "\n",
      "Analysis completed successfully!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "class NeuropixelsAnalyzer:\n",
    "    def __init__(self, firing_rate_csv, area_mapping_csv, spike_times_csv):\n",
    "        \"\"\"\n",
    "        Initialize the analyzer with CSV file paths\n",
    "        \"\"\"\n",
    "        print(\"Loading data files...\")\n",
    "        \n",
    "        # Load the data\n",
    "        self.firing_rates = pd.read_csv(firing_rate_csv)\n",
    "        self.area_mapping = pd.read_csv(area_mapping_csv)\n",
    "        self.spike_times = pd.read_csv(spike_times_csv)\n",
    "        \n",
    "        # Print basic info about loaded data\n",
    "        print(f\"Firing rates shape: {self.firing_rates.shape}\")\n",
    "        print(f\"Area mapping shape: {self.area_mapping.shape}\")\n",
    "        print(f\"Spike times shape: {self.spike_times.shape}\")\n",
    "        \n",
    "        # Print column names for debugging\n",
    "        print(\"\\nFiring rates columns:\", list(self.firing_rates.columns))\n",
    "        print(\"Area mapping columns:\", list(self.area_mapping.columns))\n",
    "        print(\"Spike times columns:\", list(self.spike_times.columns))\n",
    "        \n",
    "        # Define areas of interest\n",
    "        self.areas_of_interest = ['VISp6b', 'VISp6a', 'VISp5', 'VISp4', 'VISp2_3']\n",
    "        \n",
    "        # Get clusters of interest - using first column as cluster ID\n",
    "        cluster_id_col = self.firing_rates.columns[0]\n",
    "        self.clusters_of_interest = self.firing_rates[cluster_id_col].tolist()\n",
    "        \n",
    "        print(f\"\\nTotal clusters in firing rate data: {len(self.clusters_of_interest)}\")\n",
    "        print(f\"First 10 cluster IDs: {self.clusters_of_interest[:10]}\")\n",
    "        \n",
    "        # Process area mapping\n",
    "        self.process_area_mapping()\n",
    "        \n",
    "    def process_area_mapping(self):\n",
    "        \"\"\"\n",
    "        Process area mapping to create a lookup dictionary\n",
    "        \"\"\"\n",
    "        # Assume first column is cluster_id and second is area\n",
    "        cluster_col = self.area_mapping.columns[0]\n",
    "        area_col = self.area_mapping.columns[1]\n",
    "        \n",
    "        self.cluster_to_area = dict(zip(self.area_mapping[cluster_col], \n",
    "                                      self.area_mapping[area_col]))\n",
    "        \n",
    "        print(f\"\\nArea mapping created for {len(self.cluster_to_area)} clusters\")\n",
    "        \n",
    "        # Count clusters per area\n",
    "        area_counts = {}\n",
    "        for cluster_id in self.clusters_of_interest:\n",
    "            area = self.cluster_to_area.get(cluster_id, 'Unknown')\n",
    "            area_counts[area] = area_counts.get(area, 0) + 1\n",
    "        \n",
    "        print(\"Clusters per area:\")\n",
    "        for area, count in sorted(area_counts.items()):\n",
    "            print(f\"  {area}: {count}\")\n",
    "    \n",
    "    def area_comparison_analysis(self):\n",
    "        \"\"\"\n",
    "        Compare firing rates across different brain areas with improved error handling\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"AREA COMPARISON ANALYSIS\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        area_firing_rates = []\n",
    "        area_labels = []\n",
    "        area_cluster_counts = []\n",
    "        \n",
    "        for area in self.areas_of_interest:\n",
    "            # Find clusters in this area\n",
    "            area_clusters = [cluster_id for cluster_id in self.clusters_of_interest \n",
    "                           if self.cluster_to_area.get(cluster_id) == area]\n",
    "            \n",
    "            print(f\"\\nProcessing area: {area}\")\n",
    "            print(f\"Clusters in this area: {len(area_clusters)}\")\n",
    "            \n",
    "            if not area_clusters:\n",
    "                print(f\"  No clusters found for area {area}\")\n",
    "                continue\n",
    "                \n",
    "            # Find indices of these clusters in the firing rate dataframe\n",
    "            cluster_id_col = self.firing_rates.columns[0]\n",
    "            area_indices = []\n",
    "            \n",
    "            for cluster_id in area_clusters:\n",
    "                try:\n",
    "                    # Find the index where cluster_id matches\n",
    "                    idx = self.firing_rates[self.firing_rates[cluster_id_col] == cluster_id].index\n",
    "                    if len(idx) > 0:\n",
    "                        area_indices.extend(idx.tolist())\n",
    "                    else:\n",
    "                        print(f\"  Warning: Cluster {cluster_id} not found in firing rate data\")\n",
    "                except Exception as e:\n",
    "                    print(f\"  Error finding cluster {cluster_id}: {e}\")\n",
    "            \n",
    "            print(f\"  Valid indices found: {len(area_indices)}\")\n",
    "            print(f\"  Indices range: {min(area_indices) if area_indices else 'None'} to {max(area_indices) if area_indices else 'None'}\")\n",
    "            print(f\"  DataFrame has {len(self.firing_rates)} rows\")\n",
    "            \n",
    "            if area_indices:\n",
    "                try:\n",
    "                    # Verify indices are within bounds\n",
    "                    max_idx = max(area_indices)\n",
    "                    if max_idx >= len(self.firing_rates):\n",
    "                        print(f\"  ERROR: Index {max_idx} is out of bounds for DataFrame with {len(self.firing_rates)} rows\")\n",
    "                        continue\n",
    "                    \n",
    "                    # Get numeric columns (exclude first column which is cluster ID)\n",
    "                    numeric_cols = self.firing_rates.select_dtypes(include=[np.number]).columns\n",
    "                    \n",
    "                    # Get firing rate data for this area\n",
    "                    area_data = self.firing_rates.iloc[area_indices][numeric_cols].values\n",
    "                    \n",
    "                    # Calculate mean firing rate across all conditions\n",
    "                    area_mean_rates = np.mean(area_data[area_data > 0]) if np.any(area_data > 0) else 0\n",
    "                    \n",
    "                    area_firing_rates.append(area_mean_rates)\n",
    "                    area_labels.append(area)\n",
    "                    area_cluster_counts.append(len(area_indices))\n",
    "                    \n",
    "                    print(f\"  Mean firing rate: {area_mean_rates:.3f} Hz\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"  ERROR processing area {area}: {e}\")\n",
    "                    continue\n",
    "        \n",
    "        if len(area_firing_rates) < 2:\n",
    "            print(\"\\nInsufficient data for area comparison (need at least 2 areas)\")\n",
    "            return\n",
    "        \n",
    "        # Create visualization\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        # Plot 1: Mean firing rates by area\n",
    "        plt.subplot(2, 2, 1)\n",
    "        bars = plt.bar(area_labels, area_firing_rates, alpha=0.7, color='skyblue')\n",
    "        plt.title('Mean Firing Rates by Brain Area')\n",
    "        plt.ylabel('Firing Rate (Hz)')\n",
    "        plt.xticks(rotation=45)\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar, rate in zip(bars, area_firing_rates):\n",
    "            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1, \n",
    "                    f'{rate:.2f}', ha='center', va='bottom')\n",
    "        \n",
    "        # Plot 2: Cluster counts by area\n",
    "        plt.subplot(2, 2, 2)\n",
    "        plt.bar(area_labels, area_cluster_counts, alpha=0.7, color='lightcoral')\n",
    "        plt.title('Number of Clusters by Area')\n",
    "        plt.ylabel('Number of Clusters')\n",
    "        plt.xticks(rotation=45)\n",
    "        \n",
    "        # Plot 3: Firing rate distribution\n",
    "        plt.subplot(2, 2, 3)\n",
    "        plt.boxplot(area_firing_rates, labels=area_labels)\n",
    "        plt.title('Firing Rate Distribution')\n",
    "        plt.ylabel('Firing Rate (Hz)')\n",
    "        plt.xticks(rotation=45)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Statistical analysis\n",
    "        print(f\"\\nStatistical Summary:\")\n",
    "        print(f\"Areas analyzed: {len(area_labels)}\")\n",
    "        print(f\"Mean firing rate across areas: {np.mean(area_firing_rates):.3f} ± {np.std(area_firing_rates):.3f} Hz\")\n",
    "        \n",
    "        # Store results for later use\n",
    "        self.area_results = {\n",
    "            'areas': area_labels,\n",
    "            'firing_rates': area_firing_rates,\n",
    "            'cluster_counts': area_cluster_counts\n",
    "        }\n",
    "    \n",
    "    def integrated_statistical_analysis(self):\n",
    "        \"\"\"\n",
    "        Perform statistical tests on the area comparison results\n",
    "        \"\"\"\n",
    "        if not hasattr(self, 'area_results'):\n",
    "            print(\"No area results available. Run area_comparison_analysis() first.\")\n",
    "            return\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"STATISTICAL ANALYSIS\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        rates = self.area_results['firing_rates']\n",
    "        areas = self.area_results['areas']\n",
    "        \n",
    "        # Basic statistics\n",
    "        print(f\"Number of areas: {len(areas)}\")\n",
    "        print(f\"Overall mean: {np.mean(rates):.3f} Hz\")\n",
    "        print(f\"Overall std: {np.std(rates):.3f} Hz\")\n",
    "        print(f\"Range: {np.min(rates):.3f} - {np.max(rates):.3f} Hz\")\n",
    "        \n",
    "        # Pairwise comparisons if we have enough areas\n",
    "        if len(rates) >= 2:\n",
    "            print(\"\\nPairwise comparisons:\")\n",
    "            for i in range(len(areas)):\n",
    "                for j in range(i+1, len(areas)):\n",
    "                    diff = rates[i] - rates[j]\n",
    "                    print(f\"{areas[i]} vs {areas[j]}: {diff:.3f} Hz difference\")\n",
    "    \n",
    "    def create_summary_report(self):\n",
    "        \"\"\"\n",
    "        Create a comprehensive summary report\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"SUMMARY REPORT\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        print(f\"Dataset Overview:\")\n",
    "        print(f\"  - Total clusters: {len(self.clusters_of_interest)}\")\n",
    "        print(f\"  - Areas mapped: {len(set(self.cluster_to_area.values()))}\")\n",
    "        print(f\"  - Areas of interest analyzed: {len(self.areas_of_interest)}\")\n",
    "        \n",
    "        if hasattr(self, 'area_results'):\n",
    "            print(f\"\\nArea Analysis Results:\")\n",
    "            for i, area in enumerate(self.area_results['areas']):\n",
    "                print(f\"  - {area}: {self.area_results['firing_rates'][i]:.3f} Hz \"\n",
    "                      f\"({self.area_results['cluster_counts'][i]} clusters)\")\n",
    "        \n",
    "        print(\"\\nAnalysis completed successfully!\")\n",
    "\n",
    "# Usage example with debug information\n",
    "def run_analysis_with_debug():\n",
    "    \"\"\"\n",
    "    Run the analysis with enhanced debugging\n",
    "    \"\"\"\n",
    "    try:\n",
    "        analyzer = NeuropixelsAnalyzer(\n",
    "            'E:/plotsNPX/firingRateGroups.csv',\n",
    "            'E:/plotsNPX/allClusters.csv', \n",
    "            'E:/plotsNPX/clustersFiringtimes.csv'\n",
    "        )\n",
    "        \n",
    "        analyzer.area_comparison_analysis()\n",
    "        analyzer.integrated_statistical_analysis()\n",
    "        analyzer.create_summary_report()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during analysis: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "# Run the analysis\n",
    "if __name__ == \"__main__\":\n",
    "    run_analysis_with_debug()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59a7e2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
